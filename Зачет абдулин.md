### 1. Понятие функции потерь. Примеры функций потерь для задач регрессии и классификации.

Функция потерь это такая функция с помощью которой можно определить насколько сильно отличается полученный результат от того что должно получиться. С помощью нее, можно посчитать градиент, который покажет как каждый вес с параметром влияет на функцию патерь и пойти в сторону уиеньшения функции потерь, с помощью чего улучшается предсказания. Функция потерь одна из важнейших состовляющих ML, на ней держится метод обратного распрространения, с помощью которого происходит обучение сети. Также Функция потерь должна быть дефиринцируема и непрерывна.
Примеры функции потерь:
1. MSE
2. абсолютная ошибка
3. Кроссэнтропия
4. BCE
==Нужно тут напистаь формулы==
### 2. Градиентный спуск: идея, математическая формулировка, варианты (SGD, mini-batch, Adam).
Градиентный спуск - метод с помощью которого сеть обучается подбирая свои коэффиценты путем нахождения градиента функции потерь.
<span style="background:#fff88f">Формула градиентного спуска</span>
Одни из самых популярныйх и ихвестных примеров град спуска является:
SGD - суть заключается в том чтобы брать один случайный элемент из выборки пропускать его через сеть и считать функцию ошибку и подбивать на этой основе веса, после чего сделать так N(количество пример)=1 эпоха
Mini batch - суть в том чтобы вмсето обычно sgd где мы берем один случайный жлемент из выборки, разделить эту выборку на какоето количество подвыборок, по которым уже считать результат и loss function после чего делать обратно рапсрростронение и улучшать сеть
Adam - это один из самых популярных на данный момент оптимизаторов который базируется на логике двух другиз популяных оптимизатарах. Это SGD с Momentumom у которого взята логика по тому чтобы накапливать импульс и продолевать седловые точки, так и оптимизатора RMSProp (суть это оптимизатора заключается в том чтобы сделай learning rate для каждого веса уникальным, условно оптимизатор смотрит с помощью жкспонинциально скользящего среднего на то какой обычно градиент у этого параметра, если часто большой, для него лучше сделать шаг поменьше, если часто мальшенький, то шаг лучше сдлеать побольше)
Rpop - его суть заключается в том мы смотрим на знак градиента, мы умножаем градиенты, и если после переумножения получилось > 0 *положительное*, значит мы не пропустили ничего и можно даже немного ускориться и увеличить Learning Rate, а в обратном случае уменьишт ьLE в случае если знак поменялся.