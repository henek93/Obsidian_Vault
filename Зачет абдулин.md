# Математические основы нейронных сетей и ИИ

## Подробное руководство к зачёту с понятными объяснениями

---

# РАЗДЕЛ 1. МАТЕМАТИЧЕСКИЕ ОСНОВЫ

---

## Вопрос 1. Понятие функции потерь. Примеры для регрессии и классификации

### Что такое функция потерь и зачем она нужна

Представьте, что вы учите ребёнка бросать мяч в корзину. После каждого броска вам нужно как-то измерить, насколько хорошо или плохо он бросил. Если мяч попал — отлично, если промахнулся на метр — плохо, если на пять метров — очень плохо.

**Функция потерь (Loss Function)** — это именно такой «измеритель качества» для нейронной сети. Она берёт два числа:

- $y$ — правильный ответ (куда нужно было попасть)
- $\hat{y}$ — предсказание модели (куда на самом деле попали)

И выдаёт одно число — насколько сильно модель ошиблась.

### Математическое определение

Функция потерь $L(y, \hat{y})$ — это функция, которая:

- Равна нулю (или минимальна), когда предсказание идеально: $\hat{y} = y$
- Растёт, когда предсказание отклоняется от правильного ответа

**Общие потери на всём датасете** — это среднее по всем примерам:

$$\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} L(y_i, \hat{y}_i)$$

где $N$ — количество примеров в обучающей выборке.

**Цель обучения нейронной сети** — найти такие веса, при которых эти общие потери минимальны. То есть мы буквально ищем минимум функции потерь.

---

### Функции потерь для задач РЕГРЕССИИ

Регрессия — это когда мы предсказываем число. Например: цену квартиры, температуру завтра, возраст человека по фото.

#### 1. MSE — Mean Squared Error (Среднеквадратичная ошибка)

$$L_{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

**Как это работает:**

1. Берём разницу между правильным ответом и предсказанием: $(y - \hat{y})$
2. Возводим в квадрат: $(y - \hat{y})^2$
3. Усредняем по всем примерам

**Пример расчёта:** Допустим, модель предсказывает цены квартир (в миллионах рублей):

|Реальная цена $y$|Предсказание $\hat{y}$|Ошибка $(y-\hat{y})$|Квадрат ошибки|
|---|---|---|---|
|5.0|4.5|0.5|0.25|
|8.0|9.0|-1.0|1.00|
|3.0|3.2|-0.2|0.04|

$$MSE = \frac{0.25 + 1.00 + 0.04}{3} = \frac{1.29}{3} = 0.43$$

**Почему квадрат?**

1. **Убирает знак**: ошибка -1 и +1 одинаково плохи
2. **Сильно штрафует большие ошибки**: ошибка в 2 раза больше даёт штраф в 4 раза больше
3. **Гладкая функция**: легко считать производную для градиентного спуска

**Аналогия**: Представьте, что вы платите штраф за опоздание на работу. При MSE штраф за опоздание на 10 минут в 100 раз больше, чем за опоздание на 1 минуту. Это заставляет очень стараться не опаздывать сильно.

**Недостаток MSE**: очень чувствительна к выбросам. Если в данных есть одна квартира с ошибкой предсказания в 10 млн, она даст штраф 100 и «перетянет» всё обучение на себя.

#### 2. MAE — Mean Absolute Error (Средняя абсолютная ошибка)

$$L_{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$

**Как это работает:**

1. Берём разницу между правильным ответом и предсказанием
2. Берём модуль (абсолютное значение)
3. Усредняем

**Отличие от MSE:**

- Ошибка в 2 раза больше даёт штраф ровно в 2 раза больше (линейно, а не квадратично)
- Менее чувствительна к выбросам

**Аналогия**: Штраф за опоздание пропорционален минутам. 10 минут опоздания = 10 рублей штрафа, 20 минут = 20 рублей.

**Недостаток MAE**: не дифференцируема в точке $y = \hat{y}$ (излом графика). Это может создавать проблемы при оптимизации.

#### 3. Huber Loss — компромисс между MSE и MAE

$$L_{\delta}(y, \hat{y}) = \begin{cases} \frac{1}{2}(y - \hat{y})^2, & \text{если } |y - \hat{y}| \leq \delta \ \delta \cdot |y - \hat{y}| - \frac{1}{2}\delta^2, & \text{если } |y - \hat{y}| > \delta \end{cases}$$

Ведёт себя как MSE для маленьких ошибок и как MAE для больших. Лучшее из двух миров.

---

### Функции потерь для задач КЛАССИФИКАЦИИ

Классификация — это когда мы предсказываем категорию. Например: спам/не спам, кошка/собака, цифра от 0 до 9.

#### 1. Бинарная кросс-энтропия (Binary Cross-Entropy)

Используется, когда есть только 2 класса (да/нет, 0/1).

$$L_{BCE} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$

**Разберём по частям:**

Здесь $y \in {0, 1}$ — правильный класс, $\hat{y} \in (0, 1)$ — предсказанная вероятность класса 1.

**Случай 1: правильный ответ $y = 1$** $$L = -\log(\hat{y})$$

Если модель предсказала $\hat{y} = 0.99$ (уверена, что класс 1) → $L = -\log(0.99) \approx 0.01$ — маленькие потери ✓

Если модель предсказала $\hat{y} = 0.01$ (уверена, что класс 0, но ошиблась!) → $L = -\log(0.01) \approx 4.6$ — огромные потери ✗

**Случай 2: правильный ответ $y = 0$** $$L = -\log(1 - \hat{y})$$

Если модель предсказала $\hat{y} = 0.01$ → $L = -\log(0.99) \approx 0.01$ — маленькие потери ✓

Если модель предсказала $\hat{y} = 0.99$ → $L = -\log(0.01) \approx 4.6$ — огромные потери ✗

**Аналогия — «мера удивления»:**

Представьте, что модель — это синоптик, предсказывающий дождь.

- Синоптик сказал: «99% будет дождь». Дождь пошёл. Вы не удивлены — потери маленькие.
- Синоптик сказал: «1% будет дождь». Дождь пошёл. Вы ОЧЕНЬ удивлены — потери огромные.

Кросс-энтропия измеряет, насколько модель «удивляется» правильному ответу.

#### 2. Категориальная кросс-энтропия (Categorical Cross-Entropy)

Используется для многоклассовой классификации (например, 10 цифр).

$$L_{CCE} = -\sum_{k=1}^{K} y_k \log(\hat{y}_k)$$

Здесь:

- $K$ — количество классов
- $y_k$ — one-hot вектор правильного класса (например, для цифры 3: $[0,0,0,1,0,0,0,0,0,0]$)
- $\hat{y}_k$ — предсказанные вероятности каждого класса

**Пример:**

Распознаём цифру на картинке. Правильный ответ: 3.

One-hot: $y = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]$

Модель предсказала: $\hat{y} = [0.01, 0.01, 0.05, 0.85, 0.02, 0.01, 0.02, 0.01, 0.01, 0.01]$

$$L = -1 \cdot \log(0.85) = 0.16$$

Потери небольшие, потому что модель дала высокую вероятность правильному классу.

Если бы модель предсказала: $\hat{y} = [0.5, 0.1, 0.1, 0.05, ...]$ (вероятность 3 всего 5%):

$$L = -1 \cdot \log(0.05) = 3.0$$

Потери большие — модель ошиблась.

---

### Связь с методом максимального правдоподобия

Откуда взялась формула кросс-энтропии? Она не придумана случайно, а выводится из фундаментального статистического принципа.

**Метод максимального правдоподобия (Maximum Likelihood Estimation):**

1. Мы предполагаем, что данные порождены некоторым распределением с параметрами $\theta$
2. **Правдоподобие** — вероятность наблюдать наши данные при данных параметрах: $$\mathcal{L}(\theta) = \prod_{i=1}^{N} P(y_i | \mathbf{x}_i, \theta)$$
3. Мы хотим найти $\theta$, максимизирующие правдоподобие

**Переход к логарифму:**

Произведение вероятностей — очень маленькое число. Удобнее работать с суммой логарифмов:

$$\log \mathcal{L}(\theta) = \sum_{i=1}^{N} \log P(y_i | \mathbf{x}_i, \theta)$$

**Связь с кросс-энтропией:**

Максимизация лог-правдоподобия = Минимизация отрицательного лог-правдоподобия = Минимизация кросс-энтропии!

$$\text{Cross-Entropy} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i | \mathbf{x}_i, \theta)$$

Это не совпадение — кросс-энтропия математически обоснована как оптимальная функция потерь для классификации.

---

## Вопрос 2. Градиентный спуск: идея, формулировка, варианты (SGD, mini-batch, Adam)

### Что такое градиент — напоминание из математики

Прежде чем говорить о градиентном спуске, вспомним, что такое градиент.

**Для функции одной переменной** $f(x)$:

- Производная $f'(x)$ показывает скорость изменения функции
- Если $f'(x) > 0$ — функция растёт при увеличении $x$
- Если $f'(x) < 0$ — функция убывает

**Для функции многих переменных** $f(x_1, x_2, ..., x_n)$:

- Есть много направлений, куда можно двигаться
- Нужен вектор, показывающий направление наибыстрейшего роста

**Градиент** — это вектор из частных производных:

$$\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right)$$

**Ключевое свойство**: градиент указывает в направлении наибыстрейшего РОСТА функции.

**Пример:**

$f(x, y) = x^2 + y^2$ — парабола (чаша)

$$\nabla f = (2x, 2y)$$

В точке $(3, 4)$: $\nabla f = (6, 8)$

Это значит: если мы сдвинемся в направлении $(6, 8)$, функция будет расти быстрее всего.

### Идея градиентного спуска

**Задача**: найти минимум функции потерь $L(\theta)$, где $\theta$ — веса нейронной сети.

**Идея**: если градиент показывает направление роста, то **антиградиент** $(-\nabla L)$ показывает направление убывания!

**Алгоритм:**

1. Начинаем с произвольных весов $\theta_0$
2. Вычисляем градиент $\nabla L(\theta_t)$
3. Делаем шаг в направлении антиградиента
4. Повторяем, пока не сойдёмся к минимуму

**Аналогия — спуск с горы в тумане:**

Вы стоите на горе и хотите спуститься вниз, но туман такой густой, что вы видите только землю под ногами.

Стратегия: пощупать склон вокруг себя и сделать шаг в направлении, где склон идёт вниз круче всего.

Градиент — это «направление наибольшей крутизны вверх», поэтому мы идём в противоположную сторону.

### Математическая формулировка

**Правило обновления весов:**

$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta L(\theta_t)$$

где:

- $\theta_t$ — текущие веса на шаге $t$
- $\eta$ (эта) — **скорость обучения (learning rate)** — размер шага
- $\nabla_\theta L$ — градиент функции потерь по весам

**Выбор learning rate — критически важен:**

- $\eta$ слишком большой → перепрыгиваем через минимум, расходимся
- $\eta$ слишком маленький → сходимся очень медленно
- Типичные значения: 0.001, 0.01, 0.1

```
         Loss
           │    ╱╲
           │   ╱  ╲        η слишком большой — прыгаем туда-сюда
           │  ╱    ╲
           │ ╱      ╲
           │╱   •    ╲
           ──────────────→ θ
                минимум
```

---

### Варианты градиентного спуска

#### 1. Batch Gradient Descent (Полный градиентный спуск)

$$\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta L(y_i, f(\mathbf{x}_i; \theta_t))$$

**Как работает:**

- Берём ВСЮ обучающую выборку (все $N$ примеров)
- Вычисляем градиент по каждому примеру
- Усредняем все градиенты
- Делаем один шаг

**Плюсы:**

- Стабильная сходимость — градиент точный
- Гарантированно идём в правильном направлении

**Минусы:**

- Очень медленно на больших данных (миллионы примеров = миллионы градиентов перед одним шагом)
- Требует много памяти
- Может застрять в локальном минимуме

**Аналогия:** Перед каждым шагом вы тщательно обследуете всю гору с помощью дрона, строите 3D-модель, и только потом делаете один шаг. Точно, но очень долго.

#### 2. Stochastic Gradient Descent (SGD) — Стохастический

$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta L(y_i, f(\mathbf{x}_i; \theta_t))$$

**Как работает:**

- Берём ОДИН случайный пример из выборки
- Вычисляем градиент только по нему
- Делаем шаг

**Плюсы:**

- Очень быстро — один пример = один шаг
- Шум помогает выбраться из локальных минимумов
- Может работать с бесконечными потоками данных

**Минусы:**

- Шумный градиент — может идти не совсем в правильном направлении
- «Прыгает» вокруг минимума, не может точно сойтись

**Аналогия:** Вы делаете шаг после беглого взгляда под ноги. Быстро, но можете идти зигзагами.

```
         Loss
           │      ∿∿∿
           │    ∿    ∿
           │  ∿        ∿     SGD — зигзагообразный путь
           │∿          ∿
           │    •  ∿∿∿
           ──────────────→ θ
```

#### 3. Mini-batch Gradient Descent — Компромисс

$$\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{|B|} \sum_{i \in B} \nabla_\theta L(y_i, f(\mathbf{x}_i; \theta_t))$$

**Как работает:**

- Берём небольшую порцию (batch) из $|B|$ примеров (обычно 32, 64, 128, 256)
- Вычисляем среднее градиента по этой порции
- Делаем шаг

**Плюсы:**

- Баланс между скоростью и стабильностью
- Хорошо параллелится на GPU
- Достаточно стабильный градиент

**Это стандарт в современном глубоком обучении!**

**Терминология:**

- **Epoch (эпоха)** — один проход по всему датасету
- **Iteration (итерация)** — обновление весов на одном batch
- Если датасет 10000 примеров и batch size = 100, то в одной эпохе 100 итераций

#### 4. Adam — Adaptive Moment Estimation

Adam — самый популярный оптимизатор. Он умнее обычного SGD.

**Идея:** запоминать историю градиентов и адаптировать скорость обучения для каждого параметра отдельно.

**Формулы:**

Первый момент (экспоненциальное среднее градиентов): $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$

Второй момент (экспоненциальное среднее квадратов градиентов): $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

Коррекция смещения (важно в начале обучения): $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

Обновление весов: $$\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

**Что это значит интуитивно:**

- $m_t$ — «инерция». Если градиенты долго указывали в одном направлении, мы продолжаем двигаться туда
- $v_t$ — «крутизна». Если градиенты по этому параметру большие, мы делаем шаги меньше (чтобы не перепрыгнуть)
- Деление на $\sqrt{v_t}$ нормализует шаг: большие градиенты → маленький шаг, маленькие градиенты → большой шаг

**Стандартные гиперпараметры:**

- $\beta_1 = 0.9$ (инерция)
- $\beta_2 = 0.999$ (память о крутизне)
- $\eta = 0.001$ (learning rate)
- $\epsilon = 10^{-8}$ (для численной стабильности)

**Аналогия — умный турист:**

Обычный турист (SGD) делает шаг туда, куда указывает склон прямо сейчас.

Умный турист (Adam):

- Помнит, куда шёл раньше (момент $m$) — если долго шёл на восток, продолжает на восток даже если склон чуть-чуть на запад
- Помнит, насколько крутым был склон (дисперсия $v$) — на очень крутых участках делает осторожные маленькие шаги

---

### Сравнение оптимизаторов

|Оптимизатор|Скорость|Стабильность|Когда использовать|
|---|---|---|---|
|Batch GD|Медленно|Высокая|Маленькие датасеты, выпуклые задачи|
|SGD|Быстро|Низкая|Онлайн-обучение, очень большие данные|
|Mini-batch|Средне|Средняя|Стандартный выбор|
|Adam|Быстро|Высокая|Почти всегда хороший выбор|

---

## Вопрос 3. Алгоритм обратного распространения ошибки (Backpropagation)

### Проблема: как вычислить градиенты в глубокой сети?

Нейронная сеть может иметь миллионы параметров. Для градиентного спуска нам нужен градиент функции потерь по КАЖДОМУ параметру.

Как вычислить $\frac{\partial L}{\partial w}$ для каждого веса $w$?

**Наивный подход — численное дифференцирование:**

$$\frac{\partial L}{\partial w} \approx \frac{L(w + \epsilon) - L(w - \epsilon)}{2\epsilon}$$

Для каждого параметра нужно сделать 2 прямых прохода сети. Если параметров миллион — это 2 миллиона проходов. Неприемлемо!

**Решение — backpropagation:** вычислить все градиенты за ОДИН обратный проход.

### Правило цепочки (Chain Rule) — математическая основа

**Для функции одной переменной:**

Если $y = f(g(x))$ (композиция функций), то:

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$$

**Пример:**

$y = (3x + 2)^2$

Пусть $g = 3x + 2$, тогда $y = g^2$

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx} = 2g \cdot 3 = 6(3x + 2)$$

**Для нескольких переменных:**

Если $L = L(z)$, $z = z(a, b)$, $a = a(x)$, $b = b(x)$:

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial a} \cdot \frac{\partial a}{\partial x} + \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial b} \cdot \frac{\partial b}{\partial x}$$

### Нейронная сеть как композиция функций

Нейронная сеть — это последовательность преобразований:

$$\mathbf{x} \xrightarrow{W_1, b_1} \mathbf{z}_1 \xrightarrow{\sigma} \mathbf{a}_1 \xrightarrow{W_2, b_2} \mathbf{z}_2 \xrightarrow{\sigma} \mathbf{a}_2 \xrightarrow{...} \hat{y} \xrightarrow{L} \text{Loss}$$

где:

- $\mathbf{z}_l = W_l \mathbf{a}_{l-1} + \mathbf{b}_l$ — линейное преобразование
- $\mathbf{a}_l = \sigma(\mathbf{z}_l)$ — нелинейная активация

Это длинная цепочка композиций! Правило цепочки позволяет разложить градиент по этой цепочке.

### Алгоритм Backpropagation

**Шаг 1: Прямой проход (Forward Pass)**

Вычисляем выходы всех слоёв, двигаясь от входа к выходу:

```
x → z₁ = W₁x + b₁ → a₁ = σ(z₁) → z₂ = W₂a₁ + b₂ → a₂ = σ(z₂) → ... → Loss
```

**Важно:** запоминаем все промежуточные значения ($\mathbf{z}_l$, $\mathbf{a}_l$), они понадобятся на обратном проходе.

**Шаг 2: Обратный проход (Backward Pass)**

Вычисляем градиенты, двигаясь от выхода к входу:

```
Loss → ∂L/∂aₗ → ∂L/∂zₗ → ∂L/∂Wₗ, ∂L/∂bₗ → ∂L/∂aₗ₋₁ → ...
```

**Формулы для каждого слоя $l$:**

1. **Градиент по входу слоя (до активации):** $$\frac{\partial L}{\partial \mathbf{z}_l} = \frac{\partial L}{\partial \mathbf{a}_l} \odot \sigma'(\mathbf{z}_l)$$
    
    где $\odot$ — поэлементное умножение, $\sigma'$ — производная активации.
    
2. **Градиент по весам:** $$\frac{\partial L}{\partial W_l} = \frac{\partial L}{\partial \mathbf{z}_l} \cdot \mathbf{a}_{l-1}^T$$
    
3. **Градиент по смещениям:** $$\frac{\partial L}{\partial \mathbf{b}_l} = \frac{\partial L}{\partial \mathbf{z}_l}$$
    
4. **Передача градиента на предыдущий слой:** $$\frac{\partial L}{\partial \mathbf{a}_{l-1}} = W_l^T \cdot \frac{\partial L}{\partial \mathbf{z}_l}$$
    

### Пример: сеть с одним скрытым слоем

**Архитектура:**

- Вход: $\mathbf{x} \in \mathbb{R}^2$
- Скрытый слой: $\mathbf{h} = \sigma(W_1 \mathbf{x} + \mathbf{b}_1) \in \mathbb{R}^3$
- Выход: $\hat{y} = W_2 \mathbf{h} + b_2 \in \mathbb{R}$
- Потери: $L = \frac{1}{2}(y - \hat{y})^2$

**Прямой проход:**

1. $\mathbf{z}_1 = W_1 \mathbf{x} + \mathbf{b}_1$
2. $\mathbf{h} = \sigma(\mathbf{z}_1)$ (например, sigmoid)
3. $\hat{y} = W_2 \mathbf{h} + b_2$
4. $L = \frac{1}{2}(y - \hat{y})^2$

**Обратный проход:**

1. $\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
    
2. $\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial \hat{y}} \cdot \mathbf{h}^T = (\hat{y} - y) \cdot \mathbf{h}^T$
    
3. $\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
    
4. $\frac{\partial L}{\partial \mathbf{h}} = W_2^T \cdot \frac{\partial L}{\partial \hat{y}} = W_2^T \cdot (\hat{y} - y)$
    
5. $\frac{\partial L}{\partial \mathbf{z}_1} = \frac{\partial L}{\partial \mathbf{h}} \odot \sigma'(\mathbf{z}_1)$
    
6. $\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{z}_1} \cdot \mathbf{x}^T$
    
7. $\frac{\partial L}{\partial \mathbf{b}_1} = \frac{\partial L}{\partial \mathbf{z}_1}$
    

### Аналогия — конвейер на заводе

Представьте завод по производству автомобилей с 10 станками на конвейере.

**Прямой проход:** детали проходят через все станки, на выходе — автомобиль. Контролёр проверяет качество и находит дефект.

**Обратный проход:** контролёр идёт по конвейеру НАЗАД и спрашивает каждый станок:

- «Насколько ТЫ виноват в этом дефекте?»
- Каждый станок отвечает пропорционально тому, насколько его настройки повлияли на дефект

**Правило цепочки говорит:** вина станка = (его влияние на следующий станок) × (вина следующего станка)

**Обновление:** каждый станок корректирует свои настройки (веса) пропорционально своей «вине».

---

## Вопрос 4. Матричные и тензорные операции. Почему они важны?

### Основные операции в нейронных сетях

**Полносвязный слой — умножение матрицы на вектор:**

$$\mathbf{z} = W\mathbf{x} + \mathbf{b}$$

где:

- $\mathbf{x} \in \mathbb{R}^n$ — вход (вектор из $n$ элементов)
- $W \in \mathbb{R}^{m \times n}$ — матрица весов
- $\mathbf{b} \in \mathbb{R}^m$ — вектор смещений
- $\mathbf{z} \in \mathbb{R}^m$ — выход (вектор из $m$ элементов)

**Batch processing — умножение матриц:**

Вместо обработки примеров по одному, обрабатываем сразу $k$ примеров:

$$Z = WX + B$$

где:

- $X \in \mathbb{R}^{n \times k}$ — batch из $k$ входных векторов
- $Z \in \mathbb{R}^{m \times k}$ — batch из $k$ выходных векторов

### Почему матричные операции эффективны

#### 1. Параллелизм на GPU

**CPU** имеет 4-16 мощных ядер — хорош для последовательных вычислений.

**GPU** имеет тысячи простых ядер — идеален для параллельных вычислений.

Умножение матриц $1000 \times 1000$ — это $10^9$ умножений. На GPU все эти умножения выполняются почти одновременно!

**Сравнение:**

|Операция|CPU (1 ядро)|CPU (8 ядер)|GPU|
|---|---|---|---|
|Умножение 1000×1000 матриц|~2000 мс|~300 мс|~1 мс|

GPU быстрее в 1000+ раз!

#### 2. Кэш-эффективность

Матричные операции обрабатывают данные последовательно в памяти. Это минимизирует «промахи кэша» (cache misses).

**Аналогия:** читать книгу страница за страницей (последовательно) быстрее, чем прыгать между случайными страницами.

#### 3. Оптимизированные библиотеки

Библиотеки BLAS, cuBLAS, cuDNN — это десятилетия оптимизаций:

- Умное использование кэша
- Векторные инструкции процессора (SIMD)
- Оптимальное распределение по ядрам GPU

#### 4. Batching — ещё один уровень параллелизма

Вместо обработки примеров по одному:

```python
for x in dataset:
    z = W @ x + b  # Много маленьких операций
```

Обрабатываем batch:

```python
Z = W @ X + B  # Одна большая операция
```

Это:

- Лучше утилизирует GPU
- Уменьшает накладные расходы на запуск операций
- Даёт более стабильные градиенты

### Интуитивная аналогия

**Задача:** посчитать 1000 независимых сумм по 1000 чисел каждая.

**Последовательно (цикл for):** нанимаем одного бухгалтера. Он считает суммы одну за другой. Занимает час.

**Параллельно (матричные операции на GPU):** нанимаем 1000 бухгалтеров. Каждый считает свою сумму. Все заканчивают за минуту.

Нейронные сети — это миллионы таких независимых вычислений. GPU делает их почти одновременно.

---

# РАЗДЕЛ 2. МНОГОСЛОЙНЫЙ ПЕРСЕПТРОН (MLP)

---

## Вопрос 5. Принципы построения и обучения глубоких сетей

### Почему глубокие сети?

**Теорема Цыбенко** (вопрос 7) говорит: одного скрытого слоя достаточно для аппроксимации любой функции.

Но на практике **глубокие** сети (много слоёв) работают лучше. Почему?

#### 1. Иерархическое представление признаков

Глубокая сеть выучивает признаки на разных уровнях абстракции:

**Пример — распознавание лиц:**

- **Слой 1:** детекторы краёв (вертикальные, горизонтальные, диагональные линии)
- **Слой 2:** комбинации краёв (углы, дуги, простые формы)
- **Слой 3:** части лица (глаза, носы, рты)
- **Слой 4:** целые лица, выражения

Каждый слой строится на предыдущем, создавая всё более сложные и абстрактные признаки.

#### 2. Эффективность представления

Глубокие сети могут представить сложные функции **экспоненциально** эффективнее, чем широкие и мелкие.

**Аналогия — представление чисел:**

- В унарной системе число 1000 требует 1000 символов: 111...1
- В десятичной системе: 4 символа
- В двоичной: 10 символов

Глубина — как более эффективная система счисления для функций.

#### 3. Композиционность

Реальный мир иерархичен и композиционален:

- Атомы → молекулы → клетки → органы → организмы
- Буквы → слова → предложения → абзацы → документы

Глубокие сети естественно моделируют такую структуру.

### Принципы обучения глубоких сетей

#### 1. Правильная инициализация весов

**Проблема:** неправильная инициализация приводит к затуханию или взрыву активаций/градиентов.

**Xavier/Glorot инициализация** (для tanh, sigmoid): $$W \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right)$$

**He инициализация** (для ReLU): $$W \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)$$

**Идея:** начальные активации должны иметь примерно одинаковую дисперсию во всех слоях.

#### 2. Batch Normalization

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$ $$y_i = \gamma \hat{x}_i + \beta$$

**Что делает:**

- Нормализует активации в каждом слое (среднее = 0, дисперсия = 1)
- $\gamma$, $\beta$ — обучаемые параметры для масштабирования и сдвига

**Зачем:**

- Стабилизирует обучение
- Позволяет использовать большие learning rate
- Действует как регуляризатор

#### 3. Residual Connections (Skip Connections)

$$\mathbf{y} = F(\mathbf{x}) + \mathbf{x}$$

**Идея:** добавить «шоссе» для информации и градиентов в обход слоёв.

**Почему работает:**

- Градиент может течь напрямую, без затухания
- Легче обучить $F(\mathbf{x}) = 0$ (тождественное отображение), чем $F(\mathbf{x}) = \mathbf{x}$
- Позволяет обучать сети глубиной 100+ слоёв (ResNet)

#### 4. Dropout

Во время обучения случайно «выключаем» нейроны с вероятностью $p$.

**Зачем:**

- Регуляризация — борьба с переобучением
- Ансамблирование — каждый проход = разная подсеть
- Предотвращает ко-адаптацию нейронов

---

## Вопрос 6. Архитектура MLP: входной, скрытые и выходной слои

### Структура многослойного персептрона

```
      Входной слой         Скрытые слои           Выходной слой
          (d)                (n₁, n₂, ...)              (k)
           │                     │                       │
     ┌─────┴─────┐        ┌──────┴──────┐         ┌──────┴──────┐
     │ x₁  ...xd │   →    │ h₁⁽¹⁾...h_{n₁}⁽¹⁾ │  →  │ y₁  ...  yₖ │
     └───────────┘        └─────────────┘         └─────────────┘
           │                     │                       │
        признаки         нелинейные              предсказания
                        преобразования
```

### Входной слой

**Функция:** принимает входные данные (признаки объекта).

**Размер:** равен количеству признаков $d$.

**Примеры:**

- Изображение 28×28: $d = 784$ (пиксели развёрнуты в вектор)
- Табличные данные с 10 признаками: $d = 10$

**Входной слой не делает вычислений** — это просто «точка входа» данных в сеть.

### Скрытые слои

**Функция:** последовательные нелинейные преобразования входных данных.

Для слоя $l$: $$\mathbf{h}^{(l)} = \sigma(W_l \mathbf{h}^{(l-1)} + \mathbf{b}_l)$$

где:

- $\mathbf{h}^{(l-1)}$ — выход предыдущего слоя (или вход для первого скрытого слоя)
- $W_l$ — матрица весов
- $\mathbf{b}_l$ — вектор смещений
- $\sigma$ — функция активации (ReLU, sigmoid, tanh)

**Что происходит в скрытом слое интуитивно:**

1. Каждый нейрон «смотрит» на все входы с разными весами
2. Суммирует взвешенные входы
3. Применяет нелинейную активацию
4. Выдаёт одно число — свою «активацию»

**Аналогия — комитет экспертов:**

Каждый нейрон — это эксперт, который:

- Получает все данные
- Обращает разное внимание на разные аспекты (веса)
- Выносит своё суждение (активация)

Следующий слой — комитет экспертов более высокого уровня, который анализирует мнения предыдущего комитета.

### Выходной слой

**Функция:** формирует финальное предсказание в нужном формате.

**Зависит от задачи:**

|Задача|Размер выхода|Активация|
|---|---|---|
|Бинарная классификация|1|sigmoid|
|Многоклассовая классификация (K классов)|K|softmax|
|Регрессия|1 (или больше)|нет (линейный выход)|

**Пример для MNIST (10 цифр):**

Выходной слой: 10 нейронов с softmax. Выход: вектор из 10 вероятностей, сумма = 1. Например: $[0.01, 0.01, 0.02, 0.90, 0.01, ...]$ → предсказание: цифра 3.

### Подсчёт параметров

Для слоя с $n_{in}$ входами и $n_{out}$ выходами:

- Веса: $n_{in} \times n_{out}$
- Смещения: $n_{out}$
- **Всего:** $n_{out} \times (n_{in} + 1)$

**Пример: сеть для MNIST**

Архитектура: $[784 \to 256 \to 128 \to 10]$

|Слой|Размер|Параметры|
|---|---|---|
|1: 784→256|$256 \times 784 + 256$|200,960|
|2: 256→128|$128 \times 256 + 128$|32,896|
|3: 128→10|$10 \times 128 + 10$|1,290|
|**Всего**||**235,146**|

### Аналогия — система принятия решений в компании

**Входной слой** — отдел сбора данных. Собирает всю информацию извне.

**Первый скрытый слой** — младшие аналитики. Каждый специализируется на своём аспекте данных.

**Второй скрытый слой** — старшие аналитики. Синтезируют отчёты младших, находят более глубокие закономерности.

**Выходной слой** — руководство. Принимает финальное решение на основе всех анализов.

---

## Вопрос 7. Универсальная аппроксимационная теорема (теорема Цыбенко)

### Формулировка теоремы

**Теорема (Cybenko, 1989):**

Пусть $\sigma$ — непрерывная сигмоидальная функция (например, sigmoid). Тогда для любой непрерывной функции $f: [0,1]^d \to \mathbb{R}$ и любого $\epsilon > 0$ существует нейронная сеть с **одним скрытым слоем**:

$$F(\mathbf{x}) = \sum_{j=1}^{N} \alpha_j \sigma(\mathbf{w}_j^T \mathbf{x} + b_j)$$

такая, что для всех $\mathbf{x} \in [0,1]^d$:

$$|F(\mathbf{x}) - f(\mathbf{x})| < \epsilon$$

### Простыми словами

**Нейронная сеть с одним скрытым слоем и достаточным количеством нейронов может приблизить ЛЮБУЮ непрерывную функцию с ЛЮБОЙ точностью.**

### Значение теоремы

1. **Теоретическое обоснование:** нейронные сети — универсальные аппроксиматоры. Они не ограничены в том, какие функции могут представить.
    
2. **Гарантия существования:** для любой задачи (если она может быть описана непрерывной функцией) существует нейронная сеть, которая её решает.
    
3. **Мотивация:** если решение существует, имеет смысл его искать.
    

### Ограничения теоремы — очень важно понимать!

#### 1. Теорема экзистенциальная, не конструктивная

Теорема говорит: «Такая сеть СУЩЕСТВУЕТ».

Теорема НЕ говорит:

- Сколько нейронов нужно?
- Какие веса должны быть?
- Как найти эти веса?
- За какое время можно обучить?

#### 2. Количество нейронов может быть ЭКСПОНЕНЦИАЛЬНО большим

Для некоторых функций количество нейронов в одном слое растёт экспоненциально с размерностью входа.

Например, для аппроксимации функции в $\mathbb{R}^{100}$ может потребоваться $2^{100}$ нейронов — больше, чем атомов во Вселенной!

#### 3. Глубокие сети эффективнее на практике

Хотя теоретически одного слоя достаточно, **глубокие сети**:

- Требуют экспоненциально меньше нейронов
- Лучше выделяют иерархические признаки
- Легче обучаются на практике

**Аналогия — представление функций:**

Функцию можно представить как полином. Теорема Вейерштрасса говорит, что любую непрерывную функцию можно приблизить полиномами.

Но для сложных функций степень полинома (= количество нейронов) может быть огромной.

Глубокие сети — как более умная система представления функций.

#### 4. Не гарантирует обобщение

Теорема говорит об аппроксимации на заданном множестве (обучающей выборке).

Она НЕ гарантирует, что модель будет хорошо работать на новых данных. Можно идеально «запомнить» обучающую выборку и провалиться на тесте.

### Интуитивная аналогия

**Теорема говорит:** «С достаточным количеством точек можно нарисовать любую картину».

Технически это правда — если у вас бесконечно много пикселей, вы можете изобразить что угодно.

**Но:**

- Для «Моны Лизы» вам нужно, может быть, миллиард пикселей
- Теорема не говорит, какого цвета должен быть каждый пиксель
- Даже с правильными пикселями — как их расставить?

**Глубокие сети** — это как использование не пикселей, а мазков кистью. Художник может нарисовать картину с гораздо меньшим количеством движений, потому что каждый мазок несёт больше информации.

---

## Вопрос 8. Роль нелинейности в скрытых слоях MLP

### Что будет без нелинейности — математическое доказательство

Рассмотрим сеть с двумя слоями **без нелинейной активации**:

Слой 1: $\mathbf{h} = W_1 \mathbf{x} + \mathbf{b}_1$

Слой 2: $\hat{\mathbf{y}} = W_2 \mathbf{h} + \mathbf{b}_2$

Подставим первое во второе:

$$\hat{\mathbf{y}} = W_2 (W_1 \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2$$ $$\hat{\mathbf{y}} = W_2 W_1 \mathbf{x} + W_2 \mathbf{b}_1 + \mathbf{b}_2$$ $$\hat{\mathbf{y}} = \underbrace{(W_2 W_1)}_{W'} \mathbf{x} + \underbrace{(W_2 \mathbf{b}_1 + \mathbf{b}_2)}_{\mathbf{b}'}$$ $$\hat{\mathbf{y}} = W' \mathbf{x} + \mathbf{b}'$$

**Результат:** два линейных слоя эквивалентны ОДНОМУ линейному слою с матрицей $W' = W_2 W_1$.

**Обобщение:** сколько бы линейных слоёв мы ни складывали, результат — всегда один линейный слой!

$$W_n W_{n-1} ... W_2 W_1 = W'$$

### Что может и чего не может линейная модель

**Линейная модель** разделяет пространство **гиперплоскостью**.

В 2D — прямая линия. В 3D — плоскость. В $n$-мерном пространстве — $(n-1)$-мерная гиперплоскость.

**Линейно разделимые данные:**

```
     y
     │    o o o
     │   o o o o
     │  ─────────── граница (прямая)
     │    x x x
     │   x x x x
     └───────────── x
```

Линейный классификатор справляется отлично.

**Линейно НЕразделимые данные (XOR):**

```
     y
     │  x       o
     │     
     │  o       x
     └───────────── x
```

|$x_1$|$x_2$|XOR|
|---|---|---|
|0|0|0 (x)|
|0|1|1 (o)|
|1|0|1 (o)|
|1|1|0 (x)|

**Невозможно провести прямую линию, разделяющую классы!**

Это знаменитая проблема, из-за которой исследования нейронных сетей «заморозились» на десятилетия после работы Минского и Паперта (1969).

### Что даёт нелинейность

#### 1. Разрыв линейности

Нелинейная активация «ломает» цепочку линейных преобразований:

$$\mathbf{h} = \sigma(W_1 \mathbf{x} + \mathbf{b}_1)$$ $$\hat{\mathbf{y}} = W_2 \mathbf{h} + \mathbf{b}_2$$

Теперь это НЕ эквивалентно одному линейному слою из-за $\sigma$ между ними.

#### 2. Сложные границы решений

С нелинейностью сеть может создавать **произвольно сложные** границы между классами:

```
     y
     │  x    ╭──╮  o
     │    ╭──╯  ╰──╮
     │  o ╰────────╯ x
     └───────────── x
```

#### 3. Иерархия признаков

Каждый слой с нелинейностью создаёт **новое пространство признаков**, в котором данные могут быть легче разделимы.

### Как MLP решает XOR

**Скрытый слой** преобразует пространство так, что XOR становится линейно разделимой!

**Пример с 2 скрытыми нейронами:**

```
Исходное пространство:          После скрытого слоя:
     x₂                              h₂
     │  (0,1)●     ●(1,1)            │      ●(1,1) 
     │       1     0                 │       0
     │                               │
     │  (0,0)●     ●(1,0)            │  ●(0,0)  ●(1,0)
     │       0     1                 │   0       1
     └─────────── x₁                 └─────────── h₁
                                         ↑
                                   Теперь линейно
                                   разделимы!
```

Скрытый слой «переставляет» точки так, что классы можно разделить прямой линией.

### Аналогия — изгибание бумаги

Представьте точки на листе бумаги, которые нельзя разделить прямой линией.

**Линейный классификатор:** может только провести линию на плоском листе.

**Нелинейная сеть:** может согнуть, смять, скрутить бумагу так, что нужные точки окажутся рядом, а ненужные — далеко. Затем проводит «линию» в этом изогнутом пространстве.

---

## Вопрос 9. Матричное исчисление, Softmax, категориальная кросс-энтропия

### Матричное исчисление — основы

#### Производная скаляра по вектору

Если $L \in \mathbb{R}$ — скаляр, $\mathbf{x} \in \mathbb{R}^n$ — вектор:

$$\frac{\partial L}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial L}{\partial x_1} \ \frac{\partial L}{\partial x_2} \ \vdots \ \frac{\partial L}{\partial x_n} \end{pmatrix} \in \mathbb{R}^n$$

**Пример:** $L = x_1^2 + 2x_2^2$

$$\frac{\partial L}{\partial \mathbf{x}} = \begin{pmatrix} 2x_1 \ 4x_2 \end{pmatrix}$$

#### Матрица Якоби (производная вектора по вектору)

Если $\mathbf{y} \in \mathbb{R}^m$, $\mathbf{x} \in \mathbb{R}^n$:

$$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \ \vdots & \ddots & \vdots \ \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n} \end{pmatrix} \in \mathbb{R}^{m \times n}$$

#### Полезные формулы для нейросетей

1. $\frac{\partial}{\partial \mathbf{x}}(W\mathbf{x}) = W$
    
2. $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{x}) = 2\mathbf{x}$
    
3. $\frac{\partial}{\partial W}(W\mathbf{x}) = \mathbf{x}^T$ (в смысле: $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{z}} \mathbf{x}^T$)
    
4. $\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^T \mathbf{x}) = \mathbf{a}$
    

### Softmax — подробный разбор

#### Формула

Для вектора $\mathbf{z} = (z_1, z_2, ..., z_K)$:

$$\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

#### Что делает Softmax

1. **Экспонента** $e^{z_i}$: преобразует любое число в положительное
2. **Нормализация** (деление на сумму): превращает в вероятности (сумма = 1)

#### Пример расчёта

$\mathbf{z} = [2.0, 1.0, 0.1]$

1. Экспоненты: $[e^{2.0}, e^{1.0}, e^{0.1}] = [7.39, 2.72, 1.10]$
2. Сумма: $7.39 + 2.72 + 1.10 = 11.21$
3. Softmax: $[\frac{7.39}{11.21}, \frac{2.72}{11.21}, \frac{1.10}{11.21}] = [0.659, 0.243, 0.098]$

**Интерпретация:** модель «думает», что с вероятностью 65.9% это класс 1, 24.3% — класс 2, 9.8% — класс 3.

#### Свойства Softmax

1. **Выход в $(0, 1)$**: каждый элемент — положительное число меньше 1
2. **Сумма = 1**: $\sum_i \text{softmax}(\mathbf{z})_i = 1$
3. **Монотонность**: больший $z_i$ → больший $\text{softmax}(\mathbf{z})_i$
4. **Температура**: softmax$(z/T)$ — при $T \to 0$ становится «жёстким» (argmax), при $T \to \infty$ — равномерным

#### Производная Softmax

$$\frac{\partial \text{softmax}_i}{\partial z_j} = \text{softmax}_i \cdot (\delta_{ij} - \text{softmax}_j)$$

где $\delta_{ij} = 1$ если $i = j$, иначе $0$.

### Категориальная кросс-энтропия

#### Формула

$$L = -\sum_{k=1}^{K} y_k \log(\hat{y}_k)$$

где:

- $\mathbf{y}$ — one-hot вектор правильного класса
- $\hat{\mathbf{y}} = \text{softmax}(\mathbf{z})$ — предсказанные вероятности

#### Упрощение для one-hot

Если правильный класс — $c$ (то есть $y_c = 1$, остальные $y_k = 0$):

$$L = -\log(\hat{y}_c) = -\log(\text{softmax}(\mathbf{z})_c)$$

#### Красивый градиент Softmax + Cross-Entropy

$$\frac{\partial L}{\partial \mathbf{z}} = \hat{\mathbf{y}} - \mathbf{y} = \text{softmax}(\mathbf{z}) - \mathbf{y}$$

**Это невероятно простая и красивая формула!**

**Пример:**

- Правильный класс: 2 (one-hot: $\mathbf{y} = [0, 0, 1]$)
- Предсказание: $\hat{\mathbf{y}} = [0.3, 0.5, 0.2]$
- Градиент: $\frac{\partial L}{\partial \mathbf{z}} = [0.3, 0.5, 0.2] - [0, 0, 1] = [0.3, 0.5, -0.8]$

Модель получает «штраф» за слишком высокую уверенность в неправильных классах и «поощрение» за правильный класс.

---

# РАЗДЕЛ 3. ФУНКЦИИ АКТИВАЦИИ

---

## Вопрос 10. Основные функции активации: ReLU, sigmoid, tanh, softmax

### Sigmoid (Логистическая функция)

#### Формула

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

#### Производная

$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$

**Максимум производной:** $\sigma'(0) = 0.25$

#### График

```
σ(x)
  1 │            ╭───────
    │          ╱
0.5 │        ╱
    │      ╱
  0 │─────╯
    └─────────────────── x
        -4  -2   0   2   4
```

#### Свойства

|Плюсы|Минусы|
|---|---|
|Гладкая, дифференцируема везде|Затухающие градиенты ($\sigma' \leq 0.25$)|
|Выход в $(0, 1)$ — интерпретируется как вероятность|Выход не центрирован (всегда положительный)|
|Исторически первая, хорошо изучена|Вычислительно дорогая (экспонента)|
||«Насыщается» при больших $|

#### Когда использовать

- Выходной слой для бинарной классификации
- Гейты в LSTM/GRU (нужен выход в $(0, 1)$)
- **НЕ рекомендуется** для скрытых слоёв глубоких сетей

---

### Tanh (Гиперболический тангенс)

#### Формула

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

#### Производная

$$\tanh'(x) = 1 - \tanh^2(x)$$

**Максимум производной:** $\tanh'(0) = 1$

#### График

```
tanh(x)
  1 │            ╭───────
    │          ╱
  0 │────────╱────────────
    │      ╱
 -1 │─────╯
    └─────────────────── x
        -4  -2   0   2   4
```

#### Свойства

|Плюсы|Минусы|
|---|---|
|Выход центрирован: $(-1, 1)$|Затухающие градиенты (хотя лучше sigmoid)|
|Более сильные градиенты, чем sigmoid|Вычислительно дорогая|
|Нулевое среднее → лучше для обучения|Насыщается при больших $|

#### Когда использовать

- RNN (особенно в LSTM для $\tanh$ активации)
- Когда нужен центрированный выход
- **Редко** в современных глубоких сетях

---

### ReLU (Rectified Linear Unit)

#### Формула

$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x, & x > 0 \ 0, & x \leq 0 \end{cases}$$

#### Производная

$$\text{ReLU}'(x) = \begin{cases} 1, & x > 0 \ 0, & x \leq 0 \end{cases}$$

(Технически не определена в $x = 0$, на практике берут 0)

#### График

```
ReLU(x)
    │        ╱
    │      ╱
    │    ╱
  0 │───╱────────────────
    │
    └─────────────────── x
       -2  -1   0   1   2
```

#### Свойства

|Плюсы|Минусы|
|---|---|
|Очень быстрое вычисление (просто max)|«Мёртвые нейроны» — если $x < 0$ всегда, градиент = 0 навсегда|
|Не насыщается для $x > 0$|Не центрирована (выход ≥ 0)|
|Разреженная активация (~50% нейронов = 0)|Не дифференцируема в $x = 0$|
|Помогает с затуханием градиентов||

#### Проблема «мёртвых нейронов»

Если $W\mathbf{x} + b < 0$ для всех примеров, нейрон выдаёт 0, градиент = 0, веса не обновляются. Нейрон «умирает».

Это может произойти при:

- Плохой инициализации
- Слишком большом learning rate

#### Когда использовать

- **Стандарт** для скрытых слоёв CNN и MLP
- Не подходит для выходного слоя
![[Pasted image 20251216224910.png]]

---

### Softmax

#### Формула

$$\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

#### Особенности

- Применяется к **вектору**, не к скаляру
- Выход — **вероятностное распределение** (сумма = 1)
- Используется **только в выходном слое** для многоклассовой классификации

### Сравнительная таблица

|Функция|Диапазон|Производная max|Центрирована|Скорость|
|---|---|---|---|---|
|Sigmoid|$(0, 1)$|0.25|Нет|Медленная|
|Tanh|$(-1, 1)$|1.0|Да|Медленная|
|ReLU|$[0, +\infty)$|1.0|Нет|Быстрая|
|Softmax|$(0, 1)$, сумма=1|—|—|Средняя|

---

## Вопрос 11. Проблема затухающих градиентов

### Математическое описание

При обратном распространении градиент передаётся через слои:

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{a}_L} \cdot \prod_{l=2}^{L} \left( \frac{\partial \mathbf{a}_l}{\partial \mathbf{z}_l} \cdot \frac{\partial \mathbf{z}_l}{\partial \mathbf{a}_{l-1}} \right) \cdot \frac{\partial \mathbf{a}_1}{\partial \mathbf{z}_1} \cdot \frac{\partial \mathbf{z}_1}{\partial W_1}$$

Ключевой множитель: $\frac{\partial \mathbf{a}_l}{\partial \mathbf{z}_l} = \sigma'(\mathbf{z}_l)$ — производная активации.

### Анализ для Sigmoid

Производная sigmoid: $\sigma'(x) = \sigma(x)(1 - \sigma(x)) \leq 0.25$

При $L = 10$ слоях: $$\prod_{l=1}^{10} \sigma'(z_l) \leq 0.25^{10} = 9.5 \times 10^{-7}$$

**Градиент уменьшается в миллион раз!**

### Визуализация

```
Слой:       1        2        3        4        5
            │        │        │        │        │
Sigmoid: × 0.25 → × 0.25 → × 0.25 → × 0.25 → × 0.25
            │        │        │        │        │
Градиент: 1.0  →  0.25  → 0.0625 → 0.0156 → 0.0039
            ↓        ↓        ↓        ↓        ↓
        Первые слои получают почти нулевой градиент!
```

### Последствия

1. **Первые слои не обучаются:** градиенты слишком малы для значимого обновления весов
2. **Медленная сходимость:** нужно огромное количество эпох
3. **Застревание:** сеть не может выбраться из плохого решения

### Какие активации усугубляют проблему

**Sigmoid:** $\sigma'(x) \leq 0.25$ — самая проблемная

**Tanh:** $\tanh'(x) \leq 1$, но при насыщении ($|x| > 2$) производная близка к 0

### Решения

1. **ReLU:** $\text{ReLU}'(x) = 1$ для $x > 0$ — не затухает!
    
2. **Правильная инициализация:** Xavier, He — сохраняют дисперсию активаций
    
3. **Batch Normalization:** нормализует активации, предотвращает насыщение
    
4. **Residual Connections:** градиент может течь напрямую через skip-connection
    
5. **LSTM/GRU для RNN:** специальные гейты для контроля потока градиента
    

### Аналогия — испорченный телефон

Представьте игру «испорченный телефон» с 10 участниками:

**Sigmoid/Tanh:** каждый участник не только искажает сообщение, но и говорит ТИШЕ. К 10-му участнику доходит лишь шёпот — практически тишина.

**ReLU:** каждый участник говорит с той же громкостью (или молчит совсем). Если все говорят, сообщение доходит чётко.

**ResNet:** у каждого участника есть прямая линия связи с первым. Даже если цепочка «испортилась», можно передать сообщение напрямую.

---

## Вопрос 12. Современные функции активации: Leaky ReLU, ELU, GELU

### Проблема ReLU — «мёртвые нейроны»

Если $z < 0$, то $\text{ReLU}(z) = 0$ и $\text{ReLU}'(z) = 0$.

Если нейрон «застрял» в отрицательной зоне для всех примеров, он никогда не обновится — он «мёртв».

### Leaky ReLU

#### Формула

$$\text{LeakyReLU}(x) = \begin{cases} x, & x > 0 \ \alpha x, & x \leq 0 \end{cases}$$

где $\alpha$ — маленькое число (обычно 0.01).

#### Производная

$$\text{LeakyReLU}'(x) = \begin{cases} 1, & x > 0 \ \alpha, & x \leq 0 \end{cases}$$

#### График

```
LeakyReLU(x)
    │        ╱
    │      ╱
    │    ╱
  0 │──╱──────────────── (маленький наклон α при x<0)
    │╱
    └─────────────────── x
```

#### Преимущество

Градиент **никогда** не равен нулю! Даже при $x < 0$ градиент = $\alpha \neq 0$.

Нейроны не могут полностью «умереть».

### Parametric ReLU (PReLU)

То же, что Leaky ReLU, но $\alpha$ — **обучаемый параметр**.

Сеть сама определяет оптимальный наклон для отрицательных значений.

### ELU (Exponential Linear Unit)

#### Формула

$$\text{ELU}(x) = \begin{cases} x, & x > 0 \ \alpha(e^x - 1), & x \leq 0 \end{cases}$$

#### Производная

$$\text{ELU}'(x) = \begin{cases} 1, & x > 0 \ \text{ELU}(x) + \alpha, & x \leq 0 \end{cases}$$

#### График

```
ELU(x)
    │        ╱
    │      ╱
  0 │────╱────────────────
    │  ╱
 -α │─╯    (плавно насыщается к -α)
    └─────────────────── x
```

#### Преимущества

1. **Гладкая функция** — дифференцируема везде (в отличие от ReLU)
2. **Центрирована около нуля** — отрицательные значения «тянут» среднее вниз
3. **Насыщение для отрицательных значений** добавляет устойчивость к шуму

### GELU (Gaussian Error Linear Unit)

#### Формула

$$\text{GELU}(x) = x \cdot \Phi(x)$$

где $\Phi(x)$ — функция распределения стандартного нормального распределения.

#### Приближённая формула (для вычислений)

$$\text{GELU}(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right]\right)$$

#### График

```
GELU(x)
    │        ╱
    │      ╱
  0 │────╱────────────────
    │  ╱
    │╱   (небольшой «провал» при x<0)
    └─────────────────── x
```

#### Вероятностная интерпретация

GELU можно интерпретировать как стохастическую регуляризацию:

- Вход $x$ «пропускается» с вероятностью $\Phi(x)$
- Большие положительные $x$ почти всегда проходят
- Отрицательные $x$ редко проходят
- Около нуля — «мягкое» решение

#### Где используется

**GELU — стандарт в трансформерах!** (BERT, GPT, и др.)

### Swish / SiLU

#### Формула

$$\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$

#### Свойства

- Гладкая
- Не ограничена сверху
- Ограничена снизу (минимум ≈ -0.278)
- Похожа на ReLU, но гладкая

**Используется в EfficientNet и других современных архитектурах.**

### Сравнительная таблица

|Функция|Гладкая|Нет мёртвых нейронов|Центрирована|Типичное применение|
|---|---|---|---|---|
|ReLU|Нет|Нет|Нет|CNN, MLP (классика)|
|Leaky ReLU|Нет|✓|Нет|CNN, MLP|
|ELU|✓|✓|~Да|CNN, MLP|
|GELU|✓|✓|Нет|Transformers (BERT, GPT)|
|Swish/SiLU|✓|✓|Нет|EfficientNet, современные CNN|

### Какую выбрать?

**Практические рекомендации:**

1. **Начните с ReLU** — простой, быстрый, работает в большинстве случаев
    
2. **Если много «мёртвых нейронов»** → Leaky ReLU или ELU
    
3. **Для трансформеров** → GELU (стандарт)
    
4. **Если хотите максимальное качество** → экспериментируйте с Swish/SiLU, GELU
    
---

# РАЗДЕЛ 4. СВЁРТОЧНЫЕ НЕЙРОННЫЕ СЕТИ (CNN)

---

## Вопрос 13. Принципы свёртки и пулинга. Инвариантности

### Что такое свёртка — базовая идея

Представьте, что вы детектив, который ищет отпечатки пальцев на большой поверхности. У вас есть «эталонный» отпечаток (фильтр), и вы прикладываете его к каждому участку поверхности, проверяя: «Похоже или нет?»

**Свёртка (Convolution)** — это именно такая операция:

1. Берём маленький «шаблон» (фильтр/ядро) — обычно 3×3 или 5×5
2. Прикладываем его ко всем позициям изображения
3. В каждой позиции вычисляем «сходство» — сумму поэлементных произведений

### Математическое определение 1D свёртки

Для одномерного сигнала (аудио, временной ряд, текст):

$$(x * w)[i] = \sum_{j=0}^{k-1} x[i+j] \cdot w[j]$$

где:

- $x$ — входной сигнал длины $n$
- $w$ — фильтр (ядро) длины $k$
- $(x * w)[i]$ — значение выхода в позиции $i$

**Пример расчёта 1D свёртки:**

Вход: $x = [1, 2, 3, 4, 5]$ (5 элементов) Фильтр: $w = [1, 0, -1]$ (детектор изменений, 3 элемента)

**Позиция 0:** накладываем фильтр на элементы $x[0], x[1], x[2]$ $$(x * w)[0] = x[0] \cdot w[0] + x[1] \cdot w[1] + x[2] \cdot w[2]$$ $$= 1 \cdot 1 + 2 \cdot 0 + 3 \cdot (-1) = 1 + 0 - 3 = -2$$

**Позиция 1:** сдвигаем фильтр на 1, накладываем на $x[1], x[2], x[3]$ $$(x * w)[1] = x[1] \cdot w[0] + x[2] \cdot w[1] + x[3] \cdot w[2]$$ $$= 2 \cdot 1 + 3 \cdot 0 + 4 \cdot (-1) = 2 + 0 - 4 = -2$$

**Позиция 2:** сдвигаем ещё на 1, накладываем на $x[2], x[3], x[4]$ $$(x * w)[2] = x[2] \cdot w[0] + x[3] \cdot w[1] + x[4] \cdot w[2]$$ $$= 3 \cdot 1 + 4 \cdot 0 + 5 \cdot (-1) = 3 + 0 - 5 = -2$$

**Результат:** $y = [-2, -2, -2]$

**Интерпретация:** Фильтр $[1, 0, -1]$ вычисляет разность $x[i] - x[i+2]$. Поскольку входной сигнал линейно возрастает с шагом 1, разность везде одинакова.

---

### Математическое определение 2D свёртки

Для изображений (двумерные данные):

$$(I * K)[i, j] = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} I[i+m, j+n] \cdot K[m, n]$$

где:

- $I$ — входное изображение размера $H \times W$
- $K$ — ядро (фильтр) размера $k_h \times k_w$
- $(I * K)[i, j]$ — значение выхода в позиции $(i, j)$

**Подробный пример расчёта 2D свёртки:**

Входное изображение $I$ (5×5):

```
┌─────────────────────┐
│  1   2   3   4   5  │
│  6   7   8   9  10  │
│ 11  12  13  14  15  │
│ 16  17  18  19  20  │
│ 21  22  23  24  25  │
└─────────────────────┘
```

Фильтр $K$ (3×3) — детектор горизонтальных краёв:

```
┌─────────────┐
│ -1  -1  -1  │
│  0   0   0  │
│  1   1   1  │
└─────────────┘
```

**Вычисление для позиции [0,0]:**

Накладываем фильтр на левый верхний угол (строки 0-2, столбцы 0-2):

```
Участок изображения:     Фильтр:
┌───────────┐           ┌─────────────┐
│ 1   2   3 │     ×     │ -1  -1  -1  │
│ 6   7   8 │           │  0   0   0  │
│ 11 12  13 │           │  1   1   1  │
└───────────┘           └─────────────┘
```

$(I * K)[0,0] = 1 \cdot (-1) + 2 \cdot (-1) + 3 \cdot (-1)$ $\quad\quad\quad\quad + 6 \cdot 0 + 7 \cdot 0 + 8 \cdot 0$ $\quad\quad\quad\quad + 11 \cdot 1 + 12 \cdot 1 + 13 \cdot 1$

$= (-1 - 2 - 3) + (0 + 0 + 0) + (11 + 12 + 13)$ $= -6 + 0 + 36 = 30$

**Вычисление для позиции [0,1]:**

Сдвигаем фильтр на 1 вправо (строки 0-2, столбцы 1-3):

```
Участок изображения:     Фильтр:
┌───────────┐           ┌─────────────┐
│ 2   3   4 │     ×     │ -1  -1  -1  │
│ 7   8   9 │           │  0   0   0  │
│ 12 13  14 │           │  1   1   1  │
└───────────┘           └─────────────┘
```

$(I * K)[0,1] = 2 \cdot (-1) + 3 \cdot (-1) + 4 \cdot (-1)$ $\quad\quad\quad\quad + 7 \cdot 0 + 8 \cdot 0 + 9 \cdot 0$ $\quad\quad\quad\quad + 12 \cdot 1 + 13 \cdot 1 + 14 \cdot 1$

$= (-2 - 3 - 4) + (0) + (12 + 13 + 14)$ $= -9 + 0 + 39 = 30$

**Вычисление для позиции [0,2]:**

$(I * K)[0,2] = 3 \cdot (-1) + 4 \cdot (-1) + 5 \cdot (-1)$ $\quad\quad\quad\quad + 8 \cdot 0 + 9 \cdot 0 + 10 \cdot 0$ $\quad\quad\quad\quad + 13 \cdot 1 + 14 \cdot 1 + 15 \cdot 1$

$= -12 + 0 + 42 = 30$

**Полное выходное изображение (3×3):**

```
┌─────────────┐
│ 30  30  30  │
│ 30  30  30  │
│ 30  30  30  │
└─────────────┘
```

**Почему все значения одинаковые?**

Наше входное изображение имеет постоянный вертикальный градиент (каждая строка больше предыдущей на 5). Фильтр вычисляет разницу между нижней и верхней строками в окне 3×3. Эта разница везде одинакова = 30.

---

### Примеры работы разных фильтров

**Картинки для поиска:** «convolution kernel examples», «Sobel filter visualization», «edge detection filters»

#### 1. Детектор вертикальных краёв (Sobel X)

```
Фильтр:
┌─────────────┐
│ -1   0   1  │
│ -2   0   2  │
│ -1   0   1  │
└─────────────┘
```

**Что делает:** даёт высокий положительный отклик там, где слева тёмные пиксели, справа светлые (переход тёмное→светлое по горизонтали).

**Пример:** если на изображении вертикальная линия (левая часть тёмная, правая светлая):

```
Вход:          После Sobel X:
0 0 0 1 1 1    0  0 +4  0  0
0 0 0 1 1 1    0  0 +4  0  0
0 0 0 1 1 1    0  0 +4  0  0
```

Фильтр «подсвечивает» вертикальную границу.

#### 2. Детектор горизонтальных краёв (Sobel Y)

```
Фильтр:
┌─────────────┐
│ -1  -2  -1  │
│  0   0   0  │
│  1   2   1  │
└─────────────┘
```

**Что делает:** даёт высокий отклик там, где сверху тёмное, снизу светлое.

#### 3. Размытие (Box Blur / Average)

```
Фильтр (каждый элемент = 1/9):
┌─────────────────────┐
│ 1/9   1/9   1/9     │
│ 1/9   1/9   1/9     │
│ 1/9   1/9   1/9     │
└─────────────────────┘
```

**Что делает:** заменяет каждый пиксель средним значением его соседей. Результат — размытое изображение.

**Пример расчёта:**

Если в окне 3×3 пиксели: $[10, 20, 30, 40, 50, 60, 70, 80, 90]$

Результат $= \frac{10+20+30+40+50+60+70+80+90}{9} = \frac{450}{9} = 50$

#### 4. Повышение резкости (Sharpen)

```
Фильтр:
┌─────────────┐
│  0  -1   0  │
│ -1   5  -1  │
│  0  -1   0  │
└─────────────┘
```

**Что делает:** усиливает центральный пиксель (коэффициент 5) и вычитает соседей. Это подчёркивает различия между пикселем и его окружением — края становятся более чёткими.

---

### Операция пулинга (Pooling)

**Идея:** уменьшить пространственный размер данных, сохранив важную информацию.

**Зачем нужен пулинг:**

1. Уменьшает количество параметров и вычислений
2. Создаёт инвариантность к небольшим сдвигам
3. Увеличивает рецептивное поле

#### Max Pooling (Максимальный пулинг)

Выбирает **максимальное** значение в каждом окне.

**Подробный пример Max Pooling 2×2 со stride 2:**

Вход (4×4):

```
┌─────────────────┐
│  1   3 │  2   4 │
│  5   6 │  1   2 │
├────────┼────────┤
│  1   2 │  3   4 │
│  0   1 │  7   8 │
└─────────────────┘
```

Разбиваем на 4 непересекающихся окна 2×2:

**Окно 1 (левый верхний квадрант):**

```
│ 1  3 │
│ 5  6 │
```

$\max(1, 3, 5, 6) = 6$

**Окно 2 (правый верхний квадрант):**

```
│ 2  4 │
│ 1  2 │
```

$\max(2, 4, 1, 2) = 4$

**Окно 3 (левый нижний квадрант):**

```
│ 1  2 │
│ 0  1 │
```

$\max(1, 2, 0, 1) = 2$

**Окно 4 (правый нижний квадрант):**

```
│ 3  4 │
│ 7  8 │
```

$\max(3, 4, 7, 8) = 8$

**Выход (2×2):**

```
┌───────┐
│ 6   4 │
│ 2   8 │
└───────┘
```

**Результат:** размер уменьшился с 4×4=16 до 2×2=4 элементов (в 4 раза!).

#### Average Pooling (Средний пулинг)

Вычисляет **среднее** значение в каждом окне.

**Для того же входа:**

**Окно 1:** $\text{avg}(1, 3, 5, 6) = \frac{15}{4} = 3.75$

**Окно 2:** $\text{avg}(2, 4, 1, 2) = \frac{9}{4} = 2.25$

**Окно 3:** $\text{avg}(1, 2, 0, 1) = \frac{4}{4} = 1.0$

**Окно 4:** $\text{avg}(3, 4, 7, 8) = \frac{22}{4} = 5.5$

**Выход (2×2):**

```
┌───────────┐
│ 3.75  2.25│
│ 1.0   5.5 │
└───────────┘
```

#### Global Average Pooling

**Идея:** усреднить всю карту признаков в одно число.

Вход: карта признаков $H \times W$ Выход: одно число (среднее всех элементов)

**Пример:**

```
Вход (3×3):        Выход:
┌───────────┐
│ 1  2  3   │
│ 4  5  6   │  →   5.0
│ 7  8  9   │
└───────────┘
```

$\text{GAP} = \frac{1+2+3+4+5+6+7+8+9}{9} = \frac{45}{9} = 5.0$

**Применение:** в современных архитектурах (ResNet, EfficientNet) вместо Flatten + FC используют Global Average Pooling — меньше параметров.

---

### Инвариантности и эквивариантности

Это **ключевые свойства**, которые делают CNN такими мощными для работы с изображениями!

#### Трансляционная эквивариантность свёртки

**Определение:** Если мы сдвинем вход, выход сдвинется на ту же величину.

**Математически:** $$\text{Conv}(\text{shift}(I, \Delta)) = \text{shift}(\text{Conv}(I), \Delta)$$

**Наглядный пример:**

Исходное изображение (кошка в левом углу):

```
┌─────────────┐
│ 🐱          │
│             │
│             │
└─────────────┘
```

После свёртки — карта активаций (детектор кошки):

```
┌─────────────┐
│ ★           │
│             │
│             │
└─────────────┘
```

(★ — высокая активация там, где была кошка)

Сдвинутое изображение (кошка в правом углу):

```
┌─────────────┐
│          🐱 │
│             │
│             │
└─────────────┘
```

После свёртки ТЕМ ЖЕ фильтром:

```
┌─────────────┐
│          ★  │
│             │
│             │
└─────────────┘
```

**Важный вывод:** один и тот же фильтр обнаруживает объект в **любом месте** изображения! Не нужно обучать отдельные фильтры для каждой позиции.

**Аналогия:** Когда вы ищете слово «кот» в книге, вам неважно, на какой странице и в какой строке оно написано. Вы используете одни и те же знания о буквах к, о, т везде.

#### Трансляционная инвариантность (благодаря пулингу)

**Определение:** Небольшие сдвиги входа **не меняют** выход.

**Математически:** $$\text{Pool}(\text{smallshift}(I)) \approx \text{Pool}(I)$$

**Наглядный пример с Max Pooling 2×2:**

Исходное изображение (яркий пиксель в позиции [0,0]):

```
┌─────────┐
│ 9  1 │ 0  0 │
│ 1  1 │ 0  0 │
└─────────┘
```

После Max Pool: $[\max(9,1,1,1), \max(0,0,0,0)] = [9, 0]$

Сдвинутое на 1 пиксель вправо (яркий пиксель в [0,1]):

```
┌─────────┐
│ 1  9 │ 0  0 │
│ 1  1 │ 0  0 │
└─────────┘
```

После Max Pool: $[\max(1,9,1,1), \max(0,0,0,0)] = [9, 0]$

**Результат одинаковый!** Пулинг «поглотил» сдвиг на 1 пиксель.

**Многоуровневая инвариантность:**

В глубокой CNN несколько слоёв пулинга накапливают инвариантность:

- После 1-го пулинга 2×2: инвариантность к сдвигам ~2 пикселя
- После 2-го пулинга 2×2: инвариантность к сдвигам ~4 пикселя
- После 3-го пулинга 2×2: инвариантность к сдвигам ~8 пикселей

**Аналогия:** Когда вы смотрите на друга с расстояния 10 метров, вы узнаёте его, даже если он сдвинется на полметра влево. Ваш «детектор друга» инвариантен к небольшим смещениям.

---

### Параметры свёртки

#### 1. Stride (Шаг)

**Stride** — на сколько пикселей сдвигается фильтр между операциями.

**Stride = 1 (по умолчанию):**

```
Шаг 1:  [###].....    → выход[0]
Шаг 2:  .[###]....    → выход[1]
Шаг 3:  ..[###]...    → выход[2]
Шаг 4:  ...[###]..    → выход[3]
```

**Stride = 2:**

```
Шаг 1:  [###].....    → выход[0]
Шаг 2:  ..[###]...    → выход[1]
Шаг 3:  ....[###].    → выход[2]
```

При stride = 2 выход в 2 раза меньше по каждому измерению.

#### 2. Padding (Дополнение)

**Padding** — добавление нулей по краям изображения перед свёрткой.

**Зачем нужен:**

1. Сохранить размер выхода равным входу
2. Не терять информацию с краёв изображения

**Пример без padding (valid):**

Вход 5×5, фильтр 3×3, stride 1:

```
Вход (5×5):              Выход (3×3):
┌─────────────┐          ┌─────────┐
│ . . . . .   │          │ . . .   │
│ . . . . .   │    →     │ . . .   │
│ . . . . .   │          │ . . .   │
│ . . . . .   │          └─────────┘
│ . . . . .   │
└─────────────┘
```

Выход **меньше** входа на $(k-1)$ с каждой стороны.

**Пример с padding = 1 (same):**

```
Вход с padding:              Выход:
┌─────────────────┐          ┌─────────────┐
│ 0 0 0 0 0 0 0   │          │ . . . . .   │
│ 0 . . . . . 0   │          │ . . . . .   │
│ 0 . . . . . 0   │    →     │ . . . . .   │
│ 0 . . . . . 0   │          │ . . . . .   │
│ 0 . . . . . 0   │          │ . . . . .   │
│ 0 . . . . . 0   │          └─────────────┘
│ 0 0 0 0 0 0 0   │
└─────────────────┘
```

Выход **такого же** размера, как вход.

#### 3. Формула размера выхода

$$H_{out} = \left\lfloor \frac{H_{in} - k + 2p}{s} \right\rfloor + 1$$

где:

- $H_{in}$ — высота входа
- $k$ — размер фильтра
- $p$ — padding
- $s$ — stride
- $\lfloor \cdot \rfloor$ — округление вниз

**Примеры расчёта:**

**Пример 1:** Вход 32×32, фильтр 5×5, stride 1, padding 0 $$H_{out} = \left\lfloor \frac{32 - 5 + 0}{1} \right\rfloor + 1 = 27 + 1 = 28$$

**Пример 2:** Вход 32×32, фильтр 5×5, stride 1, padding 2 $$H_{out} = \left\lfloor \frac{32 - 5 + 4}{1} \right\rfloor + 1 = 31 + 1 = 32$$ (same padding — размер сохраняется)

**Пример 3:** Вход 224×224, фильтр 7×7, stride 2, padding 3 $$H_{out} = \left\lfloor \frac{224 - 7 + 6}{2} \right\rfloor + 1 = \left\lfloor \frac{223}{2} \right\rfloor + 1 = 111 + 1 = 112$$

**Пример 4:** Вход 13×13, фильтр 3×3, stride 2, padding 0 $$H_{out} = \left\lfloor \frac{13 - 3 + 0}{2} \right\rfloor + 1 = \left\lfloor 5 \right\rfloor + 1 = 6$$

---

## Вопрос 14. Математическое описание свёртки. Детекторы признаков, фильтры

### Многоканальная свёртка

Реальные изображения имеют несколько каналов:

- **RGB изображение:** 3 канала (красный, зелёный, синий)
- **Карты признаков после свёртки:** много каналов (32, 64, 128, ...)

**Формула многоканальной свёртки:**

$$Y[i, j, f] = \sum_{c=0}^{C_{in}-1} \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} X[i+m, j+n, c] \cdot K[m, n, c, f] + b_f$$

где:

- $X$ — вход размера $H \times W \times C_{in}$
- $K$ — тензор фильтров размера $k_h \times k_w \times C_{in} \times C_{out}$
- $Y$ — выход размера $H' \times W' \times C_{out}$
- $b_f$ — смещение (bias) для фильтра $f$

**Как это работает — пошагово:**

1. Один фильтр имеет размер $k_h \times k_w \times C_{in}$ (охватывает ВСЕ входные каналы)
2. Этот фильтр создаёт ОДНУ выходную карту признаков
3. Чтобы получить $C_{out}$ выходных каналов, нужно $C_{out}$ разных фильтров

**Визуализация для RGB→64 канала:**

```
Вход:                    Фильтры:                 Выход:
                         (64 штуки)
┌─────────┐              ┌─────┐
│ R       │              │ K₁  │ (3×3×3)         ┌─────────┐
├─────────┤              ├─────┤                 │ 64      │
│ G       │      ×       │ K₂  │ (3×3×3)    =    │ каналов │
├─────────┤              │ ... │                 │         │
│ B       │              │ K₆₄ │ (3×3×3)         └─────────┘
└─────────┘              └─────┘
  3 канала             64 фильтра              64 канала
```

**Подробный пример:**

Вход: RGB изображение 32×32×3 Фильтры: 64 штуки, каждый размера 3×3×3 Stride: 1, Padding: 1 (same)

Размер выхода: 32×32×64

**Расчёт одного выходного пикселя $Y[0,0,0]$:**

```python
Y[0,0,0] = sum over c in [0,1,2]:   # RGB каналы
             sum over m in [0,1,2]:  # высота фильтра
               sum over n in [0,1,2]:  # ширина фильтра
                 X[0+m, 0+n, c] * K[m, n, c, 0]  # фильтр #0
           + b[0]  # bias для фильтра #0
```

Это 3×3×3 = 27 умножений + сложение + bias.

### Подсчёт параметров свёрточного слоя

**Формула:**

$$\text{Параметры} = k_h \times k_w \times C_{in} \times C_{out} + C_{out}$$

Где последнее слагаемое — смещения (biases).

**Пример 1:** Первый слой типичной CNN

- Вход: RGB, $C_{in} = 3$
- Фильтры: 3×3, количество $C_{out} = 64$

$$\text{Параметры} = 3 \times 3 \times 3 \times 64 + 64 = 1,728 + 64 = 1,792$$

**Пример 2:** Глубокий слой CNN

- Вход: $C_{in} = 256$ каналов
- Фильтры: 3×3, количество $C_{out} = 512$

$$\text{Параметры} = 3 \times 3 \times 256 \times 512 + 512 = 1,179,648 + 512 = 1,180,160$$

### Сравнение с полносвязным слоем

**Задача:** обработать изображение 224×224×3, получить 64 выходных признака

**Полносвязный слой:**

Каждый из 64 нейронов связан с каждым из $224 \times 224 \times 3 = 150,528$ входов.

$$\text{Параметры}_{FC} = 150,528 \times 64 + 64 = 9,633,856$$

**Свёрточный слой (3×3 фильтры):**

$$\text{Параметры}_{Conv} = 3 \times 3 \times 3 \times 64 + 64 = 1,792$$

**Свёрточный слой в 5,375 раз меньше!**

|Тип слоя|Параметры|Отношение|
|---|---|---|
|Полносвязный|9,633,856|1.0|
|Свёрточный|1,792|0.000186|

### Почему свёртка так эффективна

#### 1. Локальная связность (Local Connectivity)

Каждый нейрон «видит» только маленькую область входа (рецептивное поле).

**Полносвязный:** каждый нейрон видит ВСЁ изображение **Свёрточный:** каждый нейрон видит только 3×3 (или 5×5) область

#### 2. Разделение весов (Weight Sharing)

**Один и тот же фильтр** применяется ко всем позициям изображения.

Это значит:

- Фильтр, обнаруживающий глаз, найдёт глаз **везде**
- Не нужно учить отдельные детекторы для каждой позиции
- Резко меньше параметров

**Аналогия:** Когда вы научились читать букву «А», вы узнаёте её в любом месте страницы — в начале, в середине, в конце. Вам не нужно учить «А в левом верхнем углу», «А в центре» и т.д.

#### 3. Иерархия признаков

Глубокие слои строят сложные признаки из простых:

```
Слой 1: Края (вертикальные, горизонтальные, диагональные)
    ↓
Слой 2: Углы, дуги, простые текстуры
    ↓
Слой 3: Части объектов (глаза, колёса, окна)
    ↓
Слой 4: Целые объекты (лица, машины, здания)
```

**Картинки для поиска:** «CNN feature visualization», «deep learning feature hierarchy», «what neural networks see»

### Рецептивное поле (Receptive Field)

**Определение:** область входного изображения, которая влияет на один нейрон в данном слое.

**Рецептивное поле первого слоя** = размер фильтра.

Для фильтра 3×3: рецептивное поле = 3×3.

**Рецептивное поле накапливается:**

Два последовательных слоя с фильтрами 3×3:

- После слоя 1: RF = 3×3
- После слоя 2: RF = 5×5

Три слоя 3×3:

- RF = 7×7

**Формула рецептивного поля:**

$$RF_l = RF_{l-1} + (k_l - 1) \times \prod_{i=1}^{l-1} s_i$$

**Пример расчёта:**

Три слоя Conv 3×3 со stride 1:

$RF_1 = 3$ $RF_2 = 3 + (3-1) \times 1 = 3 + 2 = 5$ $RF_3 = 5 + (3-1) \times 1 = 5 + 2 = 7$

**Важный вывод:** три слоя 3×3 имеют такое же рецептивное поле, как один слой 7×7, но параметров **меньше**:

- Три слоя 3×3: $3 \times (3^2 \times C^2) = 27C^2$
- Один слой 7×7: $7^2 \times C^2 = 49C^2$

Это одна из причин, почему современные сети используют маленькие фильтры 3×3.

---

## Вопрос 15. Архитектура CNN: структура слоёв, примеры

### Типичная структура CNN

```
Вход → [CONV → ACT → POOL] × N → FLATTEN → [FC → ACT] × M → Выход
         ↑                                    ↑
   Свёрточная часть                    Классификатор
   (извлечение признаков)
```

**Разбор по элементам:**

|Элемент|Что делает|Пример|
|---|---|---|
|CONV|Извлекает признаки через фильтры|Conv2D(64, 3×3)|
|ACT|Вносит нелинейность|ReLU|
|POOL|Уменьшает размер|MaxPool 2×2|
|FLATTEN|3D → 1D|7×7×512 → 25088|
|FC|Полносвязный слой|Dense(4096)|

### Пример архитектуры: LeNet-5 (1998)

**Историческая справка:** одна из первых CNN, создана Яном Лекуном для распознавания рукописных цифр (почтовые индексы).

**Архитектура:**

```
Вход: 32×32×1 (чёрно-белое изображение)
         ↓
    CONV1: 6 фильтров 5×5
         ↓
    32×32×1 → 28×28×6
         ↓
    POOL1: Average 2×2, stride 2
         ↓
    28×28×6 → 14×14×6
         ↓
    CONV2: 16 фильтров 5×5
         ↓
    14×14×6 → 10×10×16
         ↓
    POOL2: Average 2×2, stride 2
         ↓
    10×10×16 → 5×5×16
         ↓
    FLATTEN: 5×5×16 = 400
         ↓
    FC1: 400 → 120
         ↓
    FC2: 120 → 84
         ↓
    FC3: 84 → 10 (10 цифр)
```

**Подсчёт параметров LeNet-5:**

|Слой|Вход → Выход|Расчёт|Параметры|
|---|---|---|---|
|CONV1|32×32×1 → 28×28×6|5×5×1×6 + 6|156|
|CONV2|14×14×6 → 10×10×16|5×5×6×16 + 16|2,416|
|FC1|400 → 120|400×120 + 120|48,120|
|FC2|120 → 84|120×84 + 84|10,164|
|FC3|84 → 10|84×10 + 10|850|
|**Всего**|||**61,706**|

---

### Пример архитектуры: VGG16 (2014)

**Ключевая идея:** использовать только маленькие фильтры 3×3, но много слоёв.

**Почему 3×3?**

Два слоя 3×3 имеют рецептивное поле 5×5. Три слоя 3×3 имеют рецептивное поле 7×7.

Но параметров меньше:

- Три слоя 3×3 (C каналов): $3 \times 3^2 \times C^2 = 27C^2$
- Один слой 7×7: $7^2 \times C^2 = 49C^2$

Экономия: 45%!

**Архитектура VGG16:**

```
Вход: 224×224×3
         ↓
[Conv3-64] × 2 → MaxPool → 112×112×64
         ↓
[Conv3-128] × 2 → MaxPool → 56×56×128
         ↓
[Conv3-256] × 3 → MaxPool → 28×28×256
         ↓
[Conv3-512] × 3 → MaxPool → 14×14×512
         ↓
[Conv3-512] × 3 → MaxPool → 7×7×512
         ↓
Flatten: 7×7×512 = 25,088
         ↓
FC: 25,088 → 4,096 → 4,096 → 1,000
```

**Подсчёт параметров VGG16:**

|Блок|Слои|Расчёт|Параметры|
|---|---|---|---|
|Block 1|2×Conv(3→64)|2×(3×3×3×64 + 64) = 2×1,792|3,584|
|Block 2|2×Conv(64→128)|2×(3×3×64×128 + 128)|221,440|
|Block 3|3×Conv(128→256)|сложнее...|1,475,328|
|Block 4|3×Conv(256→512)|...|5,899,776|
|Block 5|3×Conv(512→512)|...|7,079,424|
|FC1|25088→4096|25088×4096 + 4096|102,764,544|
|FC2|4096→4096|4096×4096 + 4096|16,781,312|
|FC3|4096→1000|4096×1000 + 1000|4,097,000|
|**Всего**|||**~138M**|

**Важное наблюдение:** ~90% параметров в FC слоях!

---

### Пример архитектуры: ResNet (2015)

**Проблема:** очень глубокие сети (50+ слоёв) плохо обучаются — градиенты затухают.

**Решение:** Residual Connections (пропускающие соединения).

**Формула residual block:**

$$\mathbf{y} = F(\mathbf{x}) + \mathbf{x}$$

где $F(\mathbf{x})$ — результат свёрток, $\mathbf{x}$ — вход (пропущен напрямую).

**Визуализация:**

```
        x
        │
        ├─────────────────────┐
        ↓                     │
   ┌─────────┐                │
   │ Conv 3×3│                │
   └────┬────┘                │
        ↓                     │
   ┌─────────┐                │
   │  ReLU   │                │
   └────┬────┘                │
        ↓                     │
   ┌─────────┐                │
   │ Conv 3×3│                │
   └────┬────┘                │
        ↓                     │
        ⊕ ←───────────────────┘
        │        Skip Connection
        ↓
   ┌─────────┐
   │  ReLU   │
   └────┬────┘
        ↓
        y = F(x) + x
```

**Почему это работает:**

1. **Градиент течёт напрямую:** через skip connection градиент идёт без умножения на веса — не затухает
    
2. **Легче обучить «ничего не делать»:** если оптимальное преобразование близко к тождественному, сети легче обучить $F(\mathbf{x}) \approx 0$, чем $F(\mathbf{x}) \approx \mathbf{x}$
    
3. **Глубина без деградации:** ResNet успешно обучается с 152 слоями!
    

**Сравнение:**

|Архитектура|Глубина|Top-5 Error (ImageNet)|
|---|---|---|
|VGG19|19|7.3%|
|ResNet-34|34|5.71%|
|ResNet-152|152|4.49%|

---

## Вопрос 16. Типы свёрток (1D, 2D, 3D). Применение вне компьютерного зрения

### 1D свёртка

**Применяется к:** одномерным последовательностям.

**Входные данные:** тензор размера $(N, C_{in}, L)$

- $N$ — размер батча
- $C_{in}$ — количество каналов (признаков в каждой позиции)
- $L$ — длина последовательности

**Фильтр:** размер $k$ (например, 3, 5, 7)

**Примеры данных для 1D свёртки:**

|Данные|Каналы ($C_{in}$)|Длина ($L$)|
|---|---|---|
|Аудио (моно)|1|Количество сэмплов|
|Аудио (стерео)|2|Количество сэмплов|
|Текст (word2vec)|300|Количество слов|
|Временной ряд|Число датчиков|Количество точек|
|ДНК|4 (A, C, G, T)|Длина последовательности|

**Пример: классификация текста с 1D CNN**

```
Текст: "I love this movie"
         ↓
Эмбеддинги: каждое слово → вектор 100D
         ↓
Матрица: 4 слова × 100 признаков
         ↓
Conv1D с фильтрами размера 3:
  - Смотрит на 3 слова одновременно
  - "I love this" → один признак
  - "love this movie" → другой признак
         ↓
GlobalMaxPooling: берём максимум по всей длине
         ↓
FC → Softmax: положительный/отрицательный
```

**Преимущество 1D CNN перед RNN:** параллельные вычисления, быстрее обучается.

---

### 2D свёртка

**Применяется к:** данным с двумя пространственными измерениями.

**Примеры:**

|Данные|Каналы|Размер|
|---|---|---|
|RGB изображение|3|H × W|
|Grayscale изображение|1|H × W|
|Спектрограмма|1|Время × Частота|
|Карты (GIS)|много|Lat × Lon|

**Спектрограмма — особый случай:**

Аудио → Спектрограмма → 2D CNN

```
Аудио сигнал (1D):  ∿∿∿∿∿∿∿∿∿∿
        ↓
   STFT (Short-Time Fourier Transform)
        ↓
Спектрограмма (2D): 
   Частота
      ↑
      │ ░░▓▓▓░░
      │ ░▓▓▓▓▓░
      │ ▓▓░░░▓▓
      └──────────→ Время
```

Теперь можно применять обычные 2D CNN как к изображению!

---

### 3D свёртка

**Применяется к:** данным с тремя пространственными измерениями (или 2D + время).

**Входные данные:** тензор размера $(N, C_{in}, D, H, W)$

- $D$ — глубина (или количество кадров для видео)

**Фильтр:** размер $k_d \times k_h \times k_w$ (например, 3×3×3)

**Примеры:**

|Данные|Каналы|Размер|
|---|---|---|
|Видео|3 (RGB)|Кадры × H × W|
|МРТ/КТ|1|Слайсы × H × W|
|3D модель (воксели)|1|X × Y × Z|

**Пример: анализ видео**

```
Видео: 16 кадров × 112×112 × 3
           ↓
Conv3D с фильтром 3×3×3:
  - 3 кадра по времени
  - 3×3 по пространству
           ↓
Извлекает движение и пространственные признаки одновременно!
```

**Применение в медицине:**

```
МРТ головного мозга: 155 слайсов × 240×240
           ↓
3D CNN анализирует объёмную структуру
           ↓
Детекция опухолей, анализ структур
```

---

### Специальные виды свёрток

#### Depthwise Separable Convolution

**Идея:** разделить свёртку на две части для уменьшения вычислений.

**Обычная свёртка:**

- Один фильтр размера $k \times k \times C_{in}$
- $C_{out}$ таких фильтров
- Параметры: $k^2 \times C_{in} \times C_{out}$

**Depthwise Separable:**

**Шаг 1 — Depthwise:** отдельный $k \times k$ фильтр для КАЖДОГО входного канала

- Параметры: $k^2 \times C_{in}$

**Шаг 2 — Pointwise:** $1 \times 1$ свёртка для комбинации каналов

- Параметры: $C_{in} \times C_{out}$

**Сравнение:**

Обычная: $k^2 \times C_{in} \times C_{out}$ Separable: $k^2 \times C_{in} + C_{in} \times C_{out}$

**Пример:**

$k=3$, $C_{in}=64$, $C_{out}=128$

Обычная: $9 \times 64 \times 128 = 73,728$ Separable: $9 \times 64 + 64 \times 128 = 576 + 8,192 = 8,768$

**Экономия:** в 8.4 раза меньше параметров!

**Используется в:** MobileNet, EfficientNet (для мобильных устройств).

#### Dilated (Atrous) Convolution

**Идея:** расширить рецептивное поле без увеличения параметров.

**Обычный фильтр 3×3:**

```
┌───────────┐
│ * │ * │ * │
├───┼───┼───┤
│ * │ * │ * │   Рецептивное поле: 3×3
├───┼───┼───┤
│ * │ * │ * │
└───────────┘
```

**Dilated 3×3 с dilation=2:**

```
┌───────────────────┐
│ * │   │ * │   │ * │
├───┼───┼───┼───┼───┤
│   │   │   │   │   │
├───┼───┼───┼───┼───┤   Рецептивное поле: 5×5
│ * │   │ * │   │ * │   Параметры: всё ещё 9!
├───┼───┼───┼───┼───┤
│   │   │   │   │   │
├───┼───┼───┼───┼───┤
│ * │   │ * │   │ * │
└───────────────────┘
```

**Используется в:** сегментация изображений (DeepLab), WaveNet (генерация аудио).

#### Transposed Convolution (Deconvolution)

**Идея:** увеличить пространственный размер (upsampling).

Обычная свёртка: уменьшает размер (или сохраняет) Transposed: увеличивает размер

**Пример:**

Обычная Conv 3×3, stride 2: 8×8 → 4×4 Transposed Conv 3×3, stride 2: 4×4 → 8×8

**Используется в:**

- Автоэнкодеры (декодер)
- GAN (генератор)
- Сегментация (U-Net)

---

# РАЗДЕЛ 5. РЕКУРРЕНТНЫЕ НЕЙРОННЫЕ СЕТИ (RNN)

---

## Вопрос 17. Архитектура простой RNN. Скрытое состояние как память

### Зачем нужны рекуррентные сети?

**Проблема:** обычные сети (MLP, CNN) работают с входом **фиксированного размера**.

А что если данные — это **последовательность** переменной длины?

**Примеры последовательностей:**

- Текст: предложения разной длины
- Аудио: записи разной продолжительности
- Временные ряды: данные датчиков
- Видео: разное количество кадров

**Наивный подход:** развернуть последовательность в один длинный вектор.

**Проблемы:**

1. Разные длины → разные размеры входа (нельзя в обычную сеть)
2. Порядок элементов не учитывается
3. Огромное количество параметров для длинных последовательностей

### Идея RNN — обработка с памятью

**Ключевая идея:** обрабатывать последовательность **элемент за элементом**, сохраняя информацию о прошлом в **скрытом состоянии** (hidden state).

```
Последовательность:  x₁    x₂    x₃    x₄
                      ↓     ↓     ↓     ↓
                    [RNN]→[RNN]→[RNN]→[RNN]
                      ↓  h₁ ↓  h₂ ↓  h₃ ↓
                     y₁    y₂    y₃    y₄
```

**Важно:** все блоки [RNN] — это **одна и та же** сеть с **одинаковыми весами**!

### Математические формулы простой RNN

**Обновление скрытого состояния:**

$$\mathbf{h}_t = \tanh(W_{hh} \mathbf{h}_{t-1} + W_{xh} \mathbf{x}_t + \mathbf{b}_h)$$

**Вычисление выхода:**

$$\mathbf{y}_t = W_{hy} \mathbf{h}_t + \mathbf{b}_y$$

**Обозначения:**

- $\mathbf{x}_t \in \mathbb{R}^d$ — вход на шаге $t$
- $\mathbf{h}_t \in \mathbb{R}^n$ — скрытое состояние на шаге $t$
- $\mathbf{y}_t \in \mathbb{R}^m$ — выход на шаге $t$
- $W_{xh} \in \mathbb{R}^{n \times d}$ — веса вход → скрытое
- $W_{hh} \in \mathbb{R}^{n \times n}$ — веса скрытое → скрытое (рекуррентные)
- $W_{hy} \in \mathbb{R}^{m \times n}$ — веса скрытое → выход
- $\mathbf{b}_h, \mathbf{b}_y$ — смещения

### Подробный пример расчёта

**Задача:** обработать последовательность $[2, 3, 1]$ простой RNN.

**Параметры (упрощённо, все скаляры для наглядности):**

- $W_{xh} = 0.5$ (вес для входа)
- $W_{hh} = 0.8$ (рекуррентный вес)
- $b_h = 0$ (смещение)
- $h_0 = 0$ (начальное скрытое состояние)

**Шаг 1:** $x_1 = 2$

$$h_1 = \tanh(W_{hh} \cdot h_0 + W_{xh} \cdot x_1 + b_h)$$ $$h_1 = \tanh(0.8 \cdot 0 + 0.5 \cdot 2 + 0)$$ $$h_1 = \tanh(0 + 1.0 + 0) = \tanh(1.0) = 0.762$$

**Шаг 2:** $x_2 = 3$

$$h_2 = \tanh(W_{hh} \cdot h_1 + W_{xh} \cdot x_2 + b_h)$$ $$h_2 = \tanh(0.8 \cdot 0.762 + 0.5 \cdot 3 + 0)$$ $$h_2 = \tanh(0.610 + 1.5) = \tanh(2.11) = 0.971$$

**Шаг 3:** $x_3 = 1$

$$h_3 = \tanh(W_{hh} \cdot h_2 + W_{xh} \cdot x_3 + b_h)$$ $$h_3 = \tanh(0.8 \cdot 0.971 + 0.5 \cdot 1 + 0)$$ $$h_3 = \tanh(0.777 + 0.5) = \tanh(1.277) = 0.855$$

**Результат:**

|Шаг $t$|Вход $x_t$|Скрытое $h_t$|
|---|---|---|
|0|—|0.000|
|1|2|0.762|
|2|3|0.971|
|3|1|0.855|

**Ключевое наблюдение:** $h_3 = 0.855$ зависит от **всей** предыдущей последовательности $(2, 3, 1)$, а не только от последнего входа!

### Скрытое состояние как память

**Что хранит $\mathbf{h}_t$?**

$\mathbf{h}_t$ — это «сжатое представление» всей истории $(x_1, x_2, ..., x_t)$.

Можно записать: $$\mathbf{h}_t = f(\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_t)$$

**Аналогия — чтение книги:**

Представьте, что вы читаете роман:

- $\mathbf{x}_t$ — слово, которое вы читаете прямо сейчас
- $\mathbf{h}_{t-1}$ — ваше понимание сюжета до этого момента
- $\mathbf{h}_t$ — ваше **обновлённое** понимание после прочтения нового слова

Ваше понимание:

1. Зависит от текущего слова ($\mathbf{x}_t$)
2. Зависит от предыдущего понимания ($\mathbf{h}_{t-1}$)
3. Обновляется с каждым словом

После прочтения всей книги $\mathbf{h}_T$ содержит ваше финальное понимание всего сюжета.

### Подсчёт параметров RNN

**Формула:**

$$\text{Параметры} = n \times d + n \times n + n + m \times n + m$$

где:

- $n \times d$ — матрица $W_{xh}$
- $n \times n$ — матрица $W_{hh}$
- $n$ — смещение $\mathbf{b}_h$
- $m \times n$ — матрица $W_{hy}$
- $m$ — смещение $\mathbf{b}_y$

**Упрощённо (без выходного слоя):**

$$\text{Параметры} = n \times (d + n + 1)$$

**Пример:**

Размер входа $d = 100$ (эмбеддинг слова) Размер скрытого $n = 256$

$$\text{Параметры} = 256 \times (100 + 256 + 1) = 256 \times 357 = 91,392$$

**Важно:** количество параметров **не зависит от длины последовательности!**

Последовательность длины 10 и длины 1000 обрабатываются сетью с одинаковым числом параметров.

### Типы задач для RNN

#### 1. Many-to-One (Много → Один)

**Вход:** последовательность **Выход:** один вектор (используем только $\mathbf{h}_T$ или $\mathbf{y}_T$)

```
x₁ → x₂ → x₃ → x₄
                 ↓
                 y
```

**Примеры:**

- Классификация текста: «Этот фильм отличный!» → Положительный
- Анализ тональности твита
- Предсказание по временному ряду: история цен → будет расти/падать

#### 2. One-to-Many (Один → Много)

**Вход:** один вектор **Выход:** последовательность

```
x
↓
y₁ → y₂ → y₃ → y₄
```

**Примеры:**

- Image Captioning: картинка → «Кошка сидит на диване»
- Генерация музыки по жанру
- Генерация текста по теме

#### 3. Many-to-Many (синхронный)

**Вход:** последовательность длины $T$ **Выход:** последовательность той же длины $T$

```
x₁ → x₂ → x₃ → x₄
↓    ↓    ↓    ↓
y₁   y₂   y₃   y₄
```

**Примеры:**

- Part-of-Speech tagging: слово → часть речи «Кошка» → существительное, «сидит» → глагол
- Named Entity Recognition (NER): «Москва» → ГОРОД
- Разметка видео: кадр → действие

#### 4. Many-to-Many (асинхронный) — Seq2Seq

**Вход:** последовательность длины $T_1$ **Выход:** последовательность длины $T_2$ (может отличаться!)

```
Encoder:  x₁ → x₂ → x₃ → [context]
                              ↓
Decoder:                 y₁ → y₂ → y₃ → y₄ → y₅
```

**Примеры:**

- Машинный перевод: «Hello world» → «Привет мир»
- Суммаризация: длинный текст → краткое изложение
- Чат-бот: вопрос → ответ (разной длины)

### Развёртка RNN во времени (Unrolling)

При обучении RNN «разворачивается» во времени:

```
     x₁         x₂         x₃         x₄
      ↓          ↓          ↓          ↓
h₀ →[RNN]→ h₁ →[RNN]→ h₂ →[RNN]→ h₃ →[RNN]→ h₄
      ↓          ↓          ↓          ↓
     y₁         y₂         y₃         y₄
```

**Важно:**

- Все блоки [RNN] используют **одни и те же веса** $W_{xh}$, $W_{hh}$, $W_{hy}$
- Это как очень глубокая сеть с разделяемыми весами
- Backpropagation идёт **справа налево** через все шаги — BPTT (Backpropagation Through Time)