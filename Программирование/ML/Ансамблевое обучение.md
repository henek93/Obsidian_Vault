Ансамблевое обучение - объединение моделей. Ансамблевое обучение помогает моделям не переобучаться.
![[Pasted image 20250925082710.png]]
Два вида основныз ансамблей:
![[Pasted image 20250925082738.png]]
# Бэггинг
Один из самых популярных моделей бэггинга это случайный лес.
Для этого мы разьиваем исходные данные, и для каждого кусочка обучем дерево решений.
![[Pasted image 20250925091330.png]]
После разбиения и обучения мы объединяем их путем голосования. И выбираем классификацию изъодя из того как много моделей сказали что это 0 или 1.
![[Pasted image 20250925091453.png]]
# AdaBoost - adaptive boosting
Мы также объединяем слабые модлеи в один сильный. Разница тольок в том, что мы делаем это боллее осознанно. Новые модели строятся на основе старых и закрывают их слабости.
Мы присваеваем каждой точке вес 1 и обчуаем слабый алгоритм(дерево решений с глубиной 1), после этого мы считаем коэффициент масштабирования(кол. правильно классифицированных точек на неправильно классифицированных точек) и изменяем вес неправильно классифицированных точек.
![[Pasted image 20250925093454.png]]
Потом повторяем эту процедуру.
![[Pasted image 20250925093740.png]]
### Коэффициент масштабирования - это отношение между суммами весов правильно и неправильно классифицированных точек
Процесс обчуния слабых алгоритмов продолжается сколько потребуется.
Суть AdaBoosting как раз заключается в том, чтобы объединить все эти слабые модели в один сильный, но сделать это нужно так, чтобы плохти моделям меньше доверяли, а хорошим больше.
Вот свойства этого числа коэффициента:
![[Pasted image 20250925094811.png]]
# Что такое шанс? И как его использовать для составления функции коэффициента
Шанс это почти тоже самое, что и вероятность. Напримере с кубиком будет понятнее:
Вероятность выпадения 1 на кубике == 1/6.
Шанс выпадения 1 на кубике == 1/5.
Вероятность это число от 0 до 1, то шансы от 0 до бесконечности

Но так как нам нужно не только положительные числа, но и отрицательные мы вынуждены использовать другую функцию. В которой используется логорифм и сама функция шанса.
![[Pasted image 20250925095722.png]]

Объединяем наши алгоритмы и получаем.
![[Pasted image 20250925100105.png]]
![[Pasted image 20250925100209.png]]

# Gradien boosting
Суть градиентного бустинга заключается в создание последовательности деревьев. 
Например первый слабый классификатор просто дерево решений глубиной 0, функция ошибки которую мы минимизируем это средняя квадратическая ошибка. Оптимальное значение меток равно среднемк значению меток.

![[Pasted image 20250925102438.png]]
![[Pasted image 20250925103632.png]]

![[Pasted image 20250925103836.png]]

# XGBoost - extreme gradient boosting

XGBoost также применяет деревья решений в качестве слабых алгоритмов. Почти все работает также  только добавляется *показатель подобия*, а также чтобы не переобучить модель мы подрезаем ветви деревьев, тем самы добавояй этап подрезки в алгоритм. 
### Показатель подообия XGBoost
![[Pasted image 20250925111027.png]]
Лучше рассмотрим пример:
![[Pasted image 20250925111045.png]]

![[Pasted image 20250925111059.png]]
### Также есть гиперпараметр
![[Pasted image 20250925111300.png]]
Алгоритм старается выбирать разделение с максимальным подобием.
![[Pasted image 20250925112958.png]]
# Прирост подобия
Прирост подобия помогает понять нужно ли вообще разделять на два узла.
Например если меньше 1 то не надо.
![[Pasted image 20250925113251.png]]
