# Полное руководство: От теории вероятностей до автоэнкодеров

## Содержание
1. [Основы теории вероятностей](#часть-1-основы-теории-вероятностей)
2. [Метод максимального правдоподобия](#часть-2-метод-максимального-правдоподобия)
3. [От MLE к функциям потерь](#часть-3-от-mle-к-функциям-потерь)
4. [Теория информации](#часть-4-теория-информации)
5. [Автоэнкодеры: базовая теория](#часть-5-автоэнкодеры-базовая-теория)
6. [Специальные типы автоэнкодеров](#часть-6-специальные-типы-автоэнкодеров)

---

## Часть 1: Основы теории вероятностей

### 1.1 Математическое ожидание (среднее значение)

**Что это такое простыми словами:**
Представь, что ты бросаешь кубик много-много раз и записываешь результаты. Математическое ожидание — это число, вокруг которого будут "крутиться" твои результаты в среднем.

**Формула для дискретной случайной величины:**

$$E[X] = \sum_{i} x_i \cdot P(x_i)$$

Где:
- $x_i$ — возможные значения
- $P(x_i)$ — вероятность каждого значения

**Пример с кубиком:**

$$E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = 3.5$$

**Для непрерывной величины:**

$$E[X] = \int_{-\infty}^{\infty} x \cdot p(x) \, dx$$

Где $p(x)$ — функция плотности вероятности.

**Источник:** Колмогоров А.Н. "Основные понятия теории вероятностей" (классический учебник, доступен в любом университетском курсе ТерВера)

---

### 1.2 Дисперсия

**Что это такое простыми словами:**
Дисперсия показывает, насколько сильно значения "разбросаны" вокруг среднего. Если дисперсия маленькая — значения кучкуются близко к среднему. Если большая — разброс огромный.

**Формула:**

$$\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$

**Расшифровка:**
1. Берём каждое значение
2. Вычитаем среднее (получаем отклонение)
3. Возводим в квадрат (чтобы убрать знак)
4. Усредняем все эти квадраты отклонений

**Пример с кубиком:**

$$E[X] = 3.5$$

$$E[X^2] = 1^2 \cdot \frac{1}{6} + 2^2 \cdot \frac{1}{6} + \ldots + 6^2 \cdot \frac{1}{6} = \frac{91}{6}$$

$$\text{Var}(X) = \frac{91}{6} - (3.5)^2 \approx 2.92$$

**Стандартное отклонение:**

$$\sigma = \sqrt{\text{Var}(X)}$$

Это просто квадратный корень из дисперсии. Удобно, потому что имеет те же единицы измерения, что и сами данные.

**Источник:** DeGroot & Schervish, "Probability and Statistics" (4th edition), Chapter 4

---

### 1.3 Нормальное (гауссовское) распределение

**Что это такое простыми словами:**
Это самое важное распределение в статистике. Оно описывает множество естественных явлений: рост людей, ошибки измерений, результаты экзаменов и так далее. График похож на колокол.

**Функция плотности вероятности:**

$$p(x) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

Где:
- $\mu$ (мю) — математическое ожидание (центр "колокола")
- $\sigma$ (сигма) — стандартное отклонение (ширина "колокола")
- $\pi \approx 3.14159\ldots$
- $\exp(\ldots) = e^{(\ldots)}$

**Обозначение:**

$$X \sim \mathcal{N}(\mu, \sigma^2)$$

Читается: "X распределена нормально с матожиданием $\mu$ и дисперсией $\sigma^2$"

**Стандартное нормальное распределение:**

$$X \sim \mathcal{N}(0, 1)$$

Это частный случай с $\mu=0$ и $\sigma=1$.

**Важное правило "трёх сигм":**
- 68% значений лежат в интервале $[\mu-\sigma, \mu+\sigma]$
- 95% значений лежат в интервале $[\mu-2\sigma, \mu+2\sigma]$
- 99.7% значений лежат в интервале $[\mu-3\sigma, \mu+3\sigma]$

**Источник:** 
- Bishop C. "Pattern Recognition and Machine Learning", Section 2.3
- Гмурман В.Е. "Теория вероятностей и математическая статистика", глава 5

---

## Часть 2: Метод максимального правдоподобия

### 2.1 Интуиция

**Простыми словами:**
У тебя есть данные, и ты хочешь подобрать модель (например, нормальное распределение), которая лучше всего объясняет эти данные. Метод максимального правдоподобия (Maximum Likelihood Estimation, MLE) говорит: "Выбери такие параметры модели, при которых наблюдаемые данные были бы наиболее вероятными".

### 2.2 Функция правдоподобия (Likelihood)

**Для одного наблюдения:**

$$L(\theta \mid x) = p(x \mid \theta)$$

Где:
- $\theta$ (тета) — параметры модели
- $x$ — наблюдаемые данные
- $p(x|\theta)$ — вероятность увидеть $x$ при параметрах $\theta$

**Для нескольких независимых наблюдений $x_1, x_2, \ldots, x_n$:**

$$L(\theta \mid x_1,\ldots,x_n) = \prod_{i=1}^{n} p(x_i \mid \theta) = p(x_1|\theta) \cdot p(x_2|\theta) \cdot \ldots \cdot p(x_n|\theta)$$

### 2.3 Логарифм правдоподобия

**Проблема:** Произведения неудобны для оптимизации.

**Решение:** Берём логарифм (он монотонен, поэтому максимум сохраняется):

$$\log L(\theta) = \sum_{i=1}^{n} \log p(x_i \mid \theta)$$

**Преимущества:**
1. Произведение превращается в сумму
2. Числа не становятся слишком маленькими (нет проблемы с underflow)
3. Проще брать производные

### 2.4 Пример: оценка параметров нормального распределения

**Задача:** У нас есть выборка $x_1, \ldots, x_n$ из $\mathcal{N}(\mu, \sigma^2)$. Нужно найти $\mu$ и $\sigma^2$.

**Шаг 1: Записываем правдоподобие**

$$L(\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$$

**Шаг 2: Берём логарифм**

$$\log L = \sum_{i=1}^{n} \left[\log\left(\frac{1}{\sigma\sqrt{2\pi}}\right) - \frac{(x_i-\mu)^2}{2\sigma^2}\right]$$

$$= -n \cdot \log(\sigma) - \frac{n}{2} \cdot \log(2\pi) - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i-\mu)^2$$

**Шаг 3: Берём производные и приравниваем к нулю**

По $\mu$:

$$\frac{\partial(\log L)}{\partial\mu} = \frac{1}{\sigma^2} \sum_{i=1}^{n}(x_i-\mu) = 0$$

$$\Rightarrow \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i \quad \text{(выборочное среднее!)}$$

По $\sigma^2$:

$$\frac{\partial(\log L)}{\partial(\sigma^2)} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n}(x_i-\mu)^2 = 0$$

$$\Rightarrow \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n}(x_i-\hat{\mu})^2 \quad \text{(выборочная дисперсия!)}$$

**Вывод:** Метод максимального правдоподобия естественным образом даёт нам знакомые формулы для среднего и дисперсии!

**Источник:** 
- Murphy K. "Machine Learning: A Probabilistic Perspective", Chapter 9.2
- Casella & Berger, "Statistical Inference" (2nd ed), Chapter 7

---

## Часть 3: От MLE к функциям потерь

### 3.1 Связь между MLE и минимизацией потерь

**Ключевая идея:**

$$\text{Максимизировать } \log L(\theta) \Leftrightarrow \text{Минимизировать } -\log L(\theta)$$

Мы переходим от **максимизации правдоподобия** к **минимизации потерь**:

$$\mathcal{L} = -\log L(\theta) = -\sum_{i=1}^{n} \log p(x_i \mid \theta)$$

### 3.2 Вывод MSE (Mean Squared Error)

**Предположим:** Данные порождаются как:

$$y = f(x; \theta) + \varepsilon, \quad \text{где } \varepsilon \sim \mathcal{N}(0, \sigma^2)$$

Это значит:

$$p(y \mid x, \theta) = \mathcal{N}(y \mid f(x;\theta), \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left(-\frac{(y - f(x;\theta))^2}{2\sigma^2}\right)$$

**Логарифм правдоподобия:**

$$\log p(y \mid x, \theta) = -\log(\sigma) - \frac{1}{2}\log(2\pi) - \frac{(y - f(x;\theta))^2}{2\sigma^2}$$

**Для выборки из $n$ точек:**

$$\log L = \sum_{i=1}^{n} \log p(y_i \mid x_i, \theta) = \text{const} - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(y_i - f(x_i;\theta))^2$$

**Максимизация $\log L$ эквивалентна минимизации:**

$$\mathcal{L} = \frac{1}{2} \sum_{i=1}^{n}(y_i - f(x_i;\theta))^2$$

**Усредняя по выборке:**

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**Вывод:** MSE — это функция потерь, которая соответствует предположению о гауссовском шуме!

**Источник:** 
- Bishop C. "Pattern Recognition and Machine Learning", Section 3.1
- Goodfellow et al., "Deep Learning", Section 5.5

---

### 3.3 Вывод Cross-Entropy (кросс-энтропия)

**Применение:** Классификация с дискретными метками.

**Предположим:** Выход модели — вероятности классов:

$$p(y = k \mid x, \theta) = \text{softmax}_k(f(x; \theta))$$

**Правдоподобие для одного примера:**

$$p(y \mid x, \theta) = \prod_{k} p(y=k \mid x, \theta)^{y_k}$$

Где $y_k$ — one-hot кодировка ($y_k=1$ для правильного класса, 0 для остальных).

**Логарифм:**

$$\log p(y \mid x, \theta) = \sum_{k} y_k \cdot \log p(y=k \mid x, \theta)$$

**Negative log-likelihood (функция потерь):**

$$\mathcal{L} = -\sum_{k} y_k \cdot \log p(y=k \mid x, \theta)$$

Это и есть **кросс-энтропия**!

**Для бинарной классификации (Binary Cross-Entropy):**

$$\mathcal{L} = -[y \cdot \log(\hat{y}) + (1-y) \cdot \log(1-\hat{y})]$$

**Источник:** 
- Murphy K. "Probabilistic Machine Learning: An Introduction", Chapter 10.2
- Nielsen M. "Neural Networks and Deep Learning", Chapter 3

---

## Часть 4: Теория информации

### 4.1 Энтропия

**Что это такое простыми словами:**
Энтропия измеряет "неопределённость" или "информационное содержание" случайной величины. Чем более случайна величина, тем выше её энтропия.

**Формула энтропии Шеннона:**

$$H(X) = -\sum_{i} p(x_i) \log_2 p(x_i)$$

Измеряется в битах (если используется $\log_2$).

**Пример:**
- Честная монета: $H = -[0.5 \log_2(0.5) + 0.5 \log_2(0.5)] = 1$ бит
- Несправедливая монета (99% орёл): $H \approx 0.08$ бит (почти нет неопределённости)

**Свойства:**
- $H(X) \geq 0$ (всегда неотрицательна)
- $H(X) = 0$ тогда и только тогда, когда $X$ детерминирована
- Максимальна для равномерного распределения

**Источник:** 
- Shannon C.E. "A Mathematical Theory of Communication" (1948) — оригинальная статья
- Cover & Thomas, "Elements of Information Theory" (2nd ed), Chapter 2

---

### 4.2 Кросс-энтропия (Cross-Entropy)

**Определение:**
Кросс-энтропия между двумя распределениями $p$ и $q$ измеряет, сколько битов в среднем нужно для кодирования событий из распределения $p$, если мы используем оптимальный код для распределения $q$.

**Формула:**

$$H(p, q) = -\sum_{i} p(x_i) \log q(x_i)$$

**В машинном обучении:**
- $p$ — истинное распределение (метки данных)
- $q$ — предсказанное распределение (выход модели)

**Связь с энтропией:**

$$H(p, q) = H(p) + D_{KL}(p \parallel q)$$

Где $D_{KL}$ — дивергенция Кульбака-Лейблера.

**Почему минимизируем кросс-энтропию?**
Потому что $H(p)$ — константа (истинное распределение фиксировано), поэтому минимизация $H(p,q)$ эквивалентна минимизации $D_{KL}(p \parallel q)$ — то есть мы делаем $q$ максимально похожим на $p$!

**Источник:** 
- Goodfellow et al., "Deep Learning", Section 3.13
- Murphy K. "Probabilistic Machine Learning", Section 6.1

---

### 4.3 Дивергенция Кульбака-Лейблера (KL Divergence)

**Определение:**
KL-дивергенция измеряет "расстояние" между двумя вероятностными распределениями (хотя технически это не метрика, так как несимметрична).

**Формула:**

$$D_{KL}(p \parallel q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)} = \sum_{i} p(x_i) [\log p(x_i) - \log q(x_i)]$$

**Для непрерывных распределений:**

$$D_{KL}(p \parallel q) = \int p(x) \log \frac{p(x)}{q(x)} \, dx$$

**Свойства:**
- $D_{KL}(p \parallel q) \geq 0$ (всегда неотрицательна)
- $D_{KL}(p \parallel q) = 0$ тогда и только тогда, когда $p = q$ почти всюду
- Несимметрична: $D_{KL}(p \parallel q) \neq D_{KL}(q \parallel p)$

**Для двух нормальных распределений:**

$$D_{KL}(\mathcal{N}(\mu_1, \sigma_1^2) \parallel \mathcal{N}(\mu_2, \sigma_2^2)) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$

**Для многомерного случая (используется в VAE):**

$$D_{KL}(\mathcal{N}(\mu, \Sigma) \parallel \mathcal{N}(0, I)) = \frac{1}{2} \sum_{j=1}^{d} \left[\mu_j^2 + \sigma_j^2 - \log(\sigma_j^2) - 1\right]$$

**Источник:** 
- Cover & Thomas, "Elements of Information Theory", Chapter 2
- Kingma & Welling, "Auto-Encoding Variational Bayes" (2013)

---

### 4.4 Минимизация ошибки восстановления

**Постановка задачи:**
Имеем данные $\mathbf{x} \in \mathbb{R}^d$ высокой размерности. Хотим найти сжатое представление $\mathbf{z} \in \mathbb{R}^k$, где $k \ll d$, так чтобы можно было восстановить $\mathbf{x}$ с минимальной ошибкой.

**Математическая формулировка:**

$$\min_{\mathbf{z}, f, g} \mathbb{E}[\|\mathbf{x} - g(f(\mathbf{x}))\|^2]$$

Где:
- $f: \mathbb{R}^d \to \mathbb{R}^k$ — функция кодирования (encoder)
- $g: \mathbb{R}^k \to \mathbb{R}^d$ — функция декодирования (decoder)
- $\mathbf{z} = f(\mathbf{x})$ — латентное представление

**Связь с теорией информации:**
Минимизация ошибки восстановления эквивалентна максимизации взаимной информации между $\mathbf{x}$ и $\mathbf{z}$:

$$I(\mathbf{x}; \mathbf{z}) = H(\mathbf{x}) - H(\mathbf{x} \mid \mathbf{z})$$

Чем меньше условная энтропия $H(\mathbf{x} \mid \mathbf{z})$, тем лучше мы можем восстановить $\mathbf{x}$ по $\mathbf{z}$.

**Источник:**
- Hinton & Salakhutdinov, "Reducing the Dimensionality of Data with Neural Networks", Science (2006)
- Tishby & Zaslavsky, "Deep Learning and the Information Bottleneck Principle" (2015)

---

## Часть 5: Автоэнкодеры — базовая теория

### 5.1 Что такое автоэнкодер

**Простыми словами:**
Автоэнкодер — это нейронная сеть, которая учится сжимать данные (например, картинку) в компактное представление, а потом восстанавливать их обратно. Это как архиватор для данных, но обученный на конкретном типе данных.

**Архитектура:**

$$\mathbf{x} \xrightarrow{\text{Encoder}} \mathbf{z} \xrightarrow{\text{Decoder}} \hat{\mathbf{x}}$$

### 5.2 Математическая формулировка

**Компоненты:**

1. **Энкодер (Encoder):**
   $$\mathbf{z} = f_{\text{enc}}(\mathbf{x}; \theta_{\text{enc}})$$
   Сжимает входные данные $\mathbf{x} \in \mathbb{R}^d$ в компактное представление $\mathbf{z} \in \mathbb{R}^k$, где $k \ll d$.

2. **Декодер (Decoder):**
   $$\hat{\mathbf{x}} = f_{\text{dec}}(\mathbf{z}; \theta_{\text{dec}})$$
   Восстанавливает данные из латентного представления.

3. **Полное преобразование:**
   $$\hat{\mathbf{x}} = f_{\text{dec}}(f_{\text{enc}}(\mathbf{x}; \theta_{\text{enc}}); \theta_{\text{dec}})$$

**Параметры:**
- $\theta_{\text{enc}}$ — веса энкодера
- $\theta_{\text{dec}}$ — веса декодера
- $\theta = \{\theta_{\text{enc}}, \theta_{\text{dec}}\}$ — все параметры модели

---

### 5.3 Функции потерь для автоэнкодера

**Общий принцип:**
Цель — минимизировать ошибку восстановления (reconstruction error):

$$\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(\mathbf{x}_i, \hat{\mathbf{x}}_i)$$

**Варианты функции потерь:**

**1. Mean Squared Error (MSE) — для непрерывных данных:**

$$L_{\text{MSE}}(\mathbf{x}, \hat{\mathbf{x}}) = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 = \sum_{j=1}^{d} (x_j - \hat{x}_j)^2$$

**Когда использовать:** Изображения с пикселями в диапазоне $[0, 255]$ или нормализованные в $[-1, 1]$.

**2. Binary Cross-Entropy (BCE) — для бинарных данных:**

$$L_{\text{BCE}}(\mathbf{x}, \hat{\mathbf{x}}) = -\sum_{j=1}^{d} [x_j \log(\hat{x}_j) + (1-x_j) \log(1-\hat{x}_j)]$$

**Когда использовать:** Бинарные изображения или данные с пикселями нормализованными в $[0, 1]$.

**3. Mean Absolute Error (MAE):**

$$L_{\text{MAE}}(\mathbf{x}, \hat{\mathbf{x}}) = \sum_{j=1}^{d} |x_j - \hat{x}_j|$$

**Когда использовать:** Когда нужна робастность к выбросам (MAE менее чувствительна к выбросам, чем MSE).

---

### 5.4 Вывод функции потерь через MLE

**Для непрерывных данных (MSE):**

Предполагаем модель генерации:

$$p(\mathbf{x} \mid \mathbf{z}) = \mathcal{N}(\mathbf{x} \mid f_{\text{dec}}(\mathbf{z}), \sigma^2 I)$$

Логарифм правдоподобия:

$$\log p(\mathbf{x} \mid \mathbf{z}) = \text{const} - \frac{1}{2\sigma^2} \|\mathbf{x} - f_{\text{dec}}(\mathbf{z})\|^2$$

Максимизация $\log p(\mathbf{x} \mid \mathbf{z})$ эквивалентна минимизации MSE!

**Для бинарных данных (BCE):**

Предполагаем независимые распределения Бернулли для каждого пикселя:

$$p(\mathbf{x} \mid \mathbf{z}) = \prod_{j=1}^{d} [f_{\text{dec}}(\mathbf{z})_j]^{x_j} \cdot [1 - f_{\text{dec}}(\mathbf{z})_j]^{1-x_j}$$

Логарифм:

$$\log p(\mathbf{x} \mid \mathbf{z}) = \sum_{j=1}^{d} [x_j \log(f_{\text{dec}}(\mathbf{z})_j) + (1-x_j) \log(1-f_{\text{dec}}(\mathbf{z})_j)]$$

Максимизация эквивалентна минимизации BCE!

**Источник:**
- Kingma & Welling, "Auto-Encoding Variational Bayes" (2013): https://arxiv.org/abs/1312.6114
- Bishop C. "Pattern Recognition and Machine Learning", Chapter 12

---

### 5.5 Понижение размерности

**Зачем нужно?**

1. **Вычислительная эффективность:** Работа с данными меньшей размерности быстрее
2. **Визуализация:** Можно визуализировать в 2D или 3D
3. **Обобщение:** Уменьшение размерности может действовать как регуляризация
4. **Извлечение признаков:** Латентное пространство может содержать более значимые признаки

**Сравнение методов:**

| Метод | Тип | Линейность | Плюсы | Минусы |
|-------|-----|------------|-------|--------|
| PCA | Классический | Линейный | Быстрый, интерпретируемый | Только линейные зависимости |
| t-SNE | Классический | Нелинейный | Хорошая визуализация | Медленный, не обобщает на новые данные |
| Автоэнкодер | Нейронная сеть | Нелинейный | Гибкий, обобщает | Требует обучения, может переобучиться |

**Размерность латентного пространства:**

$$k = \text{размерность}(\mathbf{z}) \ll d = \text{размерность}(\mathbf{x})$$

Типичные значения:
- Для MNIST (784 пикселя): $k = 2, 10, 32$
- Для ImageNet ($224 \times 224 \times 3 = 150528$): $k = 128, 256, 512$

**Информационное узкое место (Information Bottleneck):**

Латентный слой с малой размерностью заставляет сеть выучить наиболее важные признаки данных, отбрасывая шум и детали.

**Источник:**
- Hinton & Salakhutdinov, "Reducing the Dimensionality of Data with Neural Networks", Science 313 (2006)
- Tishby & Zaslavsky, "Deep Learning and the Information Bottleneck Principle" (2015)

---

### 5.6 Применения автоэнкодеров

#### 5.6.1 Снижение размерности

**Пример: Визуализация MNIST**
```python
# Архитектура
Encoder: 784 → 512 → 256 → 2  # сжимаем до 2D для визуализации
Decoder: 2 → 256 → 512 → 784  # восстанавливаем
```

После обучения можно построить scatter plot латентного пространства, где разные цифры кластеризуются.

**Применение:**
- Визуализация высокоразмерных данных
- Предобработка для других алгоритмов
- Поиск похожих изображений

---

#### 5.6.2 Шумоподавление (Denoising)

**Идея:** Обучаем автоэнкодер восстанавливать чистое изображение из зашумлённого.

**Процесс:**
1. Берём чистое изображение $\mathbf{x}$
2. Добавляем шум: $\tilde{\mathbf{x}} = \mathbf{x} + \mathbf{n}$, где $\mathbf{n} \sim \mathcal{N}(0, \sigma^2 I)$
3. Обучаем: $\hat{\mathbf{x}} = f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}}))$
4. Функция потерь: $\mathcal{L} = \|\mathbf{x} - \hat{\mathbf{x}}\|^2$ (сравниваем с чистым!)

**Математическая формулировка:**

$$\min_{\theta} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}, \mathbf{n} \sim \mathcal{N}(0, \sigma^2 I)} [\|\mathbf{x} - f_{\theta}(\mathbf{x} + \mathbf{n})\|^2]$$

**Применение:**
- Восстановление старых фотографий
- Медицинская визуализация (удаление шума с рентгеновских снимков)
- Улучшение качества аудио

**Источник:**
- Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders" (2008)

---

#### 5.6.3 Генерация данных

**Проблема обычного автоэнкодера:**
Латентное пространство может быть "дырявым" — есть области, где декодер выдаёт бессмыслицу.

**Решение — Вариационный автоэнкодер (VAE):**
Обучаем латентное пространство быть непрерывным и структурированным.

**Процесс генерации:**
1. Сэмплируем $\mathbf{z} \sim \mathcal{N}(0, I)$ из стандартного нормального распределения
2. Генерируем: $\mathbf{x}_{\text{new}} = f_{\text{dec}}(\mathbf{z})$

**Применение:**
- Генерация новых изображений лиц
- Создание синтетических медицинских данных для обучения
- Генерация молекулярных структур в химии

---

## Часть 6: Специальные типы автоэнкодеров

### 6.1 Разрежённые автоэнкодеры (Sparse Autoencoders)

#### 6.1.1 Идея

**Проблема стандартного автоэнкодера:**
Если латентный слой имеет размерность $k \geq d$, автоэнкодер может выучить identity mapping (просто копирование входа), не извлекая полезных признаков.

**Решение:**
Добавляем ограничение разреженности (sparsity constraint) — большинство нейронов в латентном слое должны быть неактивны (близки к нулю).

**Аналогия:**
Представь, что ты описываешь картину. Вместо того, чтобы перечислять все детали (все нейроны активны), ты выделяешь только ключевые элементы (только несколько нейронов активны).

---

#### 6.1.2 Математическая формулировка

**Функция потерь:**

$$\mathcal{L}(\theta) = \underbrace{\frac{1}{n} \sum_{i=1}^{n} \|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2}_{\text{Reconstruction Loss}} + \underbrace{\lambda \sum_{j=1}^{k} \Omega(\hat{\rho}_j)}_{\text{Sparsity Penalty}}$$

Где:
- $\lambda > 0$ — коэффициент регуляризации (контролирует силу ограничения разреженности)
- $\hat{\rho}_j$ — средняя активация $j$-го нейрона латентного слоя:

$$\hat{\rho}_j = \frac{1}{n} \sum_{i=1}^{n} z_{ij}$$

где $z_{ij}$ — активация $j$-го нейрона для $i$-го примера

**Функция штрафа $\Omega$:**

**Вариант 1: L1-регуляризация**

$$\Omega(\hat{\rho}_j) = |\hat{\rho}_j|$$

Полная функция потерь:

$$\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2 + \lambda \sum_{j=1}^{k} |\hat{\rho}_j|$$

**Вариант 2: KL-дивергенция**

Задаём целевой уровень разреженности $\rho$ (например, $\rho = 0.05$ — хотим, чтобы в среднем только 5% нейронов были активны).

$$\Omega(\hat{\rho}_j) = D_{KL}(\rho \parallel \hat{\rho}_j) = \rho \log \frac{\rho}{\hat{\rho}_j} + (1-\rho) \log \frac{1-\rho}{1-\hat{\rho}_j}$$

Полная функция потерь:

$$\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2 + \lambda \sum_{j=1}^{k} D_{KL}(\rho \parallel \hat{\rho}_j)$$

**Интуиция KL-дивергенции:**
Мы хотим, чтобы распределение активаций $\hat{\rho}_j$ было близко к целевому распределению Бернулли с параметром $\rho$. KL-дивергенция штрафует отклонения от этого целевого распределения.

---

#### 6.1.3 Эффект разреженности

**Без регуляризации:**
- Многие нейроны активны одновременно
- Признаки могут быть избыточны
- Латентное представление трудно интерпретировать

**С регуляризацией разреженности:**
- Для каждого входа активны только несколько нейронов
- Каждый нейрон специализируется на определённом признаке
- Лучшая интерпретируемость (можно понять, что кодирует каждый нейрон)

**Пример на изображениях:**
Без разреженности: нейроны могут представлять смесь признаков
С разреженностью: отдельные нейроны могут выучить детекторы:
- Нейрон 1: вертикальные линии
- Нейрон 2: горизонтальные линии  
- Нейрон 3: диагонали
- Нейрон 4: углы

---

#### 6.1.4 Градиенты для обучения

**Для L1-регуляризации:**

$$\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial}{\partial \theta}\left[\text{Reconstruction Loss}\right] + \lambda \frac{\partial}{\partial \theta}\left[\sum_{j=1}^{k} |\hat{\rho}_j|\right]$$

**Для KL-дивергенции:**

$$\frac{\partial D_{KL}(\rho \parallel \hat{\rho}_j)}{\partial \hat{\rho}_j} = -\frac{\rho}{\hat{\rho}_j} + \frac{1-\rho}{1-\hat{\rho}_j}$$

Затем применяем chain rule для получения градиентов по весам.

**Источник:**
- Ng A. "Sparse Autoencoder", CS294A Lecture Notes (Stanford)
- Goodfellow et al., "Deep Learning", Section 14.1

---

### 6.2 Denoising Autoencoders (шумоподавляющие автоэнкодеры)

#### 6.2.1 Идея и мотивация

**Проблема обычного автоэнкодера:**
Может просто запомнить identity function, особенно если латентная размерность большая.

**Решение:**
Заставляем автоэнкодер учиться робастным признакам, которые устойчивы к шуму.

**Философия:**
"Хорошее представление данных должно быть устойчиво к небольшим возмущениям"

---

#### 6.2.2 Математическая формулировка

**Процесс:**

1. **Коррупция входа:**
   $$\tilde{\mathbf{x}} \sim q(\tilde{\mathbf{x}} \mid \mathbf{x})$$
   
   где $q$ — распределение шума.

2. **Кодирование зашумлённого входа:**
   $$\mathbf{z} = f_{\text{enc}}(\tilde{\mathbf{x}}; \theta_{\text{enc}})$$

3. **Декодирование:**
   $$\hat{\mathbf{x}} = f_{\text{dec}}(\mathbf{z}; \theta_{\text{dec}})$$

4. **Функция потерь (сравниваем с ЧИСТЫМ входом!):**
   $$\mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}, \tilde{\mathbf{x}} \sim q(\cdot|\mathbf{x})} [L(\mathbf{x}, f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}})))]$$

**Ключевое отличие от обычного автоэнкодера:**
- Обычный AE: вход $\mathbf{x}$ → выход $\hat{\mathbf{x}}$, минимизируем $L(\mathbf{x}, \hat{\mathbf{x}})$
- Denoising AE: вход $\tilde{\mathbf{x}}$ (зашумлённый) → выход $\hat{\mathbf{x}}$, минимизируем $L(\mathbf{x}, \hat{\mathbf{x}})$ (сравниваем с чистым!)

---

#### 6.2.3 Типы шума

**1. Гауссовский шум:**

$$\tilde{\mathbf{x}} = \mathbf{x} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2 I)$$

Параметры:
- $\sigma$ — стандартное отклонение шума (обычно $\sigma \in [0.1, 0.5]$)

**2. Маскирующий шум (Masking Noise):**

$$\tilde{x}_j = \begin{cases} 
0 & \text{с вероятностью } p \\
x_j & \text{с вероятностью } 1-p
\end{cases}$$

Параметры:
- $p$ — вероятность обнуления (dropout rate), обычно $p \in [0.1, 0.5]$

Это эквивалентно dropout на входном слое.

**3. Salt-and-pepper шум:**

$$\tilde{x}_j = \begin{cases} 
0 & \text{с вероятностью } p/2 \\
1 & \text{с вероятностью } p/2 \\
x_j & \text{с вероятностью } 1-p
\end{cases}$$

**4. Размытие (Blur):**

Применяем гауссовский фильтр или другой фильтр размытия.

---

#### 6.2.4 Полная функция потерь

**Для непрерывных данных (с гауссовским шумом):**

$$\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \|\mathbf{x}_i - f_{\text{dec}}(f_{\text{enc}}(\mathbf{x}_i + \boldsymbol{\epsilon}_i))\|^2$$

где $\boldsymbol{\epsilon}_i \sim \mathcal{N}(0, \sigma^2 I)$

**Для бинарных данных (с маскирующим шумом):**

$$\mathcal{L}(\theta) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{d} [x_{ij} \log(\hat{x}_{ij}) + (1-x_{ij}) \log(1-\hat{x}_{ij})]$$

где $\hat{\mathbf{x}}_i = f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}}_i))$ и $\tilde{\mathbf{x}}_i$ получен маскированием.

---

#### 6.2.5 Связь с распределением данных

**Теоретический результат (Vincent et al., 2008):**

Минимизация функции потерь denoising автоэнкодера эквивалентна оценке градиента логарифма плотности данных:

$$f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}})) - \tilde{\mathbf{x}} \propto \nabla_{\tilde{\mathbf{x}}} \log p(\tilde{\mathbf{x}})$$

**Интуиция:**
Автоэнкодер учится "указывать" в направлении более высокой плотности данных — то есть в направлении истинного многообразия данных (data manifold).

**Связь с score matching:**
Denoising автоэнкодеры неявно выполняют score matching — оценку $\nabla_{\mathbf{x}} \log p(\mathbf{x})$.

---

#### 6.2.6 Практические рекомендации

**Выбор уровня шума:**
- Начните с $\sigma = 0.1$ для гауссовского шума
- Начните с $p = 0.3$ для маскирующего шума
- Увеличивайте постепенно, если модель переобучается

**Curriculum learning:**
Можно начинать с малого шума и постепенно увеличивать:

$$\sigma(t) = \sigma_{\min} + (\sigma_{\max} - \sigma_{\min}) \cdot \frac{t}{T}$$

где $t$ — текущая эпоха, $T$ — общее число эпох.

**Источники:**
- Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders", ICML 2008
- Доступно: http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf
- Alain & Bengio, "What Regularized Auto-Encoders Learn from the Data-Generating Distribution", JMLR 2014

---

### 6.3 Вариационные автоэнкодеры (VAE) — краткий обзор

#### 6.3.1 Основная идея

**Проблема обычных автоэнкодеров:**
Латентное пространство не структурировано — между обученными точками могут быть "дыры", где декодер выдаёт бессмыслицу.

**Решение VAE:**
Вместо детерминированного $\mathbf{z} = f_{\text{enc}}(\mathbf{x})$, кодируем вероятностное распределение $q(\mathbf{z} \mid \mathbf{x})$.

---

#### 6.3.2 Архитектура

**Вероятностный энкодер:**

Выдаёт параметры распределения:

$$q_{\phi}(\mathbf{z} \mid \mathbf{x}) = \mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}(\mathbf{x}), \text{diag}(\boldsymbol{\sigma}^2(\mathbf{x})))$$

где энкодер — нейронная сеть, выдающая $\boldsymbol{\mu}(\mathbf{x})$ и $\boldsymbol{\sigma}^2(\mathbf{x})$.

**Reparametrization trick:**

Для обратного распространения ошибки используем:

$$\mathbf{z} = \boldsymbol{\mu}(\mathbf{x}) + \boldsymbol{\sigma}(\mathbf{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$$

где $\odot$ — поэлементное умножение.

**Декодер:**

$$p_{\theta}(\mathbf{x} \mid \mathbf{z})$$

---

#### 6.3.3 Функция потерь (ELBO)

**Evidence Lower Bound (ELBO):**

$$\mathcal{L}(\theta, \phi; \mathbf{x}) = \underbrace{\mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})}[\log p_{\theta}(\mathbf{x} \mid \mathbf{z})]}_{\text{Reconstruction Loss}} - \underbrace{D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) \parallel p(\mathbf{z}))}_{\text{KL Divergence}}$$

**Компоненты:**

1. **Reconstruction Loss:**
   - Для непрерывных данных: $-\|\mathbf{x} - \hat{\mathbf{x}}\|^2$
   - Для бинарных данных: Binary Cross-Entropy

2. **KL Divergence (для $p(\mathbf{z}) = \mathcal{N}(0, I)$):**

$$D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) \parallel \mathcal{N}(0, I)) = \frac{1}{2} \sum_{j=1}^{k} [\mu_j^2 + \sigma_j^2 - \log(\sigma_j^2) - 1]$$

**Полная функция потерь (минимизируем отрицательный ELBO):**

$$\mathcal{L}_{\text{VAE}} = -\mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})}[\log p_{\theta}(\mathbf{x} \mid \mathbf{z})] + D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) \parallel p(\mathbf{z}))$$

**Источник:**
- Kingma & Welling, "Auto-Encoding Variational Bayes" (2013): https://arxiv.org/abs/1312.6114
- Doersch C. "Tutorial on Variational Autoencoders" (2016): https://arxiv.org/abs/1606.05908

---

## Часть 7: Практический пример с кодом

### 7.1 Простой автоэнкодер для MNIST
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Гиперпараметры
input_dim = 784  # 28x28
latent_dim = 32  # размерность латентного пространства
batch_size = 128
learning_rate = 1e-3
epochs = 20

# Загрузка данных
transform = transforms.Compose([
    transforms.ToTensor(),
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Архитектура автоэнкодера
class Autoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()
        
        # Энкодер
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )
        
        # Декодер
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()  # выход в [0, 1]
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_reconstructed = self.decoder(z)
        return x_reconstructed, z

# Инициализация
model = Autoencoder(input_dim, latent_dim)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Функция потерь: Binary Cross-Entropy
criterion = nn.BCELoss(reduction='sum')

# Обучение
model.train()
for epoch in range(epochs):
    total_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        # Преобразуем изображения в вектор
        data = data.view(-1, input_dim)
        
        # Forward pass
        x_reconstructed, z = model(data)
        
        # Вычисляем loss
        loss = criterion(x_reconstructed, data)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader.dataset)
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')
```

---

### 7.2 Denoising Autoencoder
```python
class DenoisingAutoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(DenoisingAutoencoder, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_reconstructed = self.decoder(z)
        return x_reconstructed

# Функция добавления шума
def add_noise(x, noise_factor=0.3):
    """Добавляет гауссовский шум к входу"""
    noise = torch.randn_like(x) * noise_factor
    noisy_x = x + noise
    noisy_x = torch.clamp(noisy_x, 0., 1.)  # ограничиваем [0, 1]
    return noisy_x

# Обучение
model = DenoisingAutoencoder(input_dim, latent_dim)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.BCELoss(reduction='sum')

model.train()
for epoch in range(epochs):
    total_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(-1, input_dim)
        
        # Добавляем шум
        noisy_data = add_noise(data, noise_factor=0.3)
        
        # Forward pass (на вход подаём зашумлённые данные)
        x_reconstructed = model(noisy_data)
        
        # Loss: сравниваем с ЧИСТЫМИ данными!
        loss = criterion(x_reconstructed, data)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader.dataset)
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')
```

---

### 7.3 Sparse Autoencoder
```python
class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(SparseAutoencoder, self).__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, latent_dim),
            nn.Sigmoid()  # активация для разреженности
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, input_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        z = self.encoder(x)
        x_reconstructed = self.decoder(z)
        return x_reconstructed, z

def sparse_loss(model_output, target, latent, sparsity_param=0.05, beta=3):
    """
    Функция потерь для разреженного автоэнкодера
    
    Args:
        model_output: восстановленный выход
        target: целевой вход
        latent: латентное представление
        sparsity_param: целевой уровень разреженности (ρ)
        beta: коэффициент регуляризации (λ)
    """
    # Reconstruction loss
    mse_loss = nn.MSELoss()(model_output, target)
    
    # Средняя активация по батчу для каждого нейрона
    rho_hat = torch.mean(latent, dim=0)
    
    # KL divergence для разреженности
    rho = torch.tensor([sparsity_param]).to(latent.device)
    kl_div = torch.sum(
        rho * torch.log(rho / rho_hat) + 
        (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))
    )
    
    # Полная функция потерь
    total_loss = mse_loss + beta * kl_div
    
    return total_loss

# Обучение
model = SparseAutoencoder(input_dim, latent_dim)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

model.train()
for epoch in range(epochs):
    total_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(-1, input_dim)
        
        # Forward pass
        x_reconstructed, z = model(data)
        
        # Вычисляем loss с разреженностью
        loss = sparse_loss(x_reconstructed, data, z, 
                          sparsity_param=0.05, beta=3)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader.dataset)
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')
```

---

## Часть 8: Резюме и связи между концепциями

### 8.1 Иерархия концепций
```
Теория вероятностей
├── Математическое ожидание
├── Дисперсия
└── Нормальное распределение
    ↓
Метод максимального правдоподобия (MLE)
    ↓
Функции потерь
├── MSE (для гауссовского шума)
├── Cross-Entropy (для дискретных распределений)
└── KL Divergence
    ↓
Теория информации
├── Энтропия
├── Взаимная информация
└── Минимизация ошибки восстановления
    ↓
Автоэнкодеры
├── Базовый автоэнкодер (понижение размерности)
├── Denoising Autoencoder (робастность)
├── Sparse Autoencoder (интерпретируемость)
└── VAE (генерация)
```

---

### 8.2 Ключевые формулы

**1. Нормальное распределение:**

$$p(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

**2. Метод максимального правдоподобия:**

$$\hat{\theta} = \arg\max_{\theta} \sum_{i=1}^{n} \log p(x_i \mid \theta)$$

**3. MSE (из MLE с гауссовским шумом):**

$$\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**4. Binary Cross-Entropy (из MLE с Бернулли):**

$$\mathcal{L}_{\text{BCE}} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]$$

**5. KL Divergence:**

$$D_{KL}(p \parallel q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)}$$

**6. Автоэнкодер — базовая функция потерь:**

$$\mathcal{L}_{\text{AE}} = \frac{1}{n} \sum_{i=1}^{n} \|\mathbf{x}_i - f_{\text{dec}}(f_{\text{enc}}(\mathbf{x}_i))\|^2$$

**7. Denoising Autoencoder:**

$$\mathcal{L}_{\text{DAE}} = \mathbb{E}_{\mathbf{x}, \tilde{\mathbf{x}}} [\|\mathbf{x} - f_{\text{dec}}(f_{\text{enc}}(\tilde{\mathbf{x}}))\|^2]$$

**8. Sparse Autoencoder:**

$$\mathcal{L}_{\text{SAE}} = \frac{1}{n} \sum_{i=1}^{n} \|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2 + \lambda \sum_{j=1}^{k} D_{KL}(\rho \parallel \hat{\rho}_j)$$

**9. VAE (ELBO):**

$$\mathcal{L}_{\text{VAE}} = -\mathbb{E}_{q(\mathbf{z}|\mathbf{x})}[\log p(\mathbf{x} \mid \mathbf{z})] + D_{KL}(q(\mathbf{z}|\mathbf{x}) \parallel p(\mathbf{z}))$$

---

### 8.3 Таблица сравнения автоэнкодеров

| Тип | Основная идея | Функция потерь | Когда использовать |
|-----|---------------|----------------|-------------------|
| **Базовый AE** | Сжатие + восстановление | MSE или BCE | Понижение размерности, извлечение признаков |
| **Denoising AE** | Восстановление из зашумлённого входа | MSE/BCE на чистых данных | Шумоподавление, робастные признаки |
| **Sparse AE** | Разреженная активация латентного слоя | MSE/BCE + sparsity penalty | Интерпретируемые признаки |
| **VAE** | Вероятностное латентное пространство | ELBO (reconstruction + KL) | Генерация новых данных |

---

### 8.4 Практические советы

**Выбор размерности латентного пространства:**
- Слишком маленькая → плохое восстановление
- Слишком большая → identity mapping, нет сжатия
- Правило: начните с $k = \frac{d}{10}$, затем подбирайте

**Выбор функции потерь:**
- Непрерывные данные (изображения 0-255): MSE
- Бинарные данные (изображения 0-1): BCE
- Категориальные данные: Categorical Cross-Entropy

**Регуляризация:**
- Dropout в энкодере/декодере: уменьшает переобучение
- L2-регуляризация весов: делает модель более гладкой
- Sparsity constraint: делает признаки интерпретируемыми

**Архитектура:**
- Симметричный энкодер/декодер обычно работает хорошо
- Batch normalization помогает стабилизировать обучение
- Skip connections (как в U-Net) улучшают восстановление деталей

---

## Финальная проверка источников

✅ **Теория вероятностей:**
- Колмогоров А.Н. "Основные понятия теории вероятностей"
- DeGroot & Schervish, "Probability and Statistics"
- Гмурман В.Е. "Теория вероятностей и математическая статистика"

✅ **MLE и функции потерь:**
- Casella & Berger, "Statistical Inference"
- Murphy K. "Machine Learning: A Probabilistic Perspective"
- Bishop C. "Pattern Recognition and Machine Learning"

✅ **Теория информации:**
- Shannon C.E. "A Mathematical Theory of Communication" (1948)
- Cover & Thomas, "Elements of Information Theory"

✅ **Автоэнкодеры:**
- Hinton & Salakhutdinov, Science 313 (2006)
- Vincent et al., "Denoising Autoencoders", ICML 2008
- Ng A. "Sparse Autoencoder", CS294A Stanford
- Kingma & Welling, "Auto-Encoding Variational Bayes", arxiv:1312.6114

✅ **Глубокое обучение:**
- Goodfellow, Bengio, Courville, "Deep Learning" (2016)
- Nielsen M. "Neural Networks and Deep Learning"

Все источники доступны либо в научных библиотеках, либо в открытом доступе (arxiv.org, личные сайты авторов).

---

## Заключение

Мы прошли полный путь от базовых концепций теории вероятностей до современных архитектур автоэнкодеров:

1. **Теория вероятностей** дала нам математический язык для описания неопределённости
2. **MLE** связал статистику с оптимизацией
3. **Функции потерь** возникли естественным образом из вероятностных предположений
4. **Теория информации** объяснила, почему минимизация ошибки восстановления имеет смысл
5. **Автоэнкодеры** реализовали эти идеи в практических архитектурах нейронных сетей

Все эти концепции глубоко взаимосвязаны через единую вероятностную интерпретацию!