# Полное руководство: От теории вероятностей до автоэнкодеров

## Содержание
1. [Основы теории вероятностей](#часть-1-основы-теории-вероятностей)
2. [Метод максимального правдоподобия](#часть-2-метод-максимального-правдоподобия)
3. [От MLE к функциям потерь](#часть-3-от-mle-к-функциям-потерь)
4. [Теория информации](#часть-4-теория-информации)
5. [Автоэнкодеры: базовая теория](#часть-5-автоэнкодеры-базовая-теория)
6. [Специальные типы автоэнкодеров](#часть-6-специальные-типы-автоэнкодеров)

---

## Часть 1: Основы теории вероятностей

### 1.1 Математическое ожидание (среднее значение)

**Что это такое простыми словами:**
Представь, что ты бросаешь кубик много-много раз и записываешь результаты. Математическое ожидание — это число, вокруг которого будут "крутиться" твои результаты в среднем.

**Формула для дискретной случайной величины:**

$$E[X] = \sum_{i} x_i \cdot P(x_i)$$

Где:
- $x_i$ — возможные значения
- $P(x_i)$ — вероятность каждого значения

**Пример с кубиком:**

$$E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = 3.5$$

**Для непрерывной величины:**

$$E[X] = \int_{-\infty}^{\infty} x \cdot p(x) \, dx$$

Где $p(x)$ — функция плотности вероятности.

**Источник:** Колмогоров А.Н. "Основные понятия теории вероятностей" (классический учебник, доступен в любом университетском курсе ТерВера)

---

### 1.2 Дисперсия

**Что это такое простыми словами:**
Дисперсия показывает, насколько сильно значения "разбросаны" вокруг среднего. Если дисперсия маленькая — значения кучкуются близко к среднему. Если большая — разброс огромный.

**Формула:**

$$\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$

**Расшифровка:**
1. Берём каждое значение
2. Вычитаем среднее (получаем отклонение)
3. Возводим в квадрат (чтобы убрать знак)
4. Усредняем все эти квадраты отклонений

**Пример с кубиком:**

$$E[X] = 3.5$$

$$E[X^2] = 1^2 \cdot \frac{1}{6} + 2^2 \cdot \frac{1}{6} + \ldots + 6^2 \cdot \frac{1}{6} = \frac{91}{6}$$

$$\text{Var}(X) = \frac{91}{6} - (3.5)^2 \approx 2.92$$

**Стандартное отклонение:**

$$\sigma = \sqrt{\text{Var}(X)}$$

Это просто квадратный корень из дисперсии. Удобно, потому что имеет те же единицы измерения, что и сами данные.

**Источник:** DeGroot & Schervish, "Probability and Statistics" (4th edition), Chapter 4

---

### 1.3 Нормальное (гауссовское) распределение

**Что это такое простыми словами:**
Это самое важное распределение в статистике. Оно описывает множество естественных явлений: рост людей, ошибки измерений, результаты экзаменов и так далее. График похож на колокол.

**Функция плотности вероятности:**

$$p(x) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

Где:
- $\mu$ (мю) — математическое ожидание (центр "колокола")
- $\sigma$ (сигма) — стандартное отклонение (ширина "колокола")
- $\pi \approx 3.14159\ldots$
- $\exp(\ldots) = e^{(\ldots)}$

**Обозначение:**

$$X \sim \mathcal{N}(\mu, \sigma^2)$$

Читается: "X распределена нормально с матожиданием $\mu$ и дисперсией $\sigma^2$"

**Стандартное нормальное распределение:**

$$X \sim \mathcal{N}(0, 1)$$

Это частный случай с $\mu=0$ и $\sigma=1$.

**Важное правило "трёх сигм":**
- 68% значений лежат в интервале $[\mu-\sigma, \mu+\sigma]$
- 95% значений лежат в интервале $[\mu-2\sigma, \mu+2\sigma]$
- 99.7% значений лежат в интервале $[\mu-3\sigma, \mu+3\sigma]$

**Источник:** 
- Bishop C. "Pattern Recognition and Machine Learning", Section 2.3
- Гмурман В.Е. "Теория вероятностей и математическая статистика", глава 5

---

## Часть 2: Метод максимального правдоподобия

### 2.1 Интуиция

**Простыми словами:**
У тебя есть данные, и ты хочешь подобрать модель (например, нормальное распределение), которая лучше всего объясняет эти данные. Метод максимального правдоподобия (Maximum Likelihood Estimation, MLE) говорит: "Выбери такие параметры модели, при которых наблюдаемые данные были бы наиболее вероятными".

### 2.2 Функция правдоподобия (Likelihood)

**Для одного наблюдения:**

$$L(\theta \mid x) = p(x \mid \theta)$$

Где:
- $\theta$ (тета) — параметры модели
- $x$ — наблюдаемые данные
- $p(x|\theta)$ — вероятность увидеть $x$ при параметрах $\theta$

**Для нескольких независимых наблюдений $x_1, x_2, \ldots, x_n$:**

$$L(\theta \mid x_1,\ldots,x_n) = \prod_{i=1}^{n} p(x_i \mid \theta) = p(x_1|\theta) \cdot p(x_2|\theta) \cdot \ldots \cdot p(x_n|\theta)$$

### 2.3 Логарифм правдоподобия

**Проблема:** Произведения неудобны для оптимизации.

**Решение:** Берём логарифм (он монотонен, поэтому максимум сохраняется):

$$\log L(\theta) = \sum_{i=1}^{n} \log p(x_i \mid \theta)$$

**Преимущества:**
1. Произведение превращается в сумму
2. Числа не становятся слишком маленькими (нет проблемы с underflow)
3. Проще брать производные

### 2.4 Пример: оценка параметров нормального распределения

**Задача:** У нас есть выборка $x_1, \ldots, x_n$ из $\mathcal{N}(\mu, \sigma^2)$. Нужно найти $\mu$ и $\sigma^2$.

**Шаг 1: Записываем правдоподобие**

$$L(\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$$

**Шаг 2: Берём логарифм**

$$\log L = \sum_{i=1}^{n} \left[\log\left(\frac{1}{\sigma\sqrt{2\pi}}\right) - \frac{(x_i-\mu)^2}{2\sigma^2}\right]$$

$$= -n \cdot \log(\sigma) - \frac{n}{2} \cdot \log(2\pi) - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i-\mu)^2$$

**Шаг 3: Берём производные и приравниваем к нулю**

По $\mu$:

$$\frac{\partial(\log L)}{\partial\mu} = \frac{1}{\sigma^2} \sum_{i=1}^{n}(x_i-\mu) = 0$$

$$\Rightarrow \hat{\mu} = \frac{1}{n} \sum_{i=1}^{n} x_i \quad \text{(выборочное среднее!)}$$

По $\sigma^2$:

$$\frac{\partial(\log L)}{\partial(\sigma^2)} = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n}(x_i-\mu)^2 = 0$$

$$\Rightarrow \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n}(x_i-\hat{\mu})^2 \quad \text{(выборочная дисперсия!)}$$

**Вывод:** Метод максимального правдоподобия естественным образом даёт нам знакомые формулы для среднего и дисперсии!

**Источник:** 
- Murphy K. "Machine Learning: A Probabilistic Perspective", Chapter 9.2
- Casella & Berger, "Statistical Inference" (2nd ed), Chapter 7

---

## Часть 3: От MLE к функциям потерь

### 3.1 Связь между MLE и минимизацией потерь

**Ключевая идея:**

$$\text{Максимизировать } \log L(\theta) \Leftrightarrow \text{Минимизировать } -\log L(\theta)$$

Мы переходим от **максимизации правдоподобия** к **минимизации потерь**:

$$\mathcal{L} = -\log L(\theta) = -\sum_{i=1}^{n} \log p(x_i \mid \theta)$$

### 3.2 Вывод MSE (Mean Squared Error)

**Предположим:** Данные порождаются как:

$$y = f(x; \theta) + \varepsilon, \quad \text{где } \varepsilon \sim \mathcal{N}(0, \sigma^2)$$

Это значит:

$$p(y \mid x, \theta) = \mathcal{N}(y \mid f(x;\theta), \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \cdot \exp\left(-\frac{(y - f(x;\theta))^2}{2\sigma^2}\right)$$

**Логарифм правдоподобия:**

$$\log p(y \mid x, \theta) = -\log(\sigma) - \frac{1}{2}\log(2\pi) - \frac{(y - f(x;\theta))^2}{2\sigma^2}$$

**Для выборки из $n$ точек:**

$$\log L = \sum_{i=1}^{n} \log p(y_i \mid x_i, \theta) = \text{const} - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(y_i - f(x_i;\theta))^2$$

**Максимизация $\log L$ эквивалентна минимизации:**

$$\mathcal{L} = \frac{1}{2} \sum_{i=1}^{n}(y_i - f(x_i;\theta))^2$$

**Усредняя по выборке:**

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**Вывод:** MSE — это функция потерь, которая соответствует предположению о гауссовском шуме!

**Источник:** 
- Bishop C. "Pattern Recognition and Machine Learning", Section 3.1
- Goodfellow et al., "Deep Learning", Section 5.5

---

### 3.3 Вывод Cross-Entropy (кросс-энтропия)

**Применение:** Классификация с дискретными метками.

**Предположим:** Выход модели — вероятности классов:

$$p(y = k \mid x, \theta) = \text{softmax}_k(f(x; \theta))$$

**Правдоподобие для одного примера:**

$$p(y \mid x, \theta) = \prod_{k} p(y=k \mid x, \theta)^{y_k}$$

Где $y_k$ — one-hot кодировка ($y_k=1$ для правильного класса, 0 для остальных).

**Логарифм:**

$$\log p(y \mid x, \theta) = \sum_{k} y_k \cdot \log p(y=k \mid x, \theta)$$

**Negative log-likelihood (функция потерь):**

$$\mathcal{L} = -\sum_{k} y_k \cdot \log p(y=k \mid x, \theta)$$

Это и есть **кросс-энтропия**!

**Для бинарной классификации (Binary Cross-Entropy):**

$$\mathcal{L} = -[y \cdot \log(\hat{y}) + (1-y) \cdot \log(1-\hat{y})]$$

**Источник:** 
- Murphy K. "Probabilistic Machine Learning: An Introduction", Chapter 10.2
- Nielsen M. "Neural Networks and Deep Learning", Chapter 3

---

## Часть 4: Теория информации

### 4.1 Энтропия

**Что это такое простыми словами:**
Энтропия измеряет "неопределённость" или "информационное содержание" случайной величины. Чем более случайна величина, тем выше её энтропия.

**Формула энтропии Шеннона:**

$$H(X) = -\sum_{i} p(x_i) \log_2 p(x_i)$$

Измеряется в битах (если используется $\log_2$).

**Пример:**
- Честная монета: $H = -[0.5 \log_2(0.5) + 0.5 \log_2(0.5)] = 1$ бит
- Несправедливая монета (99% орёл): $H \approx 0.08$ бит (почти нет неопределённости)

**Свойства:**
- $H(X) \geq 0$ (всегда неотрицательна)
- $H(X) = 0$ тогда и только тогда, когда $X$ детерминирована
- Максимальна для равномерного распределения

**Источник:** 
- Shannon C.E. "A Mathematical Theory of Communication" (1948) — оригинальная статья
- Cover & Thomas, "Elements of Information Theory" (2nd ed), Chapter 2

---

### 4.2 Кросс-энтропия (Cross-Entropy)

**Определение:**
Кросс-энтропия между двумя распределениями $p$ и $q$ измеряет, сколько битов в среднем нужно для кодирования событий из распределения $p$, если мы используем оптимальный код для распределения $q$.

**Формула:**

$$H(p, q) = -\sum_{i} p(x_i) \log q(x_i)$$

**В машинном обучении:**
- $p$ — истинное распределение (метки данных)
- $q$ — предсказанное распределение (выход модели)

**Связь с энтропией:**

$$H(p, q) = H(p) + D_{KL}(p \parallel q)$$

Где $D_{KL}$ — дивергенция Кульбака-Лейблера.

**Почему минимизируем кросс-энтропию?**
Потому что $H(p)$ — константа (истинное распределение фиксировано), поэтому минимизация $H(p,q)$ эквивалентна минимизации $D_{KL}(p \parallel q)$ — то есть мы делаем $q$ максимально похожим на $p$!

**Источник:** 
- Goodfellow et al., "Deep Learning", Section 3.13
- Murphy K. "Probabilistic Machine Learning", Section 6.1

---

### 4.3 Дивергенция Кульбака-Лейблера (KL Divergence)

**Определение:**
KL-дивергенция измеряет "расстояние" между двумя вероятностными распределениями (хотя технически это не метрика, так как несимметрична).

**Формула:**

$$D_{KL}(p \parallel q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)} = \sum_{i} p(x_i) [\log p(x_i) - \log q(x_i)]$$

**Для непрерывных распределений:**

$$D_{KL}(p \parallel q) = \int p(x) \log \frac{p(x)}{q(x)} \, dx$$

**Свойства:**
- $D_{KL}(p \parallel q) \geq 0$ (всегда неотрицательна)
- $D_{KL}(p \parallel q) = 0$ тогда и только тогда, когда $p = q$ почти всюду
- Несимметрична: $D_{KL}(p \parallel q) \neq D_{KL}(q \parallel p)$

**Для двух нормальных распределений:**

$$D_{KL}(\mathcal{N}(\mu_1, \sigma_1^2) \parallel \mathcal{N}(\mu_2, \sigma_2^2)) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}$$

**Для многомерного случая (используется в VAE):**

$$D_{KL}(\mathcal{N}(\mu, \Sigma) \parallel \mathcal{N}(0, I)) = \frac{1}{2} \sum_{j=1}^{d} \left[\mu_j^2 + \sigma_j^2 - \log(\sigma_j^2) - 1\right]$$

**Источник:** 
- Cover & Thomas, "Elements of Information Theory", Chapter 2
- Kingma & Welling, "Auto-Encoding Variational Bayes" (2013)

---

### 4.4 Минимизация ошибки восстановления

**Постановка задачи:**
Имеем данные $\mathbf{x} \in \mathbb{R}^d$ высокой размерности. Хотим найти сжатое представление $\mathbf{z} \in \mathbb{R}^k$, где $k \ll d$, так чтобы можно было восстановить $\mathbf{x}$ с минимальной ошибкой.

**Математическая формулировка:**

$$\min_{\mathbf{z}, f, g} \mathbb{E}[\|\mathbf{x} - g(f(\mathbf{x}))\|^2]$$

Где:
- $f: \mathbb{R}^d \to \mathbb{R}^k$ — функция кодирования (encoder)
- $g: \mathbb{R}^k \to \mathbb{R}^d$ — функция декодирования (decoder)
- $\mathbf{z} = f(\mathbf{x})$ — латентное представление

**Связь с теорией информации:**
Минимизация ошибки восстановления эквивалентна максимизации взаимной информации между $\mathbf{x}$ и $\mathbf{z}$:

$$I(\mathbf{x}; \mathbf{z}) = H(\mathbf{x}) - H(\mathbf{x} \mid \mathbf{z})$$

Чем меньше условная энтропия $H(\mathbf{x} \mid \mathbf{z})$, тем лучше мы можем восстановить $\mathbf{x}$ по $\mathbf{z}$.

**Источник:**
- Hinton & Salakhutdinov, "Reducing the Dimensionality of Data with Neural Networks", Science (2006)
- Tishby & Zaslavsky, "Deep Learning and the Information Bottleneck Principle" (2015)

---

## Часть 5: Автоэнкодеры — базовая теория

### 5.1 Что такое автоэнкодер

**Простыми словами:**
Автоэнкодер — это нейронная сеть, которая учится сжимать данные (например, картинку) в компактное представление, а потом восстанавливать их обратно. Это как архиватор для данных, но обученный на конкретном типе данных.

**Архитектура:**

$$\mathbf{x} \xrightarrow{\text{Encoder}} \mathbf{z} \xrightarrow{\text{Decoder}} \hat{\mathbf{x}}$$

### 5.2 Математическая формулировка

**Компоненты:**

1. **Энкодер (Encoder):**
   $$\mathbf{z} = f_{\text{enc}}(\mathbf{x}; \theta_{\text{enc}})$$
   Сжимает входные данные $\mathbf{x} \in \mathbb{R}^d$ в компактное представление $\mathbf{z} \in \mathbb{R}^k$, где $k \ll d$.

2. **Декодер (Decoder):**
   $$\hat{\mathbf{x}} = f_{\text{dec}}(\mathbf{z}; \theta_{\text{dec}})$$
   Восстанавливает данные из латентного представления.

3. **Полное преобразование:**
   $$\hat{\mathbf{x}} = f_{\text{dec}}(f_{\text{enc}}(\mathbf{x}; \theta_{\text{enc}}); \theta_{\text{dec}})$$

**Параметры:**
- $\theta_{\text{enc}}$ — веса энкодера
- $\theta_{\text{dec}}$ — веса декодера
- $\theta = \{\theta_{\text{enc}}, \theta_{\text{dec}}\}$ — все параметры модели

---

### 5.3 Функции потерь для автоэнкодера

**Общий принцип:**
Цель — минимизировать ошибку восстановления (reconstruction error):

$$\mathcal{L}(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(\mathbf{x}_i, \hat{\mathbf{x}}_i)$$

**Варианты функции потерь:**

**1. Mean Squared Error (MSE) — для непрерывных данных:**

$$L_{\text{MSE}}(\mathbf{x}, \hat{\mathbf{x}}) = \|\mathbf{x} - \hat{\mathbf{x}}\|^2 = \sum_{j=1}^{d} (x_j - \hat{x}_j)^2$$

**Когда использовать:** Изображения с пикселями в диапазоне $[0, 255]$ или нормализованные в $[-1, 1]$.

**2. Binary Cross-Entropy (BCE) — для бинарных данных:**

$$L_{\text{BCE}}(\mathbf{x}, \hat{\mathbf{x}}) = -\sum_{j=1}^{d} [x_j \log(\hat{x}_j) + (1-x_j) \log(1-\hat{x}_j)]$$

**Когда использовать:** Бинарные изображения или данные с пикселями нормализованными в $[0, 1]$.

**3. Mean Absolute Error (MAE):**

$$L_{\text{MAE}}(\mathbf{x}, \hat{\mathbf{x}}) = \sum_{j=1}^{d} |x_j - \hat{x}_j|$$

**Когда использовать:** Когда нужна робастность к выбросам (MAE менее чувствительна к выбросам, чем MSE).

---

### 5.4 Вывод функции потерь через MLE

**Для непрерывных данных (MSE):**

Предполагаем модель генерации:

$$p(\mathbf{x} \mid \mathbf{z}) = \mathcal{N}(\mathbf{x} \mid f_{\text{dec}}(\mathbf{z}), \sigma^2 I)$$

Логарифм правдоподобия:

$$\log p(\mathbf{x} \mid \mathbf{z}) = \text{const} - \frac{1}{2\sigma^2} \|\mathbf{x} - f_{\text{dec}}(\mathbf{z})\|^2$$

Максимизация $\log p(\mathbf{x} \mid \mathbf{z})$ эквивалентна минимизации MSE!

**Для бинарных данных (BCE):**

Предполагаем независимые распределения Бернулли для каждого пикселя:

$$p(\mathbf{x} \mid \mathbf{z}) = \prod_{j=1}^{d} [f_{\text{dec}}(\mathbf{z})_j]^{x_j} \cdot [1 - f_{\text{dec}}(\mathbf{z})_j]^{1-x_j}$$

Логарифм:

$$\log p(\mathbf{x} \mid \mathbf{z}) = \sum_{j=1}^{d} [x_j \log(f_{\text{dec}}(\mathbf{z})_j) + (1-x_j) \log(1-f_{\text{dec}}(\mathbf{z})_j)]$$

Максимизация эквивалентна минимизации BCE!

**Источник:**
- Kingma & Welling, "Auto-Encoding Variational Bayes" (2013): https://arxiv.org/abs/1312.6114
- Bishop C. "Pattern Recognition and Machine Learning", Chapter 12

---

### 5.5 Понижение размерности

**Зачем нужно?**

1. **Вычислительная эффективность:** Работа с данными меньшей размерности быстрее
2. **Визуализация:** Можно визуализировать в 2D или 3D
3. **Обобщение:** Уменьшение размерности может действовать как регуляризация
4. **Извлечение признаков:** Латентное пространство может содержать более значимые признаки

**Сравнение методов:**

| Метод | Тип | Линейность | Плюсы | Минусы |
|-------|-----|------------|-------|--------|
| PCA | Классический | Линейный | Быстрый, интерпретируемый | Только линейные зависимости |
| t-SNE | Классический | Нелинейный | Хорошая визуализация | Медленный, не обобщает на новые данные |
| Автоэнкодер | Нейронная сеть | Нелинейный | Гибкий, обобщает | Требует обучения, может переобучиться |

**Размерность латентного пространства:**

$$k = \text{размерность}(\mathbf{z}) \ll d = \text{размерность}(\mathbf{x})$$

Типичные значения:
- Для MNIST (784 пикселя): $k = 2, 10, 32$
- Для ImageNet ($224 \times 224 \times 3 = 150528$): $k = 128, 256, 512$

**Информационное узкое место (Information Bottleneck):**

Латентный слой с малой размерностью заставляет сеть выучить наиболее важные признаки данных, отбрасывая шум и детали.

**Источник:**
- Hinton & Salakhutdinov, "Reducing the Dimensionality of Data with Neural Networks", Science 313 (2006)
- Tishby & Zaslavsky, "Deep Learning and the Information Bottleneck Principle" (2015)

---

### 5.6 Применения автоэнкодеров

#### 5.6.1 Снижение размерности

**Пример: Визуализация MNIST**
```python