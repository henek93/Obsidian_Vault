# Математические основы нейронных сетей и ИИ

## Подробное руководство к зачёту с понятными объяснениями

---

# РАЗДЕЛ 1. МАТЕМАТИЧЕСКИЕ ОСНОВЫ

---

## Вопрос 1. Понятие функции потерь. Примеры для регрессии и классификации

### Что такое функция потерь и зачем она нужна

Представьте, что вы учите ребёнка бросать мяч в корзину. После каждого броска вам нужно как-то измерить, насколько хорошо или плохо он бросил. Если мяч попал — отлично, если промахнулся на метр — плохо, если на пять метров — очень плохо.

**Функция потерь (Loss Function)** — это именно такой «измеритель качества» для нейронной сети. Она берёт два числа:

- $y$ — правильный ответ (куда нужно было попасть)
- $\hat{y}$ — предсказание модели (куда на самом деле попали)

И выдаёт одно число — насколько сильно модель ошиблась.

### Математическое определение

Функция потерь $L(y, \hat{y})$ — это функция, которая:

- Равна нулю (или минимальна), когда предсказание идеально: $\hat{y} = y$
- Растёт, когда предсказание отклоняется от правильного ответа

**Общие потери на всём датасете** — это среднее по всем примерам:

$$\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} L(y_i, \hat{y}_i)$$

где $N$ — количество примеров в обучающей выборке.

**Цель обучения нейронной сети** — найти такие веса, при которых эти общие потери минимальны. То есть мы буквально ищем минимум функции потерь.

---

### Функции потерь для задач РЕГРЕССИИ

Регрессия — это когда мы предсказываем число. Например: цену квартиры, температуру завтра, возраст человека по фото.

#### 1. MSE — Mean Squared Error (Среднеквадратичная ошибка)

$$L_{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

**Как это работает:**

1. Берём разницу между правильным ответом и предсказанием: $(y - \hat{y})$
2. Возводим в квадрат: $(y - \hat{y})^2$
3. Усредняем по всем примерам

**Пример расчёта:** Допустим, модель предсказывает цены квартир (в миллионах рублей):

|Реальная цена $y$|Предсказание $\hat{y}$|Ошибка $(y-\hat{y})$|Квадрат ошибки|
|---|---|---|---|
|5.0|4.5|0.5|0.25|
|8.0|9.0|-1.0|1.00|
|3.0|3.2|-0.2|0.04|

$$MSE = \frac{0.25 + 1.00 + 0.04}{3} = \frac{1.29}{3} = 0.43$$

**Почему квадрат?**

1. **Убирает знак**: ошибка -1 и +1 одинаково плохи
2. **Сильно штрафует большие ошибки**: ошибка в 2 раза больше даёт штраф в 4 раза больше
3. **Гладкая функция**: легко считать производную для градиентного спуска

**Аналогия**: Представьте, что вы платите штраф за опоздание на работу. При MSE штраф за опоздание на 10 минут в 100 раз больше, чем за опоздание на 1 минуту. Это заставляет очень стараться не опаздывать сильно.

**Недостаток MSE**: очень чувствительна к выбросам. Если в данных есть одна квартира с ошибкой предсказания в 10 млн, она даст штраф 100 и «перетянет» всё обучение на себя.

#### 2. MAE — Mean Absolute Error (Средняя абсолютная ошибка)

$$L_{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$

**Как это работает:**

1. Берём разницу между правильным ответом и предсказанием
2. Берём модуль (абсолютное значение)
3. Усредняем

**Отличие от MSE:**

- Ошибка в 2 раза больше даёт штраф ровно в 2 раза больше (линейно, а не квадратично)
- Менее чувствительна к выбросам

**Аналогия**: Штраф за опоздание пропорционален минутам. 10 минут опоздания = 10 рублей штрафа, 20 минут = 20 рублей.

**Недостаток MAE**: не дифференцируема в точке $y = \hat{y}$ (излом графика). Это может создавать проблемы при оптимизации.

#### 3. Huber Loss — компромисс между MSE и MAE

$$L_{\delta}(y, \hat{y}) = \begin{cases} \frac{1}{2}(y - \hat{y})^2, & \text{если } |y - \hat{y}| \leq \delta \ \delta \cdot |y - \hat{y}| - \frac{1}{2}\delta^2, & \text{если } |y - \hat{y}| > \delta \end{cases}$$

Ведёт себя как MSE для маленьких ошибок и как MAE для больших. Лучшее из двух миров.

---

### Функции потерь для задач КЛАССИФИКАЦИИ

Классификация — это когда мы предсказываем категорию. Например: спам/не спам, кошка/собака, цифра от 0 до 9.

#### 1. Бинарная кросс-энтропия (Binary Cross-Entropy)

Используется, когда есть только 2 класса (да/нет, 0/1).

$$L_{BCE} = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$

**Разберём по частям:**

Здесь $y \in {0, 1}$ — правильный класс, $\hat{y} \in (0, 1)$ — предсказанная вероятность класса 1.

**Случай 1: правильный ответ $y = 1$** $$L = -\log(\hat{y})$$

Если модель предсказала $\hat{y} = 0.99$ (уверена, что класс 1) → $L = -\log(0.99) \approx 0.01$ — маленькие потери ✓

Если модель предсказала $\hat{y} = 0.01$ (уверена, что класс 0, но ошиблась!) → $L = -\log(0.01) \approx 4.6$ — огромные потери ✗

**Случай 2: правильный ответ $y = 0$** $$L = -\log(1 - \hat{y})$$

Если модель предсказала $\hat{y} = 0.01$ → $L = -\log(0.99) \approx 0.01$ — маленькие потери ✓

Если модель предсказала $\hat{y} = 0.99$ → $L = -\log(0.01) \approx 4.6$ — огромные потери ✗

**Аналогия — «мера удивления»:**

Представьте, что модель — это синоптик, предсказывающий дождь.

- Синоптик сказал: «99% будет дождь». Дождь пошёл. Вы не удивлены — потери маленькие.
- Синоптик сказал: «1% будет дождь». Дождь пошёл. Вы ОЧЕНЬ удивлены — потери огромные.

Кросс-энтропия измеряет, насколько модель «удивляется» правильному ответу.

#### 2. Категориальная кросс-энтропия (Categorical Cross-Entropy)

Используется для многоклассовой классификации (например, 10 цифр).

$$L_{CCE} = -\sum_{k=1}^{K} y_k \log(\hat{y}_k)$$

Здесь:

- $K$ — количество классов
- $y_k$ — one-hot вектор правильного класса (например, для цифры 3: $[0,0,0,1,0,0,0,0,0,0]$)
- $\hat{y}_k$ — предсказанные вероятности каждого класса

**Пример:**

Распознаём цифру на картинке. Правильный ответ: 3.

One-hot: $y = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]$

Модель предсказала: $\hat{y} = [0.01, 0.01, 0.05, 0.85, 0.02, 0.01, 0.02, 0.01, 0.01, 0.01]$

$$L = -1 \cdot \log(0.85) = 0.16$$

Потери небольшие, потому что модель дала высокую вероятность правильному классу.

Если бы модель предсказала: $\hat{y} = [0.5, 0.1, 0.1, 0.05, ...]$ (вероятность 3 всего 5%):

$$L = -1 \cdot \log(0.05) = 3.0$$

Потери большие — модель ошиблась.

---

### Связь с методом максимального правдоподобия

Откуда взялась формула кросс-энтропии? Она не придумана случайно, а выводится из фундаментального статистического принципа.

**Метод максимального правдоподобия (Maximum Likelihood Estimation):**

1. Мы предполагаем, что данные порождены некоторым распределением с параметрами $\theta$
2. **Правдоподобие** — вероятность наблюдать наши данные при данных параметрах: $$\mathcal{L}(\theta) = \prod_{i=1}^{N} P(y_i | \mathbf{x}_i, \theta)$$
3. Мы хотим найти $\theta$, максимизирующие правдоподобие

**Переход к логарифму:**

Произведение вероятностей — очень маленькое число. Удобнее работать с суммой логарифмов:

$$\log \mathcal{L}(\theta) = \sum_{i=1}^{N} \log P(y_i | \mathbf{x}_i, \theta)$$

**Связь с кросс-энтропией:**

Максимизация лог-правдоподобия = Минимизация отрицательного лог-правдоподобия = Минимизация кросс-энтропии!

$$\text{Cross-Entropy} = -\frac{1}{N} \sum_{i=1}^{N} \log P(y_i | \mathbf{x}_i, \theta)$$

Это не совпадение — кросс-энтропия математически обоснована как оптимальная функция потерь для классификации.

---

## Вопрос 2. Градиентный спуск: идея, формулировка, варианты (SGD, mini-batch, Adam)

### Что такое градиент — напоминание из математики

Прежде чем говорить о градиентном спуске, вспомним, что такое градиент.

**Для функции одной переменной** $f(x)$:

- Производная $f'(x)$ показывает скорость изменения функции
- Если $f'(x) > 0$ — функция растёт при увеличении $x$
- Если $f'(x) < 0$ — функция убывает

**Для функции многих переменных** $f(x_1, x_2, ..., x_n)$:

- Есть много направлений, куда можно двигаться
- Нужен вектор, показывающий направление наибыстрейшего роста

**Градиент** — это вектор из частных производных:

$$\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right)$$

**Ключевое свойство**: градиент указывает в направлении наибыстрейшего РОСТА функции.

**Пример:**

$f(x, y) = x^2 + y^2$ — парабола (чаша)

$$\nabla f = (2x, 2y)$$

В точке $(3, 4)$: $\nabla f = (6, 8)$

Это значит: если мы сдвинемся в направлении $(6, 8)$, функция будет расти быстрее всего.

### Идея градиентного спуска

**Задача**: найти минимум функции потерь $L(\theta)$, где $\theta$ — веса нейронной сети.

**Идея**: если градиент показывает направление роста, то **антиградиент** $(-\nabla L)$ показывает направление убывания!

**Алгоритм:**

1. Начинаем с произвольных весов $\theta_0$
2. Вычисляем градиент $\nabla L(\theta_t)$
3. Делаем шаг в направлении антиградиента
4. Повторяем, пока не сойдёмся к минимуму

**Аналогия — спуск с горы в тумане:**

Вы стоите на горе и хотите спуститься вниз, но туман такой густой, что вы видите только землю под ногами.

Стратегия: пощупать склон вокруг себя и сделать шаг в направлении, где склон идёт вниз круче всего.

Градиент — это «направление наибольшей крутизны вверх», поэтому мы идём в противоположную сторону.

### Математическая формулировка

**Правило обновления весов:**

$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta L(\theta_t)$$

где:

- $\theta_t$ — текущие веса на шаге $t$
- $\eta$ (эта) — **скорость обучения (learning rate)** — размер шага
- $\nabla_\theta L$ — градиент функции потерь по весам

**Выбор learning rate — критически важен:**

- $\eta$ слишком большой → перепрыгиваем через минимум, расходимся
- $\eta$ слишком маленький → сходимся очень медленно
- Типичные значения: 0.001, 0.01, 0.1

```
         Loss
           │    ╱╲
           │   ╱  ╲        η слишком большой — прыгаем туда-сюда
           │  ╱    ╲
           │ ╱      ╲
           │╱   •    ╲
           ──────────────→ θ
                минимум
```

---

### Варианты градиентного спуска

#### 1. Batch Gradient Descent (Полный градиентный спуск)

$$\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta L(y_i, f(\mathbf{x}_i; \theta_t))$$

**Как работает:**

- Берём ВСЮ обучающую выборку (все $N$ примеров)
- Вычисляем градиент по каждому примеру
- Усредняем все градиенты
- Делаем один шаг

**Плюсы:**

- Стабильная сходимость — градиент точный
- Гарантированно идём в правильном направлении

**Минусы:**

- Очень медленно на больших данных (миллионы примеров = миллионы градиентов перед одним шагом)
- Требует много памяти
- Может застрять в локальном минимуме

**Аналогия:** Перед каждым шагом вы тщательно обследуете всю гору с помощью дрона, строите 3D-модель, и только потом делаете один шаг. Точно, но очень долго.

#### 2. Stochastic Gradient Descent (SGD) — Стохастический

$$\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta L(y_i, f(\mathbf{x}_i; \theta_t))$$

**Как работает:**

- Берём ОДИН случайный пример из выборки
- Вычисляем градиент только по нему
- Делаем шаг

**Плюсы:**

- Очень быстро — один пример = один шаг
- Шум помогает выбраться из локальных минимумов
- Может работать с бесконечными потоками данных

**Минусы:**

- Шумный градиент — может идти не совсем в правильном направлении
- «Прыгает» вокруг минимума, не может точно сойтись

**Аналогия:** Вы делаете шаг после беглого взгляда под ноги. Быстро, но можете идти зигзагами.

```
         Loss
           │      ∿∿∿
           │    ∿    ∿
           │  ∿        ∿     SGD — зигзагообразный путь
           │∿          ∿
           │    •  ∿∿∿
           ──────────────→ θ
```

#### 3. Mini-batch Gradient Descent — Компромисс

$$\theta_{t+1} = \theta_t - \eta \cdot \frac{1}{|B|} \sum_{i \in B} \nabla_\theta L(y_i, f(\mathbf{x}_i; \theta_t))$$

**Как работает:**

- Берём небольшую порцию (batch) из $|B|$ примеров (обычно 32, 64, 128, 256)
- Вычисляем среднее градиента по этой порции
- Делаем шаг

**Плюсы:**

- Баланс между скоростью и стабильностью
- Хорошо параллелится на GPU
- Достаточно стабильный градиент

**Это стандарт в современном глубоком обучении!**

**Терминология:**

- **Epoch (эпоха)** — один проход по всему датасету
- **Iteration (итерация)** — обновление весов на одном batch
- Если датасет 10000 примеров и batch size = 100, то в одной эпохе 100 итераций

#### 4. Adam — Adaptive Moment Estimation

Adam — самый популярный оптимизатор. Он умнее обычного SGD.

**Идея:** запоминать историю градиентов и адаптировать скорость обучения для каждого параметра отдельно.

**Формулы:**

Первый момент (экспоненциальное среднее градиентов): $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$

Второй момент (экспоненциальное среднее квадратов градиентов): $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

Коррекция смещения (важно в начале обучения): $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

Обновление весов: $$\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

**Что это значит интуитивно:**

- $m_t$ — «инерция». Если градиенты долго указывали в одном направлении, мы продолжаем двигаться туда
- $v_t$ — «крутизна». Если градиенты по этому параметру большие, мы делаем шаги меньше (чтобы не перепрыгнуть)
- Деление на $\sqrt{v_t}$ нормализует шаг: большие градиенты → маленький шаг, маленькие градиенты → большой шаг

**Стандартные гиперпараметры:**

- $\beta_1 = 0.9$ (инерция)
- $\beta_2 = 0.999$ (память о крутизне)
- $\eta = 0.001$ (learning rate)
- $\epsilon = 10^{-8}$ (для численной стабильности)

**Аналогия — умный турист:**

Обычный турист (SGD) делает шаг туда, куда указывает склон прямо сейчас.

Умный турист (Adam):

- Помнит, куда шёл раньше (момент $m$) — если долго шёл на восток, продолжает на восток даже если склон чуть-чуть на запад
- Помнит, насколько крутым был склон (дисперсия $v$) — на очень крутых участках делает осторожные маленькие шаги

---

### Сравнение оптимизаторов

|Оптимизатор|Скорость|Стабильность|Когда использовать|
|---|---|---|---|
|Batch GD|Медленно|Высокая|Маленькие датасеты, выпуклые задачи|
|SGD|Быстро|Низкая|Онлайн-обучение, очень большие данные|
|Mini-batch|Средне|Средняя|Стандартный выбор|
|Adam|Быстро|Высокая|Почти всегда хороший выбор|

---

## Вопрос 3. Алгоритм обратного распространения ошибки (Backpropagation)

### Проблема: как вычислить градиенты в глубокой сети?

Нейронная сеть может иметь миллионы параметров. Для градиентного спуска нам нужен градиент функции потерь по КАЖДОМУ параметру.

Как вычислить $\frac{\partial L}{\partial w}$ для каждого веса $w$?

**Наивный подход — численное дифференцирование:**

$$\frac{\partial L}{\partial w} \approx \frac{L(w + \epsilon) - L(w - \epsilon)}{2\epsilon}$$

Для каждого параметра нужно сделать 2 прямых прохода сети. Если параметров миллион — это 2 миллиона проходов. Неприемлемо!

**Решение — backpropagation:** вычислить все градиенты за ОДИН обратный проход.

### Правило цепочки (Chain Rule) — математическая основа

**Для функции одной переменной:**

Если $y = f(g(x))$ (композиция функций), то:

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$$

**Пример:**

$y = (3x + 2)^2$

Пусть $g = 3x + 2$, тогда $y = g^2$

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx} = 2g \cdot 3 = 6(3x + 2)$$

**Для нескольких переменных:**

Если $L = L(z)$, $z = z(a, b)$, $a = a(x)$, $b = b(x)$:

$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial a} \cdot \frac{\partial a}{\partial x} + \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial b} \cdot \frac{\partial b}{\partial x}$$

### Нейронная сеть как композиция функций

Нейронная сеть — это последовательность преобразований:

$$\mathbf{x} \xrightarrow{W_1, b_1} \mathbf{z}_1 \xrightarrow{\sigma} \mathbf{a}_1 \xrightarrow{W_2, b_2} \mathbf{z}_2 \xrightarrow{\sigma} \mathbf{a}_2 \xrightarrow{...} \hat{y} \xrightarrow{L} \text{Loss}$$

где:

- $\mathbf{z}_l = W_l \mathbf{a}_{l-1} + \mathbf{b}_l$ — линейное преобразование
- $\mathbf{a}_l = \sigma(\mathbf{z}_l)$ — нелинейная активация

Это длинная цепочка композиций! Правило цепочки позволяет разложить градиент по этой цепочке.

### Алгоритм Backpropagation

**Шаг 1: Прямой проход (Forward Pass)**

Вычисляем выходы всех слоёв, двигаясь от входа к выходу:

```
x → z₁ = W₁x + b₁ → a₁ = σ(z₁) → z₂ = W₂a₁ + b₂ → a₂ = σ(z₂) → ... → Loss
```

**Важно:** запоминаем все промежуточные значения ($\mathbf{z}_l$, $\mathbf{a}_l$), они понадобятся на обратном проходе.

**Шаг 2: Обратный проход (Backward Pass)**

Вычисляем градиенты, двигаясь от выхода к входу:

```
Loss → ∂L/∂aₗ → ∂L/∂zₗ → ∂L/∂Wₗ, ∂L/∂bₗ → ∂L/∂aₗ₋₁ → ...
```

**Формулы для каждого слоя $l$:**

1. **Градиент по входу слоя (до активации):** $$\frac{\partial L}{\partial \mathbf{z}_l} = \frac{\partial L}{\partial \mathbf{a}_l} \odot \sigma'(\mathbf{z}_l)$$
    
    где $\odot$ — поэлементное умножение, $\sigma'$ — производная активации.
    
2. **Градиент по весам:** $$\frac{\partial L}{\partial W_l} = \frac{\partial L}{\partial \mathbf{z}_l} \cdot \mathbf{a}_{l-1}^T$$
    
3. **Градиент по смещениям:** $$\frac{\partial L}{\partial \mathbf{b}_l} = \frac{\partial L}{\partial \mathbf{z}_l}$$
    
4. **Передача градиента на предыдущий слой:** $$\frac{\partial L}{\partial \mathbf{a}_{l-1}} = W_l^T \cdot \frac{\partial L}{\partial \mathbf{z}_l}$$
    

### Пример: сеть с одним скрытым слоем

**Архитектура:**

- Вход: $\mathbf{x} \in \mathbb{R}^2$
- Скрытый слой: $\mathbf{h} = \sigma(W_1 \mathbf{x} + \mathbf{b}_1) \in \mathbb{R}^3$
- Выход: $\hat{y} = W_2 \mathbf{h} + b_2 \in \mathbb{R}$
- Потери: $L = \frac{1}{2}(y - \hat{y})^2$

**Прямой проход:**

1. $\mathbf{z}_1 = W_1 \mathbf{x} + \mathbf{b}_1$
2. $\mathbf{h} = \sigma(\mathbf{z}_1)$ (например, sigmoid)
3. $\hat{y} = W_2 \mathbf{h} + b_2$
4. $L = \frac{1}{2}(y - \hat{y})^2$

**Обратный проход:**

1. $\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
    
2. $\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial \hat{y}} \cdot \mathbf{h}^T = (\hat{y} - y) \cdot \mathbf{h}^T$
    
3. $\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
    
4. $\frac{\partial L}{\partial \mathbf{h}} = W_2^T \cdot \frac{\partial L}{\partial \hat{y}} = W_2^T \cdot (\hat{y} - y)$
    
5. $\frac{\partial L}{\partial \mathbf{z}_1} = \frac{\partial L}{\partial \mathbf{h}} \odot \sigma'(\mathbf{z}_1)$
    
6. $\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{z}_1} \cdot \mathbf{x}^T$
    
7. $\frac{\partial L}{\partial \mathbf{b}_1} = \frac{\partial L}{\partial \mathbf{z}_1}$
    

### Аналогия — конвейер на заводе

Представьте завод по производству автомобилей с 10 станками на конвейере.

**Прямой проход:** детали проходят через все станки, на выходе — автомобиль. Контролёр проверяет качество и находит дефект.

**Обратный проход:** контролёр идёт по конвейеру НАЗАД и спрашивает каждый станок:

- «Насколько ТЫ виноват в этом дефекте?»
- Каждый станок отвечает пропорционально тому, насколько его настройки повлияли на дефект

**Правило цепочки говорит:** вина станка = (его влияние на следующий станок) × (вина следующего станка)

**Обновление:** каждый станок корректирует свои настройки (веса) пропорционально своей «вине».

---

## Вопрос 4. Матричные и тензорные операции. Почему они важны?

### Основные операции в нейронных сетях

**Полносвязный слой — умножение матрицы на вектор:**

$$\mathbf{z} = W\mathbf{x} + \mathbf{b}$$

где:

- $\mathbf{x} \in \mathbb{R}^n$ — вход (вектор из $n$ элементов)
- $W \in \mathbb{R}^{m \times n}$ — матрица весов
- $\mathbf{b} \in \mathbb{R}^m$ — вектор смещений
- $\mathbf{z} \in \mathbb{R}^m$ — выход (вектор из $m$ элементов)

**Batch processing — умножение матриц:**

Вместо обработки примеров по одному, обрабатываем сразу $k$ примеров:

$$Z = WX + B$$

где:

- $X \in \mathbb{R}^{n \times k}$ — batch из $k$ входных векторов
- $Z \in \mathbb{R}^{m \times k}$ — batch из $k$ выходных векторов

### Почему матричные операции эффективны

#### 1. Параллелизм на GPU

**CPU** имеет 4-16 мощных ядер — хорош для последовательных вычислений.

**GPU** имеет тысячи простых ядер — идеален для параллельных вычислений.

Умножение матриц $1000 \times 1000$ — это $10^9$ умножений. На GPU все эти умножения выполняются почти одновременно!

**Сравнение:**

|Операция|CPU (1 ядро)|CPU (8 ядер)|GPU|
|---|---|---|---|
|Умножение 1000×1000 матриц|~2000 мс|~300 мс|~1 мс|

GPU быстрее в 1000+ раз!

#### 2. Кэш-эффективность

Матричные операции обрабатывают данные последовательно в памяти. Это минимизирует «промахи кэша» (cache misses).

**Аналогия:** читать книгу страница за страницей (последовательно) быстрее, чем прыгать между случайными страницами.

#### 3. Оптимизированные библиотеки

Библиотеки BLAS, cuBLAS, cuDNN — это десятилетия оптимизаций:

- Умное использование кэша
- Векторные инструкции процессора (SIMD)
- Оптимальное распределение по ядрам GPU

#### 4. Batching — ещё один уровень параллелизма

Вместо обработки примеров по одному:

```python
for x in dataset:
    z = W @ x + b  # Много маленьких операций
```

Обрабатываем batch:

```python
Z = W @ X + B  # Одна большая операция
```

Это:

- Лучше утилизирует GPU
- Уменьшает накладные расходы на запуск операций
- Даёт более стабильные градиенты

### Интуитивная аналогия

**Задача:** посчитать 1000 независимых сумм по 1000 чисел каждая.

**Последовательно (цикл for):** нанимаем одного бухгалтера. Он считает суммы одну за другой. Занимает час.

**Параллельно (матричные операции на GPU):** нанимаем 1000 бухгалтеров. Каждый считает свою сумму. Все заканчивают за минуту.

Нейронные сети — это миллионы таких независимых вычислений. GPU делает их почти одновременно.

---

# РАЗДЕЛ 2. МНОГОСЛОЙНЫЙ ПЕРСЕПТРОН (MLP)

---

## Вопрос 5. Принципы построения и обучения глубоких сетей

### Почему глубокие сети?

**Теорема Цыбенко** (вопрос 7) говорит: одного скрытого слоя достаточно для аппроксимации любой функции.

Но на практике **глубокие** сети (много слоёв) работают лучше. Почему?

#### 1. Иерархическое представление признаков

Глубокая сеть выучивает признаки на разных уровнях абстракции:

**Пример — распознавание лиц:**

- **Слой 1:** детекторы краёв (вертикальные, горизонтальные, диагональные линии)
- **Слой 2:** комбинации краёв (углы, дуги, простые формы)
- **Слой 3:** части лица (глаза, носы, рты)
- **Слой 4:** целые лица, выражения

Каждый слой строится на предыдущем, создавая всё более сложные и абстрактные признаки.

#### 2. Эффективность представления

Глубокие сети могут представить сложные функции **экспоненциально** эффективнее, чем широкие и мелкие.

**Аналогия — представление чисел:**

- В унарной системе число 1000 требует 1000 символов: 111...1
- В десятичной системе: 4 символа
- В двоичной: 10 символов

Глубина — как более эффективная система счисления для функций.

#### 3. Композиционность

Реальный мир иерархичен и композиционален:

- Атомы → молекулы → клетки → органы → организмы
- Буквы → слова → предложения → абзацы → документы

Глубокие сети естественно моделируют такую структуру.

### Принципы обучения глубоких сетей

#### 1. Правильная инициализация весов

**Проблема:** неправильная инициализация приводит к затуханию или взрыву активаций/градиентов.

**Xavier/Glorot инициализация** (для tanh, sigmoid): $$W \sim \mathcal{N}\left(0, \frac{2}{n_{in} + n_{out}}\right)$$

**He инициализация** (для ReLU): $$W \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)$$

**Идея:** начальные активации должны иметь примерно одинаковую дисперсию во всех слоях.

#### 2. Batch Normalization

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$ $$y_i = \gamma \hat{x}_i + \beta$$

**Что делает:**

- Нормализует активации в каждом слое (среднее = 0, дисперсия = 1)
- $\gamma$, $\beta$ — обучаемые параметры для масштабирования и сдвига

**Зачем:**

- Стабилизирует обучение
- Позволяет использовать большие learning rate
- Действует как регуляризатор

#### 3. Residual Connections (Skip Connections)

$$\mathbf{y} = F(\mathbf{x}) + \mathbf{x}$$

**Идея:** добавить «шоссе» для информации и градиентов в обход слоёв.

**Почему работает:**

- Градиент может течь напрямую, без затухания
- Легче обучить $F(\mathbf{x}) = 0$ (тождественное отображение), чем $F(\mathbf{x}) = \mathbf{x}$
- Позволяет обучать сети глубиной 100+ слоёв (ResNet)

#### 4. Dropout

Во время обучения случайно «выключаем» нейроны с вероятностью $p$.

**Зачем:**

- Регуляризация — борьба с переобучением
- Ансамблирование — каждый проход = разная подсеть
- Предотвращает ко-адаптацию нейронов

---

## Вопрос 6. Архитектура MLP: входной, скрытые и выходной слои

### Структура многослойного персептрона

```
      Входной слой         Скрытые слои           Выходной слой
          (d)                (n₁, n₂, ...)              (k)
           │                     │                       │
     ┌─────┴─────┐        ┌──────┴──────┐         ┌──────┴──────┐
     │ x₁  ...xd │   →    │ h₁⁽¹⁾...h_{n₁}⁽¹⁾ │  →  │ y₁  ...  yₖ │
     └───────────┘        └─────────────┘         └─────────────┘
           │                     │                       │
        признаки         нелинейные              предсказания
                        преобразования
```

### Входной слой

**Функция:** принимает входные данные (признаки объекта).

**Размер:** равен количеству признаков $d$.

**Примеры:**

- Изображение 28×28: $d = 784$ (пиксели развёрнуты в вектор)
- Табличные данные с 10 признаками: $d = 10$

**Входной слой не делает вычислений** — это просто «точка входа» данных в сеть.

### Скрытые слои

**Функция:** последовательные нелинейные преобразования входных данных.

Для слоя $l$: $$\mathbf{h}^{(l)} = \sigma(W_l \mathbf{h}^{(l-1)} + \mathbf{b}_l)$$

где:

- $\mathbf{h}^{(l-1)}$ — выход предыдущего слоя (или вход для первого скрытого слоя)
- $W_l$ — матрица весов
- $\mathbf{b}_l$ — вектор смещений
- $\sigma$ — функция активации (ReLU, sigmoid, tanh)

**Что происходит в скрытом слое интуитивно:**

1. Каждый нейрон «смотрит» на все входы с разными весами
2. Суммирует взвешенные входы
3. Применяет нелинейную активацию
4. Выдаёт одно число — свою «активацию»

**Аналогия — комитет экспертов:**

Каждый нейрон — это эксперт, который:

- Получает все данные
- Обращает разное внимание на разные аспекты (веса)
- Выносит своё суждение (активация)

Следующий слой — комитет экспертов более высокого уровня, который анализирует мнения предыдущего комитета.

### Выходной слой

**Функция:** формирует финальное предсказание в нужном формате.

**Зависит от задачи:**

|Задача|Размер выхода|Активация|
|---|---|---|
|Бинарная классификация|1|sigmoid|
|Многоклассовая классификация (K классов)|K|softmax|
|Регрессия|1 (или больше)|нет (линейный выход)|

**Пример для MNIST (10 цифр):**

Выходной слой: 10 нейронов с softmax. Выход: вектор из 10 вероятностей, сумма = 1. Например: $[0.01, 0.01, 0.02, 0.90, 0.01, ...]$ → предсказание: цифра 3.

### Подсчёт параметров

Для слоя с $n_{in}$ входами и $n_{out}$ выходами:

- Веса: $n_{in} \times n_{out}$
- Смещения: $n_{out}$
- **Всего:** $n_{out} \times (n_{in} + 1)$

**Пример: сеть для MNIST**

Архитектура: $[784 \to 256 \to 128 \to 10]$

|Слой|Размер|Параметры|
|---|---|---|
|1: 784→256|$256 \times 784 + 256$|200,960|
|2: 256→128|$128 \times 256 + 128$|32,896|
|3: 128→10|$10 \times 128 + 10$|1,290|
|**Всего**||**235,146**|

### Аналогия — система принятия решений в компании

**Входной слой** — отдел сбора данных. Собирает всю информацию извне.

**Первый скрытый слой** — младшие аналитики. Каждый специализируется на своём аспекте данных.

**Второй скрытый слой** — старшие аналитики. Синтезируют отчёты младших, находят более глубокие закономерности.

**Выходной слой** — руководство. Принимает финальное решение на основе всех анализов.

---

## Вопрос 7. Универсальная аппроксимационная теорема (теорема Цыбенко)

### Формулировка теоремы

**Теорема (Cybenko, 1989):**

Пусть $\sigma$ — непрерывная сигмоидальная функция (например, sigmoid). Тогда для любой непрерывной функции $f: [0,1]^d \to \mathbb{R}$ и любого $\epsilon > 0$ существует нейронная сеть с **одним скрытым слоем**:

$$F(\mathbf{x}) = \sum_{j=1}^{N} \alpha_j \sigma(\mathbf{w}_j^T \mathbf{x} + b_j)$$

такая, что для всех $\mathbf{x} \in [0,1]^d$:

$$|F(\mathbf{x}) - f(\mathbf{x})| < \epsilon$$

### Простыми словами

**Нейронная сеть с одним скрытым слоем и достаточным количеством нейронов может приблизить ЛЮБУЮ непрерывную функцию с ЛЮБОЙ точностью.**

### Значение теоремы

1. **Теоретическое обоснование:** нейронные сети — универсальные аппроксиматоры. Они не ограничены в том, какие функции могут представить.
    
2. **Гарантия существования:** для любой задачи (если она может быть описана непрерывной функцией) существует нейронная сеть, которая её решает.
    
3. **Мотивация:** если решение существует, имеет смысл его искать.
    

### Ограничения теоремы — очень важно понимать!

#### 1. Теорема экзистенциальная, не конструктивная

Теорема говорит: «Такая сеть СУЩЕСТВУЕТ».

Теорема НЕ говорит:

- Сколько нейронов нужно?
- Какие веса должны быть?
- Как найти эти веса?
- За какое время можно обучить?

#### 2. Количество нейронов может быть ЭКСПОНЕНЦИАЛЬНО большим

Для некоторых функций количество нейронов в одном слое растёт экспоненциально с размерностью входа.

Например, для аппроксимации функции в $\mathbb{R}^{100}$ может потребоваться $2^{100}$ нейронов — больше, чем атомов во Вселенной!

#### 3. Глубокие сети эффективнее на практике

Хотя теоретически одного слоя достаточно, **глубокие сети**:

- Требуют экспоненциально меньше нейронов
- Лучше выделяют иерархические признаки
- Легче обучаются на практике

**Аналогия — представление функций:**

Функцию можно представить как полином. Теорема Вейерштрасса говорит, что любую непрерывную функцию можно приблизить полиномами.

Но для сложных функций степень полинома (= количество нейронов) может быть огромной.

Глубокие сети — как более умная система представления функций.

#### 4. Не гарантирует обобщение

Теорема говорит об аппроксимации на заданном множестве (обучающей выборке).

Она НЕ гарантирует, что модель будет хорошо работать на новых данных. Можно идеально «запомнить» обучающую выборку и провалиться на тесте.

### Интуитивная аналогия

**Теорема говорит:** «С достаточным количеством точек можно нарисовать любую картину».

Технически это правда — если у вас бесконечно много пикселей, вы можете изобразить что угодно.

**Но:**

- Для «Моны Лизы» вам нужно, может быть, миллиард пикселей
- Теорема не говорит, какого цвета должен быть каждый пиксель
- Даже с правильными пикселями — как их расставить?

**Глубокие сети** — это как использование не пикселей, а мазков кистью. Художник может нарисовать картину с гораздо меньшим количеством движений, потому что каждый мазок несёт больше информации.

---

## Вопрос 8. Роль нелинейности в скрытых слоях MLP

### Что будет без нелинейности — математическое доказательство

Рассмотрим сеть с двумя слоями **без нелинейной активации**:

Слой 1: $\mathbf{h} = W_1 \mathbf{x} + \mathbf{b}_1$

Слой 2: $\hat{\mathbf{y}} = W_2 \mathbf{h} + \mathbf{b}_2$

Подставим первое во второе:

$$\hat{\mathbf{y}} = W_2 (W_1 \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2$$ $$\hat{\mathbf{y}} = W_2 W_1 \mathbf{x} + W_2 \mathbf{b}_1 + \mathbf{b}_2$$ $$\hat{\mathbf{y}} = \underbrace{(W_2 W_1)}_{W'} \mathbf{x} + \underbrace{(W_2 \mathbf{b}_1 + \mathbf{b}_2)}_{\mathbf{b}'}$$ $$\hat{\mathbf{y}} = W' \mathbf{x} + \mathbf{b}'$$

**Результат:** два линейных слоя эквивалентны ОДНОМУ линейному слою с матрицей $W' = W_2 W_1$.

**Обобщение:** сколько бы линейных слоёв мы ни складывали, результат — всегда один линейный слой!

$$W_n W_{n-1} ... W_2 W_1 = W'$$

### Что может и чего не может линейная модель

**Линейная модель** разделяет пространство **гиперплоскостью**.

В 2D — прямая линия. В 3D — плоскость. В $n$-мерном пространстве — $(n-1)$-мерная гиперплоскость.

**Линейно разделимые данные:**

```
     y
     │    o o o
     │   o o o o
     │  ─────────── граница (прямая)
     │    x x x
     │   x x x x
     └───────────── x
```

Линейный классификатор справляется отлично.

**Линейно НЕразделимые данные (XOR):**

```
     y
     │  x       o
     │     
     │  o       x
     └───────────── x
```

|$x_1$|$x_2$|XOR|
|---|---|---|
|0|0|0 (x)|
|0|1|1 (o)|
|1|0|1 (o)|
|1|1|0 (x)|

**Невозможно провести прямую линию, разделяющую классы!**

Это знаменитая проблема, из-за которой исследования нейронных сетей «заморозились» на десятилетия после работы Минского и Паперта (1969).

### Что даёт нелинейность

#### 1. Разрыв линейности

Нелинейная активация «ломает» цепочку линейных преобразований:

$$\mathbf{h} = \sigma(W_1 \mathbf{x} + \mathbf{b}_1)$$ $$\hat{\mathbf{y}} = W_2 \mathbf{h} + \mathbf{b}_2$$

Теперь это НЕ эквивалентно одному линейному слою из-за $\sigma$ между ними.

#### 2. Сложные границы решений

С нелинейностью сеть может создавать **произвольно сложные** границы между классами:

```
     y
     │  x    ╭──╮  o
     │    ╭──╯  ╰──╮
     │  o ╰────────╯ x
     └───────────── x
```

#### 3. Иерархия признаков

Каждый слой с нелинейностью создаёт **новое пространство признаков**, в котором данные могут быть легче разделимы.

### Как MLP решает XOR

**Скрытый слой** преобразует пространство так, что XOR становится линейно разделимой!

**Пример с 2 скрытыми нейронами:**

```
Исходное пространство:          После скрытого слоя:
     x₂                              h₂
     │  (0,1)●     ●(1,1)            │      ●(1,1) 
     │       1     0                 │       0
     │                               │
     │  (0,0)●     ●(1,0)            │  ●(0,0)  ●(1,0)
     │       0     1                 │   0       1
     └─────────── x₁                 └─────────── h₁
                                         ↑
                                   Теперь линейно
                                   разделимы!
```

Скрытый слой «переставляет» точки так, что классы можно разделить прямой линией.

### Аналогия — изгибание бумаги

Представьте точки на листе бумаги, которые нельзя разделить прямой линией.

**Линейный классификатор:** может только провести линию на плоском листе.

**Нелинейная сеть:** может согнуть, смять, скрутить бумагу так, что нужные точки окажутся рядом, а ненужные — далеко. Затем проводит «линию» в этом изогнутом пространстве.

---

## Вопрос 9. Матричное исчисление, Softmax, категориальная кросс-энтропия

### Матричное исчисление — основы

#### Производная скаляра по вектору

Если $L \in \mathbb{R}$ — скаляр, $\mathbf{x} \in \mathbb{R}^n$ — вектор:

$$\frac{\partial L}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial L}{\partial x_1} \ \frac{\partial L}{\partial x_2} \ \vdots \ \frac{\partial L}{\partial x_n} \end{pmatrix} \in \mathbb{R}^n$$

**Пример:** $L = x_1^2 + 2x_2^2$

$$\frac{\partial L}{\partial \mathbf{x}} = \begin{pmatrix} 2x_1 \ 4x_2 \end{pmatrix}$$

#### Матрица Якоби (производная вектора по вектору)

Если $\mathbf{y} \in \mathbb{R}^m$, $\mathbf{x} \in \mathbb{R}^n$:

$$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{pmatrix} \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \ \vdots & \ddots & \vdots \ \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n} \end{pmatrix} \in \mathbb{R}^{m \times n}$$

#### Полезные формулы для нейросетей

1. $\frac{\partial}{\partial \mathbf{x}}(W\mathbf{x}) = W$
    
2. $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{x}) = 2\mathbf{x}$
    
3. $\frac{\partial}{\partial W}(W\mathbf{x}) = \mathbf{x}^T$ (в смысле: $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \mathbf{z}} \mathbf{x}^T$)
    
4. $\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^T \mathbf{x}) = \mathbf{a}$
    

### Softmax — подробный разбор

#### Формула

Для вектора $\mathbf{z} = (z_1, z_2, ..., z_K)$:

$$\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

#### Что делает Softmax

1. **Экспонента** $e^{z_i}$: преобразует любое число в положительное
2. **Нормализация** (деление на сумму): превращает в вероятности (сумма = 1)

#### Пример расчёта

$\mathbf{z} = [2.0, 1.0, 0.1]$

1. Экспоненты: $[e^{2.0}, e^{1.0}, e^{0.1}] = [7.39, 2.72, 1.10]$
2. Сумма: $7.39 + 2.72 + 1.10 = 11.21$
3. Softmax: $[\frac{7.39}{11.21}, \frac{2.72}{11.21}, \frac{1.10}{11.21}] = [0.659, 0.243, 0.098]$

**Интерпретация:** модель «думает», что с вероятностью 65.9% это класс 1, 24.3% — класс 2, 9.8% — класс 3.

#### Свойства Softmax

1. **Выход в $(0, 1)$**: каждый элемент — положительное число меньше 1
2. **Сумма = 1**: $\sum_i \text{softmax}(\mathbf{z})_i = 1$
3. **Монотонность**: больший $z_i$ → больший $\text{softmax}(\mathbf{z})_i$
4. **Температура**: softmax$(z/T)$ — при $T \to 0$ становится «жёстким» (argmax), при $T \to \infty$ — равномерным

#### Производная Softmax

$$\frac{\partial \text{softmax}_i}{\partial z_j} = \text{softmax}_i \cdot (\delta_{ij} - \text{softmax}_j)$$

где $\delta_{ij} = 1$ если $i = j$, иначе $0$.

### Категориальная кросс-энтропия

#### Формула

$$L = -\sum_{k=1}^{K} y_k \log(\hat{y}_k)$$

где:

- $\mathbf{y}$ — one-hot вектор правильного класса
- $\hat{\mathbf{y}} = \text{softmax}(\mathbf{z})$ — предсказанные вероятности

#### Упрощение для one-hot

Если правильный класс — $c$ (то есть $y_c = 1$, остальные $y_k = 0$):

$$L = -\log(\hat{y}_c) = -\log(\text{softmax}(\mathbf{z})_c)$$

#### Красивый градиент Softmax + Cross-Entropy

$$\frac{\partial L}{\partial \mathbf{z}} = \hat{\mathbf{y}} - \mathbf{y} = \text{softmax}(\mathbf{z}) - \mathbf{y}$$

**Это невероятно простая и красивая формула!**

**Пример:**

- Правильный класс: 2 (one-hot: $\mathbf{y} = [0, 0, 1]$)
- Предсказание: $\hat{\mathbf{y}} = [0.3, 0.5, 0.2]$
- Градиент: $\frac{\partial L}{\partial \mathbf{z}} = [0.3, 0.5, 0.2] - [0, 0, 1] = [0.3, 0.5, -0.8]$

Модель получает «штраф» за слишком высокую уверенность в неправильных классах и «поощрение» за правильный класс.

---

# РАЗДЕЛ 3. ФУНКЦИИ АКТИВАЦИИ

---

## Вопрос 10. Основные функции активации: ReLU, sigmoid, tanh, softmax

### Sigmoid (Логистическая функция)

#### Формула

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

#### Производная

$$\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))$$

**Максимум производной:** $\sigma'(0) = 0.25$

#### График

```
σ(x)
  1 │            ╭───────
    │          ╱
0.5 │        ╱
    │      ╱
  0 │─────╯
    └─────────────────── x
        -4  -2   0   2   4
```

#### Свойства

|Плюсы|Минусы|
|---|---|
|Гладкая, дифференцируема везде|Затухающие градиенты ($\sigma' \leq 0.25$)|
|Выход в $(0, 1)$ — интерпретируется как вероятность|Выход не центрирован (всегда положительный)|
|Исторически первая, хорошо изучена|Вычислительно дорогая (экспонента)|
||«Насыщается» при больших $|

#### Когда использовать

- Выходной слой для бинарной классификации
- Гейты в LSTM/GRU (нужен выход в $(0, 1)$)
- **НЕ рекомендуется** для скрытых слоёв глубоких сетей

---

### Tanh (Гиперболический тангенс)

#### Формула

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

#### Производная

$$\tanh'(x) = 1 - \tanh^2(x)$$

**Максимум производной:** $\tanh'(0) = 1$

#### График

```
tanh(x)
  1 │            ╭───────
    │          ╱
  0 │────────╱────────────
    │      ╱
 -1 │─────╯
    └─────────────────── x
        -4  -2   0   2   4
```

#### Свойства

|Плюсы|Минусы|
|---|---|
|Выход центрирован: $(-1, 1)$|Затухающие градиенты (хотя лучше sigmoid)|
|Более сильные градиенты, чем sigmoid|Вычислительно дорогая|
|Нулевое среднее → лучше для обучения|Насыщается при больших $|

#### Когда использовать

- RNN (особенно в LSTM для $\tanh$ активации)
- Когда нужен центрированный выход
- **Редко** в современных глубоких сетях

---

### ReLU (Rectified Linear Unit)

#### Формула

$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x, & x > 0 \ 0, & x \leq 0 \end{cases}$$

#### Производная

$$\text{ReLU}'(x) = \begin{cases} 1, & x > 0 \ 0, & x \leq 0 \end{cases}$$

(Технически не определена в $x = 0$, на практике берут 0)

#### График

```
ReLU(x)
    │        ╱
    │      ╱
    │    ╱
  0 │───╱────────────────
    │
    └─────────────────── x
       -2  -1   0   1   2
```

#### Свойства

|Плюсы|Минусы|
|---|---|
|Очень быстрое вычисление (просто max)|«Мёртвые нейроны» — если $x < 0$ всегда, градиент = 0 навсегда|
|Не насыщается для $x > 0$|Не центрирована (выход ≥ 0)|
|Разреженная активация (~50% нейронов = 0)|Не дифференцируема в $x = 0$|
|Помогает с затуханием градиентов||

#### Проблема «мёртвых нейронов»

Если $W\mathbf{x} + b < 0$ для всех примеров, нейрон выдаёт 0, градиент = 0, веса не обновляются. Нейрон «умирает».

Это может произойти при:

- Плохой инициализации
- Слишком большом learning rate

#### Когда использовать

- **Стандарт** для скрытых слоёв CNN и MLP
- Не подходит для выходного слоя

---

### Softmax

#### Формула

$$\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

#### Особенности

- Применяется к **вектору**, не к скаляру
- Выход — **вероятностное распределение** (сумма = 1)
- Используется **только в выходном слое** для многоклассовой классификации

### Сравнительная таблица

|Функция|Диапазон|Производная max|Центрирована|Скорость|
|---|---|---|---|---|
|Sigmoid|$(0, 1)$|0.25|Нет|Медленная|
|Tanh|$(-1, 1)$|1.0|Да|Медленная|
|ReLU|$[0, +\infty)$|1.0|Нет|Быстрая|
|Softmax|$(0, 1)$, сумма=1|—|—|Средняя|

---

## Вопрос 11. Проблема затухающих градиентов

### Математическое описание

При обратном распространении градиент передаётся через слои:

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \mathbf{a}_L} \cdot \prod_{l=2}^{L} \left( \frac{\partial \mathbf{a}_l}{\partial \mathbf{z}_l} \cdot \frac{\partial \mathbf{z}_l}{\partial \mathbf{a}_{l-1}} \right) \cdot \frac{\partial \mathbf{a}_1}{\partial \mathbf{z}_1} \cdot \frac{\partial \mathbf{z}_1}{\partial W_1}$$

Ключевой множитель: $\frac{\partial \mathbf{a}_l}{\partial \mathbf{z}_l} = \sigma'(\mathbf{z}_l)$ — производная активации.

### Анализ для Sigmoid

Производная sigmoid: $\sigma'(x) = \sigma(x)(1 - \sigma(x)) \leq 0.25$

При $L = 10$ слоях: $$\prod_{l=1}^{10} \sigma'(z_l) \leq 0.25^{10} = 9.5 \times 10^{-7}$$

**Градиент уменьшается в миллион раз!**

### Визуализация

```
Слой:       1        2        3        4        5
            │        │        │        │        │
Sigmoid: × 0.25 → × 0.25 → × 0.25 → × 0.25 → × 0.25
            │        │        │        │        │
Градиент: 1.0  →  0.25  → 0.0625 → 0.0156 → 0.0039
            ↓        ↓        ↓        ↓        ↓
        Первые слои получают почти нулевой градиент!
```

### Последствия

1. **Первые слои не обучаются:** градиенты слишком малы для значимого обновления весов
2. **Медленная сходимость:** нужно огромное количество эпох
3. **Застревание:** сеть не может выбраться из плохого решения

### Какие активации усугубляют проблему

**Sigmoid:** $\sigma'(x) \leq 0.25$ — самая проблемная

**Tanh:** $\tanh'(x) \leq 1$, но при насыщении ($|x| > 2$) производная близка к 0

### Решения

1. **ReLU:** $\text{ReLU}'(x) = 1$ для $x > 0$ — не затухает!
    
2. **Правильная инициализация:** Xavier, He — сохраняют дисперсию активаций
    
3. **Batch Normalization:** нормализует активации, предотвращает насыщение
    
4. **Residual Connections:** градиент может течь напрямую через skip-connection
    
5. **LSTM/GRU для RNN:** специальные гейты для контроля потока градиента
    

### Аналогия — испорченный телефон

Представьте игру «испорченный телефон» с 10 участниками:

**Sigmoid/Tanh:** каждый участник не только искажает сообщение, но и говорит ТИШЕ. К 10-му участнику доходит лишь шёпот — практически тишина.

**ReLU:** каждый участник говорит с той же громкостью (или молчит совсем). Если все говорят, сообщение доходит чётко.

**ResNet:** у каждого участника есть прямая линия связи с первым. Даже если цепочка «испортилась», можно передать сообщение напрямую.

---

## Вопрос 12. Современные функции активации: Leaky ReLU, ELU, GELU

### Проблема ReLU — «мёртвые нейроны»

Если $z < 0$, то $\text{ReLU}(z) = 0$ и $\text{ReLU}'(z) = 0$.

Если нейрон «застрял» в отрицательной зоне для всех примеров, он никогда не обновится — он «мёртв».

### Leaky ReLU

#### Формула

$$\text{LeakyReLU}(x) = \begin{cases} x, & x > 0 \ \alpha x, & x \leq 0 \end{cases}$$

где $\alpha$ — маленькое число (обычно 0.01).

#### Производная

$$\text{LeakyReLU}'(x) = \begin{cases} 1, & x > 0 \ \alpha, & x \leq 0 \end{cases}$$

#### График

```
LeakyReLU(x)
    │        ╱
    │      ╱
    │    ╱
  0 │──╱──────────────── (маленький наклон α при x<0)
    │╱
    └─────────────────── x
```

#### Преимущество

Градиент **никогда** не равен нулю! Даже при $x < 0$ градиент = $\alpha \neq 0$.

Нейроны не могут полностью «умереть».

### Parametric ReLU (PReLU)

То же, что Leaky ReLU, но $\alpha$ — **обучаемый параметр**.

Сеть сама определяет оптимальный наклон для отрицательных значений.

### ELU (Exponential Linear Unit)

#### Формула

$$\text{ELU}(x) = \begin{cases} x, & x > 0 \ \alpha(e^x - 1), & x \leq 0 \end{cases}$$

#### Производная

$$\text{ELU}'(x) = \begin{cases} 1, & x > 0 \ \text{ELU}(x) + \alpha, & x \leq 0 \end{cases}$$

#### График

```
ELU(x)
    │        ╱
    │      ╱
  0 │────╱────────────────
    │  ╱
 -α │─╯    (плавно насыщается к -α)
    └─────────────────── x
```

#### Преимущества

1. **Гладкая функция** — дифференцируема везде (в отличие от ReLU)
2. **Центрирована около нуля** — отрицательные значения «тянут» среднее вниз
3. **Насыщение для отрицательных значений** добавляет устойчивость к шуму

### GELU (Gaussian Error Linear Unit)

#### Формула

$$\text{GELU}(x) = x \cdot \Phi(x)$$

где $\Phi(x)$ — функция распределения стандартного нормального распределения.

#### Приближённая формула (для вычислений)

$$\text{GELU}(x) \approx 0.5x\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right]\right)$$

#### График

```
GELU(x)
    │        ╱
    │      ╱
  0 │────╱────────────────
    │  ╱
    │╱   (небольшой «провал» при x<0)
    └─────────────────── x
```

#### Вероятностная интерпретация

GELU можно интерпретировать как стохастическую регуляризацию:

- Вход $x$ «пропускается» с вероятностью $\Phi(x)$
- Большие положительные $x$ почти всегда проходят
- Отрицательные $x$ редко проходят
- Около нуля — «мягкое» решение

#### Где используется

**GELU — стандарт в трансформерах!** (BERT, GPT, и др.)

### Swish / SiLU

#### Формула

$$\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}$$

#### Свойства

- Гладкая
- Не ограничена сверху
- Ограничена снизу (минимум ≈ -0.278)
- Похожа на ReLU, но гладкая

**Используется в EfficientNet и других современных архитектурах.**

### Сравнительная таблица

|Функция|Гладкая|Нет мёртвых нейронов|Центрирована|Типичное применение|
|---|---|---|---|---|
|ReLU|Нет|Нет|Нет|CNN, MLP (классика)|
|Leaky ReLU|Нет|✓|Нет|CNN, MLP|
|ELU|✓|✓|~Да|CNN, MLP|
|GELU|✓|✓|Нет|Transformers (BERT, GPT)|
|Swish/SiLU|✓|✓|Нет|EfficientNet, современные CNN|

### Какую выбрать?

**Практические рекомендации:**

1. **Начните с ReLU** — простой, быстрый, работает в большинстве случаев
    
2. **Если много «мёртвых нейронов»** → Leaky ReLU или ELU
    
3. **Для трансформеров** → GELU (стандарт)
    
4. **Если хотите максимальное качество** → экспериментируйте с Swish/SiLU, GELU
    

# Вопросы 24-28 (продолжение)

---

## Вопрос 24. Разрежённые и denoising автоэнкодеры

### Проблема обычных автоэнкодеров

**Что если латентное пространство слишком большое?**

Если $\dim(\mathbf{z}) \geq \dim(\mathbf{x})$, автоэнкодер может просто выучить **тождественное отображение**:

$$f_{enc}(\mathbf{x}) = \mathbf{x}, \quad f_{dec}(\mathbf{z}) = \mathbf{z}$$

Никакого полезного сжатия не происходит!

**Решение:** добавить **регуляризацию** на латентное представление.

---

### Разрежённые автоэнкодеры (Sparse Autoencoders)

**Идея:** заставить большинство нейронов латентного слоя быть «выключенными» (близкими к нулю).

**Функция потерь:**

$$L = L_{rec} + \lambda \cdot L_{sparse}$$

где:

- $L_{rec} = |\mathbf{x} - \hat{\mathbf{x}}|^2$ — ошибка реконструкции
- $L_{sparse}$ — штраф за неразреженность
- $\lambda$ — коэффициент регуляризации

#### Вариант 1: L1-регуляризация

$$L_{sparse} = \sum_{j=1}^{d} |z_j|$$

**Пример расчёта:**

Пусть $\mathbf{z} = [0.8, 0.1, 0.9, 0.05, 0.7]$

$$L_{sparse} = |0.8| + |0.1| + |0.9| + |0.05| + |0.7| = 2.55$$

L1-штраф **поощряет** многие $z_j$ быть ровно нулём.

#### Вариант 2: KL-дивергенция

**Идея:** хотим, чтобы средняя активация каждого нейрона была близка к малому числу $\rho$ (например, $\rho = 0.05$).

**Средняя активация нейрона $j$:**

$$\hat{\rho}_j = \frac{1}{N} \sum_{i=1}^{N} z_j^{(i)}$$

**KL-дивергенция:**

$$L_{sparse} = \sum_{j=1}^{d} KL(\rho | \hat{\rho}_j) = \sum_{j=1}^{d} \left[ \rho \log\frac{\rho}{\hat{\rho}_j} + (1-\rho) \log\frac{1-\rho}{1-\hat{\rho}_j} \right]$$

**Пример расчёта:**

Целевая разреженность: $\rho = 0.05$ Фактическая активация нейрона 1: $\hat{\rho}_1 = 0.8$

$$KL(0.05 | 0.8) = 0.05 \log\frac{0.05}{0.8} + 0.95 \log\frac{0.95}{0.2}$$ $$= 0.05 \times (-2.77) + 0.95 \times 1.56 = -0.14 + 1.48 = 1.34$$

Большой штраф! Нейрон слишком активен.

Если бы $\hat{\rho}_1 = 0.05$: $$KL(0.05 | 0.05) = 0$$

Нет штрафа — нейрон достаточно разрежен.

### Что даёт разреженность

1. **Интерпретируемость:** каждый нейрон отвечает за конкретный признак
2. **Робастность:** сеть не полагается на все нейроны одновременно
3. **Эффективность:** большинство нейронов = 0, можно использовать разреженные вычисления

**Аналогия:**

Обычный код: «Эта картинка — 50% кошка, 30% собака, 20% птица»

Разреженный код: «Эта картинка — 100% кошка, 0% собака, 0% птица»

Разреженное представление более чёткое и интерпретируемое.

---

### Denoising автоэнкодеры (DAE)

**Идея:** обучить автоэнкодер восстанавливать **чистые** данные из **зашумлённых**.

#### Алгоритм обучения DAE

**Шаг 1:** Берём чистый пример $\mathbf{x}$

**Шаг 2:** Добавляем шум: $\tilde{\mathbf{x}} = \text{corrupt}(\mathbf{x})$

**Шаг 3:** Кодируем зашумлённый вход: $\mathbf{z} = f_{enc}(\tilde{\mathbf{x}})$

**Шаг 4:** Декодируем: $\hat{\mathbf{x}} = f_{dec}(\mathbf{z})$

**Шаг 5:** Вычисляем потери относительно **ЧИСТОГО** $\mathbf{x}$:

$$L = |\mathbf{x} - \hat{\mathbf{x}}|^2$$

**Ключевое отличие:** сравниваем с оригиналом, а не с зашумлённым входом!

#### Типы шума

**1. Гауссовский шум (для непрерывных данных):**

$$\tilde{\mathbf{x}} = \mathbf{x} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I)$$

**Пример:**

```
Оригинал:    [0.5, 0.8, 0.2]
+ шум N(0, 0.1²): [0.03, -0.05, 0.08]
= Зашумлённый: [0.53, 0.75, 0.28]
```

**2. Маскирующий шум (Masking / Dropout noise):**

$$\tilde{x}_i = \begin{cases} x_i, & \text{с вероятностью } 1-p \ 0, & \text{с вероятностью } p \end{cases}$$

**Пример (p = 0.3):**

```
Оригинал:    [0.5, 0.8, 0.2, 0.9, 0.4]
Маска:       [1,   0,   1,   1,   0  ]
Зашумлённый: [0.5, 0,   0.2, 0.9, 0  ]
```

**3. Salt-and-pepper шум (для изображений):**

$$\tilde{x}_i = \begin{cases} x_i, & \text{с вероятностью } 1-p \ 0, & \text{с вероятностью } p/2 \ 1, & \text{с вероятностью } p/2 \end{cases}$$

#### Почему DAE работает

**Математическое обоснование:**

DAE максимизирует нижнюю границу log-правдоподобия:

$$\log p(\mathbf{x}) \geq \mathbb{E}_{q(\tilde{\mathbf{x}}|\mathbf{x})}[\log p(\mathbf{x}|\tilde{\mathbf{x}})]$$

**Интуиция:**

Чтобы восстановить чистый сигнал из зашумлённого, сеть должна **понять структуру** данных:

- Что является «настоящим» сигналом
- Что является шумом

**Аналогия:**

Представьте, что вы учите человека читать размытые фотографии документов:

- Вы показываете: «Вот размытое → вот чёткое»
- Человек учится понимать структуру букв, слов, предложений
- Теперь он может восстанавливать даже сильно повреждённые документы

#### Преимущества DAE

1. **Более устойчивые признаки:** не зависят от шума
2. **Лучшая обобщающая способность:** сеть видела разные версии данных
3. **Инвариантность к возмущениям:** небольшие изменения входа не меняют код

---

# РАЗДЕЛ 7. ВАРИАЦИОННЫЕ АВТОЭНКОДЕРЫ (VAE)

---

## Вопрос 25. Основная идея вариационного вывода в VAE

### Проблема обычного автоэнкодера для генерации

**Обычный автоэнкодер** создаёт **детерминированное** отображение $\mathbf{x} \to \mathbf{z}$:

- Каждому входу соответствует ровно одна точка в латентном пространстве
- Латентное пространство может быть «рваным» — между точками пустоты
- Нет естественного способа сэмплировать новые примеры

**Визуализация проблемы:**

```
Латентное пространство обычного AE:

      z₂
       │    •            •
       │         •
       │  •           •
       │       [пусто]
       │    •      •
       └────────────────── z₁
       
Генерация из [пусто] → бессмысленный результат
```

### Идея VAE

**VAE** трактует латентное представление как **случайную величину** с известным распределением.

**Вместо:** $\mathbf{z} = f_{enc}(\mathbf{x})$ (детерминированно)

**Используем:** $\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})$ (распределение)

#### Генеративная модель VAE

1. Сэмплируем из **априорного распределения**: $\mathbf{z} \sim p(\mathbf{z}) = \mathcal{N}(0, I)$
2. Генерируем данные через **декодировщик**: $\mathbf{x} \sim p_\theta(\mathbf{x}|\mathbf{z})$

**Графическая модель:**

```
    z     →     x
    ↑           
 p(z)=N(0,I)   p(x|z)=Decoder
```

### Проблема: интрактабельный интеграл

**Хотим:** максимизировать правдоподобие данных $p(\mathbf{x})$.

**Правдоподобие:**

$$p(\mathbf{x}) = \int p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}$$

**Проблема:** интеграл по всему латентному пространству — невозможно вычислить аналитически!

### Решение: вариационный вывод

**Идея:** вместо точного $p(\mathbf{z}|\mathbf{x})$ используем приближение $q_\phi(\mathbf{z}|\mathbf{x})$.

**$q_\phi(\mathbf{z}|\mathbf{x})$** — это **encoder**, который для каждого $\mathbf{x}$ выдаёт распределение на $\mathbf{z}$.

**Выбор формы $q$:**

$$q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \mu_\phi(\mathbf{x}), \text{diag}(\sigma_\phi^2(\mathbf{x})))$$

Encoder выдаёт **параметры** распределения: $\mu$ и $\sigma$.

### ELBO — Evidence Lower Bound

**Можно показать, что:**

$$\log p(\mathbf{x}) \geq \underbrace{\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]}_{\text{Реконструкция}} - \underbrace{D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) | p(\mathbf{z}))}_{\text{Регуляризация}}$$

$$\log p(\mathbf{x}) \geq \text{ELBO}(\mathbf{x}; \theta, \phi)$$

**ELBO** (Evidence Lower Bound) — нижняя граница логарифма правдоподобия.

**Максимизируя ELBO, мы косвенно максимизируем $\log p(\mathbf{x})$!**

### Интерпретация членов ELBO

#### Реконструкционный член

$$\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]$$

**Что это:** ожидаемое log-правдоподобие реконструкции.

**Как вычислить:**

1. Сэмплируем $\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})$
2. Декодируем: $\hat{\mathbf{x}} = \text{Decoder}(\mathbf{z})$
3. Вычисляем: $\log p(\mathbf{x}|\hat{\mathbf{x}})$ (например, $-|\mathbf{x} - \hat{\mathbf{x}}|^2$ для Гауссиана)

**Смысл:** насколько хорошо декодировщик восстанавливает $\mathbf{x}$ из $\mathbf{z}$.

#### Регуляризационный член (KL-дивергенция)

$$D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) | p(\mathbf{z}))$$

**Что это:** насколько сильно апостериорное распределение $q$ отличается от априорного $p$.

**Смысл:** штраф за отклонение латентного распределения от $\mathcal{N}(0, I)$.

### Почему KL-член важен

**Без KL-члена:**

- Encoder может делать $\sigma \to 0$ (дельта-функция)
- Латентное пространство коллапсирует в отдельные точки
- Возвращаемся к проблеме обычного автоэнкодера

**С KL-членом:**

- Encoder вынужден держать $q$ близким к $\mathcal{N}(0, I)$
- Латентное пространство становится **непрерывным** и **плотным**
- Можно сэмплировать из $\mathcal{N}(0, I)$ и получать осмысленные генерации

**Визуализация:**

```
Латентное пространство VAE:

      z₂
       │   ••••••••••
       │  ••••••••••••
       │ •••••••••••••   Плотно заполнено!
       │  ••••••••••••
       │   ••••••••••
       └────────────────── z₁
       
Генерация из любой точки → осмысленный результат
```

### Интуитивная аналогия

**Обычный AE:** художник запоминает картины по номерам.

- «Картина 42» → конкретное изображение
- «Картина 42.5» → ??? (не определено)

**VAE:** художник работает с непрерывным пространством стилей.

- «Координаты стиля (0.5, 0.3)» → конкретное изображение
- «Координаты стиля (0.6, 0.3)» → похожее изображение (немного другое)
- Можно плавно интерполировать между картинами

---

## Вопрос 26. Репараметризационный трюк. Зачем он нужен?

### Проблема: как обучать через сэмплирование?

**Архитектура VAE:**

```
x → Encoder → (μ, σ) → z ~ N(μ, σ²) → Decoder → x̂
                           ↑
                    Сэмплирование
```

**Проблема:** операция сэмплирования $\mathbf{z} \sim \mathcal{N}(\mu, \sigma^2)$ **не дифференцируема!**

Мы не можем вычислить $\frac{\partial \mathbf{z}}{\partial \mu}$ или $\frac{\partial \mathbf{z}}{\partial \sigma}$, потому что $\mathbf{z}$ — случайная величина.

**Без градиентов невозможно обучать encoder методом backpropagation!**

### Репараметризационный трюк (Reparameterization Trick)

**Идея:** вынести случайность «за скобки» нейросети.

**Вместо:** $$\mathbf{z} \sim \mathcal{N}(\mu_\phi(\mathbf{x}), \sigma_\phi^2(\mathbf{x}))$$

**Записываем:** $$\epsilon \sim \mathcal{N}(0, I) \quad \text{(внешний шум)}$$ $$\mathbf{z} = \mu_\phi(\mathbf{x}) + \sigma_\phi(\mathbf{x}) \odot \epsilon$$

### Почему это эквивалентно

**Факт из теории вероятностей:**

Если $\epsilon \sim \mathcal{N}(0, 1)$, то $\mu + \sigma \cdot \epsilon \sim \mathcal{N}(\mu, \sigma^2)$.

**Доказательство:**

$$\mathbb{E}[\mu + \sigma\epsilon] = \mu + \sigma \cdot 0 = \mu$$ $$\text{Var}[\mu + \sigma\epsilon] = \sigma^2 \cdot \text{Var}[\epsilon] = \sigma^2 \cdot 1 = \sigma^2$$

### Почему теперь можно дифференцировать

**Ключевое отличие:** $\epsilon$ **не зависит от параметров** $\phi$!

$\epsilon$ сэмплируется из фиксированного распределения $\mathcal{N}(0, I)$.

**Теперь $\mathbf{z}$ — детерминированная функция от $\mu$, $\sigma$ и внешнего $\epsilon$:**

$$\mathbf{z} = \mu + \sigma \odot \epsilon$$

**Градиенты:**

$$\frac{\partial \mathbf{z}}{\partial \mu} = I \quad \text{(единичная матрица)}$$

$$\frac{\partial \mathbf{z}}{\partial \sigma} = \text{diag}(\epsilon)$$

**Градиенты существуют и могут течь через $\mathbf{z}$ к параметрам encoder'а $\phi$!**

### Визуализация вычислительного графа

**Без репараметризации (не работает):**

```
x → [Encoder] → (μ, σ) → SAMPLE → z → [Decoder] → x̂
                            ↑
                     Градиент не проходит!
```

**С репараметризацией (работает):**

```
                    ε ~ N(0,I)
                        ↓
x → [Encoder] → (μ, σ) → z = μ + σ⊙ε → [Decoder] → x̂
        ↑                    ↑
        └────── Градиент проходит! ──────┘
```

### Пример расчёта

**Encoder выдал:** $\mu = 0.5$, $\sigma = 0.2$

**Сэмплируем:** $\epsilon = 1.3$ (из $\mathcal{N}(0, 1)$)

**Вычисляем:** $z = 0.5 + 0.2 \times 1.3 = 0.5 + 0.26 = 0.76$

**Градиенты:**

$$\frac{\partial z}{\partial \mu} = 1$$ $$\frac{\partial z}{\partial \sigma} = \epsilon = 1.3$$

**Теперь backprop работает!**

### Код (PyTorch)

```python
class VAE(nn.Module):
    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        log_var = self.fc_logvar(h)  # Логарифм дисперсии для стабильности
        return mu, log_var
    
    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)  # σ = exp(0.5 * log(σ²))
        eps = torch.randn_like(std)      # ε ~ N(0, I)
        return mu + std * eps            # z = μ + σ ⊙ ε
    
    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        return self.decode(z), mu, log_var
```

### Интуитивная аналогия

**Без репараметризации:**

- Команда роботу: «Бросай дротики случайно в область X»
- Обратная связь: «Смести область левее»
- Проблема: случайность внутри команды → не знаем, как сместить

**С репараметризацией:**

- Команда роботу: «Целься в точку (X, Y), потом добавь случайное отклонение от внешнего генератора»
- Обратная связь: «Смести точку прицеливания левее»
- Работает: случайность внешняя → можем управлять точкой прицеливания

---

## Вопрос 27. Функция потерь VAE: реконструкция и KL-дивергенция

### Полная функция потерь VAE

$$L_{VAE} = -\text{ELBO} = L_{rec} + \beta \cdot L_{KL}$$

где $\beta$ — коэффициент баланса (в базовом VAE $\beta = 1$).

---

### Реконструкционный член

$$L_{rec} = -\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]$$

#### Для непрерывных данных (Гауссовский декодер)

**Предположение:** $p_\theta(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mathbf{x}; \mu_\theta(\mathbf{z}), \sigma^2 I)$

Декодер выдаёт $\mu_\theta(\mathbf{z})$, дисперсия фиксирована.

$$\log p_\theta(\mathbf{x}|\mathbf{z}) = -\frac{1}{2\sigma^2}|\mathbf{x} - \mu_\theta(\mathbf{z})|^2 + \text{const}$$

**При $\sigma = 1$ (или поглощении в константу):**

$$L_{rec} = |\mathbf{x} - \hat{\mathbf{x}}|^2 = \sum_{i=1}^{D} (x_i - \hat{x}_i)^2$$

**Это просто MSE!**

#### Для бинарных данных (Бернуллиевский декодер)

**Предположение:** $p_\theta(\mathbf{x}|\mathbf{z}) = \prod_{i=1}^{D} \text{Bernoulli}(x_i; p_i(\mathbf{z}))$

Декодер выдаёт вероятности $p_i = \sigma(\text{output}_i)$ через sigmoid.

$$L_{rec} = -\sum_{i=1}^{D} [x_i \log(p_i) + (1 - x_i) \log(1 - p_i)]$$

**Это бинарная кросс-энтропия (BCE)!**

#### Пример расчёта реконструкции (MSE)

**Вход:** $\mathbf{x} = [0.8, 0.2, 0.9]$ **Реконструкция:** $\hat{\mathbf{x}} = [0.75, 0.3, 0.85]$

$$L_{rec} = (0.8-0.75)^2 + (0.2-0.3)^2 + (0.9-0.85)^2$$ $$= 0.0025 + 0.01 + 0.0025 = 0.015$$

---

### Регуляризационный член (KL-дивергенция)

$$L_{KL} = D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) | p(\mathbf{z}))$$

#### Для Гауссовских распределений

**Encoder выдаёт:** $q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mu, \text{diag}(\sigma^2))$

**Априорное:** $p(\mathbf{z}) = \mathcal{N}(0, I)$

**Аналитическая формула:**

$$D_{KL} = \frac{1}{2} \sum_{j=1}^{d} \left( \mu_j^2 + \sigma_j^2 - \log(\sigma_j^2) - 1 \right)$$

**Или через log-дисперсию $\log\sigma^2$:**

$$D_{KL} = \frac{1}{2} \sum_{j=1}^{d} \left( \mu_j^2 + \exp(\log\sigma_j^2) - \log\sigma_j^2 - 1 \right)$$

#### Вывод формулы KL-дивергенции

Для одномерного случая:

$$D_{KL}(\mathcal{N}(\mu, \sigma^2) | \mathcal{N}(0, 1)) = \int \mathcal{N}(z; \mu, \sigma^2) \log\frac{\mathcal{N}(z; \mu, \sigma^2)}{\mathcal{N}(z; 0, 1)} dz$$

После упрощения (используя свойства Гауссиана):

$$= \frac{1}{2}(\mu^2 + \sigma^2 - \log\sigma^2 - 1)$$

Для многомерного случая — сумма по всем измерениям.

#### Пример расчёта KL-дивергенции

**Encoder выдал:** $\mu = [0.5, -0.3]$, $\sigma = [1.2, 0.8]$

$$D_{KL} = \frac{1}{2} \sum_{j=1}^{2} (\mu_j^2 + \sigma_j^2 - \log(\sigma_j^2) - 1)$$

**Для $j=1$:** $\mu_1 = 0.5$, $\sigma_1 = 1.2$ $$= 0.5^2 + 1.2^2 - \log(1.44) - 1 = 0.25 + 1.44 - 0.365 - 1 = 0.325$$

**Для $j=2$:** $\mu_2 = -0.3$, $\sigma_2 = 0.8$ $$= (-0.3)^2 + 0.8^2 - \log(0.64) - 1 = 0.09 + 0.64 - (-0.446) - 1 = 0.176$$

$$D_{KL} = \frac{1}{2}(0.325 + 0.176) = \frac{1}{2} \times 0.501 = 0.25$$

---

### Интерпретация членов потерь

|Член|Что минимизирует|Эффект|
|---|---|---|
|$L_{rec}$|Ошибку реконструкции|Качественное восстановление|
|$L_{KL}$|Отклонение от $\mathcal{N}(0,I)$|Структурированное латентное пространство|

---

### β-VAE: управление балансом

В **β-VAE** можно варьировать коэффициент при KL:

$$L = L_{rec} + \beta \cdot L_{KL}$$

**$\beta > 1$ (сильная регуляризация):**

- Латентное пространство очень «организованное»
- Разделение факторов (disentanglement)
- Но: размытые реконструкции

**$\beta < 1$ (слабая регуляризация):**

- Чёткие реконструкции
- Но: хуже генерация из случайных точек
- Ближе к обычному автоэнкодеру

**$\beta = 1$ (стандартный VAE):**

- Баланс между реконструкцией и структурой

---

### Проблема баланса — KL collapse

**Проблема:** иногда сеть «схлопывает» KL-член в ноль:

- $\mu \to 0$, $\sigma \to 1$ для всех $\mathbf{x}$
- Encoder игнорирует вход!
- Decoder пытается реконструировать из шума

**Решение — KL annealing:**

- Начинаем с $\beta = 0$ (только реконструкция)
- Постепенно увеличиваем $\beta$ до 1

```python
# KL annealing
beta = min(1.0, epoch / warmup_epochs)
loss = reconstruction_loss + beta * kl_loss
```

---

# РАЗДЕЛ 8. ПАЙПЛАЙН ОБУЧЕНИЯ

---

## Вопрос 28. Этапы пайплайна машинного обучения. Зачем нужна валидационная выборка?

### Полный пайплайн ML

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  1. Постановка   2. Сбор      3. Разведочный    4. Предобработка│
│     задачи    →    данных   →    анализ (EDA) →   данных        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  5. Разделение   6. Выбор     7. Обучение    8. Валидация       │
│     данных    →    модели   →   модели    →  и настройка       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│  9. Финальное    10. Развёртывание   11. Мониторинг             │
│     тестирование →   (Deployment)  →    и поддержка            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Детальное описание этапов

#### Этап 1: Постановка задачи

**Что делаем:**

- Определяем бизнес-цель
- Формулируем задачу в терминах ML
- Определяем метрики успеха

**Вопросы:**

- Это классификация, регрессия или что-то другое?
- Какие данные доступны?
- Как будет использоваться модель?

#### Этап 2: Сбор данных

**Что делаем:**

- Собираем данные из различных источников
- Объединяем в единый датасет
- Проверяем качество и полноту

**Источники данных:**

- Базы данных
- API
- Веб-скрапинг
- Размеченные датасеты (Kaggle, UCI, etc.)

#### Этап 3: Разведочный анализ данных (EDA)

**Что делаем:**

- Визуализируем данные
- Изучаем распределения признаков
- Находим выбросы и аномалии
- Исследуем корреляции

**Примеры:**

- Гистограммы распределений
- Scatter plots
- Correlation matrix
- Boxplots для выбросов

#### Этап 4: Предобработка данных

**Что делаем:**

**Очистка:**

- Удаление дубликатов
- Обработка пропущенных значений
- Исправление ошибок

**Трансформация:**

- Нормализация / стандартизация
- Кодирование категориальных признаков
- Feature engineering

**Пример нормализации:**

$$x_{norm} = \frac{x - \mu}{\sigma}$$

**Пример Min-Max scaling:**

$$x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}$$

#### Этап 5: Разделение данных

**Стандартное разделение:**

$$\text{Данные} = \text{Train} + \text{Validation} + \text{Test}$$

**Типичные пропорции:**

- 60% / 20% / 20%
- 70% / 15% / 15%
- 80% / 10% / 10%

**Пример для 10,000 примеров:**

- Train: 6,000
- Validation: 2,000
- Test: 2,000

#### Этап 6: Выбор модели и гиперпараметров

**Что выбираем:**

- Архитектура (MLP, CNN, RNN, Transformer, ...)
- Количество слоёв, нейронов
- Функции активации
- Оптимизатор (Adam, SGD, ...)
- Learning rate
- Batch size
- Регуляризация (dropout, L2, ...)

#### Этап 7: Обучение модели

**Процесс:**

```python
for epoch in range(num_epochs):
    for batch in train_loader:
        # Forward pass
        predictions = model(batch.x)
        loss = criterion(predictions, batch.y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

#### Этап 8: Валидация и настройка

**После каждой эпохи:**

1. Вычисляем метрики на validation set
2. Записываем в лог
3. Сохраняем лучшую модель
4. Применяем early stopping если нужно

```python
if val_loss < best_val_loss:
    best_val_loss = val_loss
    save_model(model)
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= patience:
        break  # Early stopping
```

#### Этап 9: Финальное тестирование

**Важно:** Test set используется **ТОЛЬКО ОДИН РАЗ** — в самом конце!

**Процесс:**

1. Загружаем лучшую модель (по validation)
2. Вычисляем метрики на test set
3. Это финальная оценка качества модели

#### Этап 10: Развёртывание (Deployment)

**Варианты:**

- REST API
- Встраивание в приложение
- Облачные сервисы (AWS, GCP, Azure)
- Edge deployment (на устройстве)

#### Этап 11: Мониторинг и поддержка

**Отслеживаем:**

- Качество предсказаний в продакшене
- Data drift (изменение распределения данных)
- Latency и производительность

---

### Зачем нужна валидационная выборка?

#### Проблема без валидации

**Если использовать test set для подбора гиперпараметров:**

1. Вы пробуете модель A на test → accuracy 85%
2. Вы пробуете модель B на test → accuracy 87%
3. Вы выбираете B
4. **Но:** вы «подстроились» под test set!
5. На реальных данных модель B может работать хуже

**Test set «загрязнён»** — оценка оптимистична.

#### Роль валидации

**Validation set — это «тренировочный экзамен»:**

- Используется для подбора гиперпараметров
- Используется для early stopping
- Можно смотреть много раз

**Test set — это «реальный экзамен»:**

- Используется только один раз
- Даёт честную оценку
- Нельзя подсматривать и менять модель!

#### Конкретные применения валидации

**1. Подбор гиперпараметров:**

```
Learning rate:  0.01  → val_acc = 85%
Learning rate:  0.001 → val_acc = 92%  ← Лучший!
Learning rate:  0.0001→ val_acc = 88%
```

**2. Выбор архитектуры:**

```
2 слоя, 64 нейрона  → val_acc = 89%
3 слоя, 128 нейронов→ val_acc = 94%  ← Лучший!
4 слоя, 256 нейронов→ val_acc = 93%
```

**3. Early Stopping:**

```
Epoch 1: train_loss=0.5, val_loss=0.52
Epoch 2: train_loss=0.3, val_loss=0.35
Epoch 3: train_loss=0.1, val_loss=0.25  ← best
Epoch 4: train_loss=0.05, val_loss=0.30  ← val ухудшился
Epoch 5: train_loss=0.02, val_loss=0.35  ← ещё хуже
→ Останавливаемся, берём модель с эпохи 3
```

**4. Выбор лучшего чекпоинта:**

```
Эпоха 10: сохранить? val_loss = 0.5
Эпоха 20: сохранить? val_loss = 0.3  ← лучше, сохраняем
Эпоха 30: сохранить? val_loss = 0.4  ← хуже, не сохраняем
→ Используем модель с эпохи 20
```

---

### Кросс-валидация (Cross-Validation)

**Когда использовать:** данных мало, нужна надёжная оценка.

#### K-Fold Cross-Validation

**Процесс:**

1. Разбиваем данные на $k$ частей (folds)
2. Обучаем $k$ раз, каждый раз используя другую часть для валидации
3. Усредняем результаты

```
Fold 1: [VAL][TRAIN][TRAIN][TRAIN][TRAIN] → score₁
Fold 2: [TRAIN][VAL][TRAIN][TRAIN][TRAIN] → score₂
Fold 3: [TRAIN][TRAIN][VAL][TRAIN][TRAIN] → score₃
Fold 4: [TRAIN][TRAIN][TRAIN][VAL][TRAIN] → score₄
Fold 5: [TRAIN][TRAIN][TRAIN][TRAIN][VAL] → score₅

Final score = (score₁ + score₂ + score₃ + score₄ + score₅) / 5
```

**Преимущества:**

- Все данные используются и для обучения, и для валидации
- Более робастная оценка
- Понимание variance модели

**Недостаток:**

- $k$ раз больше вычислений

#### Stratified K-Fold

**Идея:** сохранять пропорции классов в каждом fold.

**Пример:**

Данные: 70% класс A, 30% класс B

Без стратификации (плохо):

```
Fold 1: 90% A, 10% B  ← несбалансированно!
Fold 2: 50% A, 50% B
```

Со стратификацией (хорошо):

```
Fold 1: 70% A, 30% B  ← сохранены пропорции
Fold 2: 70% A, 30% B
```

### Аналогия — подготовка к экзамену

**Train set** — это учебник, по которому вы учитесь.

**Validation set** — это пробные тесты:

- Вы решаете их, чтобы понять свой уровень
- Если плохо — возвращаетесь к учебнику
- Можете решать много раз

**Test set** — это реальный экзамен:

- Вы сдаёте его один раз
- Результат показывает ваш истинный уровень
- Нельзя подглядывать в ответы заранее!

Если бы вы заранее знали вопросы реального экзамена и готовились по ним, оценка была бы завышена и не отражала бы реальные знания.